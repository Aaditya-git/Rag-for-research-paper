{"chunk_id": "drag_fixed_p1_c0", "pdf_name": "drag", "page_number": 1, "char_count": 877, "text": "Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 15159–15177\nNovember 12-16, 2024 ©2024 Association for Computational Linguistics\nDense X Retrieval: What Retrieval Granularity Should We Use? Tong Chen♣∗Hongwei Wang♢Sihao Chen♡Wenhao Yu♢\nKaixin Ma♢Xinran Zhao♠Hongming Zhang♢Dong Yu♢\n♣University of Washington\n♢Tencent AI Lab\n♡University of Pennsylvania\n♠Carnegie Mellon University\nAbstract\nDense retrieval has become a prominent\nmethod to obtain relevant context or world\nknowledge in open-domain NLP tasks. When\nwe use a learned dense retriever on a retrieval\ncorpus at inference time, an often-overlooked\ndesign choice is the retrieval unit in which the\ncorpus is indexed, e.g. document, passage, or\nsentence. We discover that the retrieval unit\nchoice significantly impacts the performance of\nboth retrieval and downstream tasks.", "chunk_type": "fixed", "chunker": "fixed_recursive_v1"}
{"chunk_id": "drag_fixed_p1_c1", "pdf_name": "drag", "page_number": 1, "char_count": 855, "text": "We discover that the retrieval unit\nchoice significantly impacts the performance of\nboth retrieval and downstream tasks. Distinct\nfrom the typical approach of using passages or\nsentences, we introduce a novel retrieval unit,\nproposition, for dense retrieval. Propositions\nare defined as atomic expressions within text,\neach encapsulating a distinct factoid and pre-\nsented in a concise, self-contained natural lan-\nguage format. We conduct an empirical com-\nparison of different retrieval granularity. Our\nexperiments reveal that indexing a corpus by\nfine-grained units such as propositions signif-\nicantly outperforms passage-level units in re-\ntrieval tasks. Moreover, constructing prompts\nwith fine-grained retrieved units for retrieval-\naugmented language models improves the per-\nformance of downstream QA tasks given a spe-\ncific computation budget.", "chunk_type": "fixed", "chunker": "fixed_recursive_v1"}
{"chunk_id": "drag_fixed_p1_c2", "pdf_name": "drag", "page_number": 1, "char_count": 880, "text": "Moreover, constructing prompts\nwith fine-grained retrieved units for retrieval-\naugmented language models improves the per-\nformance of downstream QA tasks given a spe-\ncific computation budget. 1\nIntroduction\nDense retrievers are a popular class of techniques\nfor accessing external information sources for open-\ndomain NLP tasks (Karpukhin et al., 2020). Before\nwe use a learned dense retriever to retrieve from a\ncorpus, an imperative design decision we have to\nmake is the retrieval unit – i.e. the granularity at\nwhich we segment and index the retrieval corpus\nfor inference. In practice, the choice of retrieval\nunits, e.g. documents, fixed-length passage chunks\nor sentences, etc, is usually pre-determined based\non how the dense retrieval model is instantiated\n∗Work was done during internship at Tencent AI Lab,\nBellevue. Question: What is the angle of the Tower of Pisa?", "chunk_type": "fixed", "chunker": "fixed_recursive_v1"}
{"chunk_id": "drag_fixed_p1_c3", "pdf_name": "drag", "page_number": 1, "char_count": 914, "text": "Question: What is the angle of the Tower of Pisa? Passage\nRetrieval\nPrior to restoration work performed be-\ntween 1990 and 2001, the tower leaned at\nan angle of 5.5 degrees, but the tower now\nleans at about 3.99 degrees. This means\nthe top of the Leaning Tower of Pisa is dis-\nplaced horizontally 3.9 meters (12 ft 10 in)\nfrom the center. Sentence\nRetrieval\nPrior to restoration work performed be-\ntween 1990 and 2001, the tower leaned at\nan angle of 5.5 degrees, but the tower now\nleans at about 3.99 degrees. Proposition\nRetrieval\nThe Leaning Tower of Pisa now leans at\nabout 3.99 degrees. Contriever\nGTR\n35\n45\n55\n65\n75\nRecall@5 (%)\n43.0\n65.2\n47.3\n66.7\n52.7\n68.0\nPassage Retrieval\nContriever\nGTR\n25\n30\n35\n40\n45\nEM@500 (%)\n34.1\n38.5\n36.2\n40.1\n37.3\n41.3\nOpen-domain QA\nPassage\nSentence\nProposition\nFigure 1: (Top) An example of three granularities of\nretrieval units of Wikipedia text when using dense re-\ntrieval.", "chunk_type": "fixed", "chunker": "fixed_recursive_v1"}
{"chunk_id": "drag_fixed_p1_c4", "pdf_name": "drag", "page_number": 1, "char_count": 944, "text": "M@500 (%)\n34.1\n38.5\n36.2\n40.1\n37.3\n41.3\nOpen-domain QA\nPassage\nSentence\nProposition\nFigure 1: (Top) An example of three granularities of\nretrieval units of Wikipedia text when using dense re-\ntrieval. (Bottom) We observe that retrieving by proposi-\ntions yields the best retrieval performance in both pas-\nsage retrieval task and downstream open-domain QA\ntask, e.g. with Contriever (Izacard et al., 2022) or GTR\n(Ni et al., 2022) as the backbone retriever. Highlight in-\ndicates the part that contains the answer to the question. or trained (Lewis et al., 2020; Lee et al., 2021a;\nSanthanam et al., 2022; Ni et al., 2022). In this paper, we investigate an overlooked re-\nsearch question with dense retrieval inference – at\nwhat retrieval granularity should we segment and\nindex the retrieval corpus? We aim to investigate\nthis question in two aspects. • First, we examine how the granularity of the\nindex affects passage retrieval performance.", "chunk_type": "fixed", "chunker": "fixed_recursive_v1"}
{"chunk_id": "drag_fixed_p1_c5", "pdf_name": "drag", "page_number": 1, "char_count": 194, "text": "• First, we examine how the granularity of the\nindex affects passage retrieval performance. • Second, we investigate whether fine-grained units\ncan replace passages in downstream QA tasks. 15159", "chunk_type": "fixed", "chunker": "fixed_recursive_v1"}
{"chunk_id": "drag_semantic_p1_c0", "pdf_name": "drag", "page_number": 1, "char_count": 724, "text": "Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 15159–15177\nNovember 12-16, 2024 ©2024 Association for Computational Linguistics\nDense X Retrieval: What Retrieval Granularity Should We Use? Tong Chen♣∗Hongwei Wang♢Sihao Chen♡Wenhao Yu♢\nKaixin Ma♢Xinran Zhao♠Hongming Zhang♢Dong Yu♢\n♣University of Washington\n♢Tencent AI Lab\n♡University of Pennsylvania\n♠Carnegie Mellon University\nAbstract\nDense retrieval has become a prominent\nmethod to obtain relevant context or world\nknowledge in open-domain NLP tasks. When\nwe use a learned dense retriever on a retrieval\ncorpus at inference time, an often-overlooked\ndesign choice is the retrieval unit in which the\ncorpus is indexed, e.g.", "chunk_type": "semantic", "chunker": "semantic_adjacent_v1"}
{"chunk_id": "drag_semantic_p1_c1", "pdf_name": "drag", "page_number": 1, "char_count": 460, "text": "document, passage, or\nsentence. We discover that the retrieval unit\nchoice significantly impacts the performance of\nboth retrieval and downstream tasks. Distinct\nfrom the typical approach of using passages or\nsentences, we introduce a novel retrieval unit,\nproposition, for dense retrieval. Propositions\nare defined as atomic expressions within text,\neach encapsulating a distinct factoid and pre-\nsented in a concise, self-contained natural lan-\nguage format.", "chunk_type": "semantic", "chunker": "semantic_adjacent_v1"}
{"chunk_id": "drag_semantic_p1_c2", "pdf_name": "drag", "page_number": 1, "char_count": 426, "text": "We conduct an empirical com-\nparison of different retrieval granularity. Our\nexperiments reveal that indexing a corpus by\nfine-grained units such as propositions signif-\nicantly outperforms passage-level units in re-\ntrieval tasks. Moreover, constructing prompts\nwith fine-grained retrieved units for retrieval-\naugmented language models improves the per-\nformance of downstream QA tasks given a spe-\ncific computation budget.", "chunk_type": "semantic", "chunker": "semantic_adjacent_v1"}
{"chunk_id": "drag_semantic_p1_c3", "pdf_name": "drag", "page_number": 1, "char_count": 385, "text": "1\nIntroduction\nDense retrievers are a popular class of techniques\nfor accessing external information sources for open-\ndomain NLP tasks (Karpukhin et al., 2020). Before\nwe use a learned dense retriever to retrieve from a\ncorpus, an imperative design decision we have to\nmake is the retrieval unit – i.e. the granularity at\nwhich we segment and index the retrieval corpus\nfor inference.", "chunk_type": "semantic", "chunker": "semantic_adjacent_v1"}
{"chunk_id": "drag_semantic_p1_c4", "pdf_name": "drag", "page_number": 1, "char_count": 470, "text": "In practice, the choice of retrieval\nunits, e.g. documents, fixed-length passage chunks\nor sentences, etc, is usually pre-determined based\non how the dense retrieval model is instantiated\n∗Work was done during internship at Tencent AI Lab,\nBellevue. Question: What is the angle of the Tower of Pisa? Passage\nRetrieval\nPrior to restoration work performed be-\ntween 1990 and 2001, the tower leaned at\nan angle of 5.5 degrees, but the tower now\nleans at about 3.99 degrees.", "chunk_type": "semantic", "chunker": "semantic_adjacent_v1"}
{"chunk_id": "drag_semantic_p1_c5", "pdf_name": "drag", "page_number": 1, "char_count": 370, "text": "This means\nthe top of the Leaning Tower of Pisa is dis-\nplaced horizontally 3.9 meters (12 ft 10 in)\nfrom the center. Sentence\nRetrieval\nPrior to restoration work performed be-\ntween 1990 and 2001, the tower leaned at\nan angle of 5.5 degrees, but the tower now\nleans at about 3.99 degrees. Proposition\nRetrieval\nThe Leaning Tower of Pisa now leans at\nabout 3.99 degrees.", "chunk_type": "semantic", "chunker": "semantic_adjacent_v1"}
{"chunk_id": "drag_semantic_p1_c6", "pdf_name": "drag", "page_number": 1, "char_count": 322, "text": "Contriever\nGTR\n35\n45\n55\n65\n75\nRecall@5 (%)\n43.0\n65.2\n47.3\n66.7\n52.7\n68.0\nPassage Retrieval\nContriever\nGTR\n25\n30\n35\n40\n45\nEM@500 (%)\n34.1\n38.5\n36.2\n40.1\n37.3\n41.3\nOpen-domain QA\nPassage\nSentence\nProposition\nFigure 1: (Top) An example of three granularities of\nretrieval units of Wikipedia text when using dense re-\ntrieval.", "chunk_type": "semantic", "chunker": "semantic_adjacent_v1"}
{"chunk_id": "drag_semantic_p1_c7", "pdf_name": "drag", "page_number": 1, "char_count": 329, "text": "(Bottom) We observe that retrieving by proposi-\ntions yields the best retrieval performance in both pas-\nsage retrieval task and downstream open-domain QA\ntask, e.g. with Contriever (Izacard et al., 2022) or GTR\n(Ni et al., 2022) as the backbone retriever. Highlight in-\ndicates the part that contains the answer to the question.", "chunk_type": "semantic", "chunker": "semantic_adjacent_v1"}
{"chunk_id": "drag_semantic_p1_c8", "pdf_name": "drag", "page_number": 1, "char_count": 516, "text": "or trained (Lewis et al., 2020; Lee et al., 2021a;\nSanthanam et al., 2022; Ni et al., 2022). In this paper, we investigate an overlooked re-\nsearch question with dense retrieval inference – at\nwhat retrieval granularity should we segment and\nindex the retrieval corpus? We aim to investigate\nthis question in two aspects. • First, we examine how the granularity of the\nindex affects passage retrieval performance. • Second, we investigate whether fine-grained units\ncan replace passages in downstream QA tasks. 15159", "chunk_type": "semantic", "chunker": "semantic_adjacent_v1"}
{"chunk_id": "drag_fixed_p2_c6", "pdf_name": "drag", "page_number": 2, "char_count": 991, "text": "1. Prior to restoration work performed \nbetween 1990 and 2001, the Leaning Tower \nof Pisa leaned at an angle of 5.5 degrees. 2. The Leaning Tower of Pisa now leans at \nabout 3.99 degrees. 3. The top of the Leaning Tower of Pisa is \ndisplaced horizontally 3.9 meters (12 ft 10 in) \nfrom the center. Prior to restoration work performed between \n1990 and 2001, the tower leaned at an angle of \n5.5 degrees , // but the tower now leans at \nabout 3.99 degrees. // This means the top of the \nLearning Tower of Pisa is displaced horizontally \n3.9 meters (12 ft 10 in) from the center. WIkipedia\nFactoidWiki\n? ✓\nAnswer\nQuery\nQA Model\nRetrieval Units\nRetrieval \nUnits\nPassage Retrieval\nB\nC\nD\nPropositionizer\n? Query\nCorpus\nPassages\nSentences\nPropositions\nRetrieval \nUnits\nRetriever\nA\nFigure 2: We discover that segmenting and indexing a retrieval corpus on the proposition level can be a simple yet\neffective strategy to increase dense retrievers’ generalization performance at inference time (A, B).", "chunk_type": "fixed", "chunker": "fixed_recursive_v1"}
{"chunk_id": "drag_fixed_p2_c7", "pdf_name": "drag", "page_number": 2, "char_count": 956, "text": "cover that segmenting and indexing a retrieval corpus on the proposition level can be a simple yet\neffective strategy to increase dense retrievers’ generalization performance at inference time (A, B). We empirically\ncompare the retrieval and downstream open-domain QA task performance when dense retrievers work with\nWikipedia indexed at the level of 100-word passages, sentences, or propositions (C, D). Based on our empirical experiments, we discover\nthat selecting the proper retrieval granularity at in-\nference time can be a simple yet effective strategy\nfor improving dense retrievers’ retrieval and down-\nstream QA performance. We illustrate our intuition\nwith an example of open-domain QA in Table 1. The example shows retrieved text by the same re-\ntriever at three different granularities. The pas-\nsage, which represents a coarser retrieval unit with\na longer context, is theoretically able to provide\nmore relevant information for the question.", "chunk_type": "fixed", "chunker": "fixed_recursive_v1"}
{"chunk_id": "drag_fixed_p2_c8", "pdf_name": "drag", "page_number": 2, "char_count": 869, "text": "The pas-\nsage, which represents a coarser retrieval unit with\na longer context, is theoretically able to provide\nmore relevant information for the question. How-\never, a passage often includes extraneous details\n(e.g., restoration period and horizontal displace-\nment in the example of Table 1) that could poten-\ntially distract both the retriever and the language\nmodel in downstream tasks (Shi et al., 2023; Yu\net al., 2023b). On the other hand, sentence-level in-\ndexing provides a finer-grained approach but does\nnot entirely address the issue (Akkalyoncu Yilmaz\net al., 2019; Yang et al., 2020). This is because\nsentences can still be complex and compounded,\nand they are often not self-contained, lacking nec-\nessary contextual information (e.g., in the example\nof Table 1, “the tower” is the coreference of “Pisa\nTower”) for judging the query-document relevance.", "chunk_type": "fixed", "chunker": "fixed_recursive_v1"}
{"chunk_id": "drag_fixed_p2_c9", "pdf_name": "drag", "page_number": 2, "char_count": 838, "text": "hey are often not self-contained, lacking nec-\nessary contextual information (e.g., in the example\nof Table 1, “the tower” is the coreference of “Pisa\nTower”) for judging the query-document relevance. To address these shortcomings of typical re-\ntrieval units such as passages or sentences, we pro-\npose using proposition as a novel retrieval unit for\ndense retrieval. Propositions are defined as atomic\nexpressions within text, where each encapsulates\na distinct factoid and is presented in a concise,\nself-contained natural language format. We show\nan example proposition in Table 1. The proposi-\ntion describes the information regarding the Tower\nof Pisa’s current leaning angle in a self-contained\nway and precisely responds to what the question\nis querying. We provide a more detailed definition\nand description of proposition in §2.", "chunk_type": "fixed", "chunker": "fixed_recursive_v1"}
{"chunk_id": "drag_fixed_p2_c10", "pdf_name": "drag", "page_number": 2, "char_count": 867, "text": "We provide a more detailed definition\nand description of proposition in §2. To validate\nthe efficacy of using proposition as a retrieval unit\nfor dense retrievers inference, we first process and\nindex an English Wikipedia dump with all docu-\nments segmented into propositions, which we refer\nto as FACTOIDWIKI. We conduct experiments on five different open-\ndomain QA datasets and empirically compare the\nperformance of four dual-encoder retrievers when\nWikipedia is indexed by passages, sentences, and\nour proposed propositions. Notably, our findings in-\ndicate that proposition-based retrieval outperforms\nsentence and passage-based retrieval, especially in\nterms of generalization, as discussed in §5. This\nsuggests that propositions, being both compact and\nrich in context, enable dense retrievers to access\nprecise information while maintaining adequate\ncontext.", "chunk_type": "fixed", "chunker": "fixed_recursive_v1"}
{"chunk_id": "drag_fixed_p2_c11", "pdf_name": "drag", "page_number": 2, "char_count": 907, "text": "This\nsuggests that propositions, being both compact and\nrich in context, enable dense retrievers to access\nprecise information while maintaining adequate\ncontext. The average improvement over passage-\nbased retrieval of Recall@20 is +10.1 on unsuper-\nvised dense retrievers and +2.7 on supervised re-\ntrievers, even though these retrievers were directly\ntrained on passage-level retrieval. Furthermore,\nwe observe a distinct advantage of proposition-\nbased retrieval in downstream QA performance\nwhen using retrieval-augmented language models,\nas elaborated in §6. Retrieval by finer-grained units\ninherently provides a higher density of question-\nrelevant information. This finding implies using\nfiner-grained units in the prompts achieves the same\nperformance with a shorter input length, and hence,\na faster inference time. Our main contributions are:\n• We provide a systemic study on how retrieval\n15160", "chunk_type": "fixed", "chunker": "fixed_recursive_v1"}
{"chunk_id": "drag_semantic_p2_c9", "pdf_name": "drag", "page_number": 2, "char_count": 455, "text": "1. Prior to restoration work performed \nbetween 1990 and 2001, the Leaning Tower \nof Pisa leaned at an angle of 5.5 degrees. 2. The Leaning Tower of Pisa now leans at \nabout 3.99 degrees. 3. The top of the Leaning Tower of Pisa is \ndisplaced horizontally 3.9 meters (12 ft 10 in) \nfrom the center. Prior to restoration work performed between \n1990 and 2001, the tower leaned at an angle of \n5.5 degrees , // but the tower now leans at \nabout 3.99 degrees.", "chunk_type": "semantic", "chunker": "semantic_adjacent_v1"}
{"chunk_id": "drag_semantic_p2_c10", "pdf_name": "drag", "page_number": 2, "char_count": 969, "text": "// This means the top of the \nLearning Tower of Pisa is displaced horizontally \n3.9 meters (12 ft 10 in) from the center. WIkipedia\nFactoidWiki\n? ✓\nAnswer\nQuery\nQA Model\nRetrieval Units\nRetrieval \nUnits\nPassage Retrieval\nB\nC\nD\nPropositionizer\n? Query\nCorpus\nPassages\nSentences\nPropositions\nRetrieval \nUnits\nRetriever\nA\nFigure 2: We discover that segmenting and indexing a retrieval corpus on the proposition level can be a simple yet\neffective strategy to increase dense retrievers’ generalization performance at inference time (A, B). We empirically\ncompare the retrieval and downstream open-domain QA task performance when dense retrievers work with\nWikipedia indexed at the level of 100-word passages, sentences, or propositions (C, D). Based on our empirical experiments, we discover\nthat selecting the proper retrieval granularity at in-\nference time can be a simple yet effective strategy\nfor improving dense retrievers’ retrieval and down-\nstream QA performance.", "chunk_type": "semantic", "chunker": "semantic_adjacent_v1"}
{"chunk_id": "drag_semantic_p2_c11", "pdf_name": "drag", "page_number": 2, "char_count": 321, "text": "We illustrate our intuition\nwith an example of open-domain QA in Table 1. The example shows retrieved text by the same re-\ntriever at three different granularities. The pas-\nsage, which represents a coarser retrieval unit with\na longer context, is theoretically able to provide\nmore relevant information for the question.", "chunk_type": "semantic", "chunker": "semantic_adjacent_v1"}
{"chunk_id": "drag_semantic_p2_c12", "pdf_name": "drag", "page_number": 2, "char_count": 443, "text": "How-\never, a passage often includes extraneous details\n(e.g., restoration period and horizontal displace-\nment in the example of Table 1) that could poten-\ntially distract both the retriever and the language\nmodel in downstream tasks (Shi et al., 2023; Yu\net al., 2023b). On the other hand, sentence-level in-\ndexing provides a finer-grained approach but does\nnot entirely address the issue (Akkalyoncu Yilmaz\net al., 2019; Yang et al., 2020).", "chunk_type": "semantic", "chunker": "semantic_adjacent_v1"}
{"chunk_id": "drag_semantic_p2_c13", "pdf_name": "drag", "page_number": 2, "char_count": 436, "text": "This is because\nsentences can still be complex and compounded,\nand they are often not self-contained, lacking nec-\nessary contextual information (e.g., in the example\nof Table 1, “the tower” is the coreference of “Pisa\nTower”) for judging the query-document relevance. To address these shortcomings of typical re-\ntrieval units such as passages or sentences, we pro-\npose using proposition as a novel retrieval unit for\ndense retrieval.", "chunk_type": "semantic", "chunker": "semantic_adjacent_v1"}
{"chunk_id": "drag_semantic_p2_c14", "pdf_name": "drag", "page_number": 2, "char_count": 393, "text": "Propositions are defined as atomic\nexpressions within text, where each encapsulates\na distinct factoid and is presented in a concise,\nself-contained natural language format. We show\nan example proposition in Table 1. The proposi-\ntion describes the information regarding the Tower\nof Pisa’s current leaning angle in a self-contained\nway and precisely responds to what the question\nis querying.", "chunk_type": "semantic", "chunker": "semantic_adjacent_v1"}
{"chunk_id": "drag_semantic_p2_c15", "pdf_name": "drag", "page_number": 2, "char_count": 704, "text": "We provide a more detailed definition\nand description of proposition in §2. To validate\nthe efficacy of using proposition as a retrieval unit\nfor dense retrievers inference, we first process and\nindex an English Wikipedia dump with all docu-\nments segmented into propositions, which we refer\nto as FACTOIDWIKI. We conduct experiments on five different open-\ndomain QA datasets and empirically compare the\nperformance of four dual-encoder retrievers when\nWikipedia is indexed by passages, sentences, and\nour proposed propositions. Notably, our findings in-\ndicate that proposition-based retrieval outperforms\nsentence and passage-based retrieval, especially in\nterms of generalization, as discussed in §5.", "chunk_type": "semantic", "chunker": "semantic_adjacent_v1"}
{"chunk_id": "drag_semantic_p2_c16", "pdf_name": "drag", "page_number": 2, "char_count": 389, "text": "This\nsuggests that propositions, being both compact and\nrich in context, enable dense retrievers to access\nprecise information while maintaining adequate\ncontext. The average improvement over passage-\nbased retrieval of Recall@20 is +10.1 on unsuper-\nvised dense retrievers and +2.7 on supervised re-\ntrievers, even though these retrievers were directly\ntrained on passage-level retrieval.", "chunk_type": "semantic", "chunker": "semantic_adjacent_v1"}
{"chunk_id": "drag_semantic_p2_c17", "pdf_name": "drag", "page_number": 2, "char_count": 517, "text": "Furthermore,\nwe observe a distinct advantage of proposition-\nbased retrieval in downstream QA performance\nwhen using retrieval-augmented language models,\nas elaborated in §6. Retrieval by finer-grained units\ninherently provides a higher density of question-\nrelevant information. This finding implies using\nfiner-grained units in the prompts achieves the same\nperformance with a shorter input length, and hence,\na faster inference time. Our main contributions are:\n• We provide a systemic study on how retrieval\n15160", "chunk_type": "semantic", "chunker": "semantic_adjacent_v1"}
{"chunk_id": "drag_fixed_p3_c12", "pdf_name": "drag", "page_number": 3, "char_count": 913, "text": "granularity impacts retrieval and downstream\ntask performance. We observe that the retrieval\nunits have a significant impact on performance. • We introduce FACTOIDWIKI, a processed En-\nglish Wikipedia dump, where each page is seg-\nmented into multiple granularities: passages, sen-\ntences, and our proposed propositions. • We propose retrieval by proposition as an alter-\nnative strategy, which achieves better retrieval\nand QA accuracy and generalization performance\n(with unsupervised retriever), compared to pas-\nsage or sentence as retrieval unit. 2\nProposition as a Retrieval Unit\nThe goal of our study is to understand how the gran-\nularity of a retrieval corpus influences the dense\nretrieval models’ performance empirically. Aside\nfrom commonly-used retrieval units such as 100-\nword passages (Karpukhin et al., 2020) or sen-\ntences, we propose using proposition as an alterna-\ntive retrieval unit choice.", "chunk_type": "fixed", "chunker": "fixed_recursive_v1"}
{"chunk_id": "drag_fixed_p3_c13", "pdf_name": "drag", "page_number": 3, "char_count": 772, "text": "Aside\nfrom commonly-used retrieval units such as 100-\nword passages (Karpukhin et al., 2020) or sen-\ntences, we propose using proposition as an alterna-\ntive retrieval unit choice. Here, propositions repre-\nsent atomic expressions of meanings in text (Min\net al., 2023) with three defining principles below. 1. Each proposition should correspond to a distinct\npiece of meaning in text, where the composition\nof all propositions would represent the seman-\ntics of the entire text. 2. A proposition should be minimal, i.e. it cannot\nbe further split into separate propositions. 3. A proposition should be contextualized and self-\ncontained (Choi et al., 2021). A proposition\nshould include all the necessary context from the\ntext (e.g. coreference) to interpret its meaning.", "chunk_type": "fixed", "chunker": "fixed_recursive_v1"}
{"chunk_id": "drag_fixed_p3_c14", "pdf_name": "drag", "page_number": 3, "char_count": 1000, "text": "coreference) to interpret its meaning. The use of proposition as a retrieval unit is in-\nspired by a recent line of work (Min et al., 2023;\nKamoi et al., 2023; Chen et al., 2023a,b), which\nfinds success in representing and evaluating text\nsemantics at the level of propositions. We demon-\nstrate the concept of proposition and how a passage\ncan be split into a set of propositions by an example\non the left side of Figure 2. The passage contains\nthree propositions, each of which corresponds to\na distinct factoid about the Leaning Tower of Pisa:\nthe angle before the restoration, the current angle,\nand the horizontal displacement. Within each proposition, necessary context from\nthe passage is incorporated so that the meaning of\nthe proposition can be interpreted independently of\nthe original text, e.g. the reference of the tower is\n# units\nAvg. # words\nPassages\n41,393,528\n58.5\nSentences\n114,219,127\n21.0\nPropositions\n256,885,003\n11.2\nTable 1: Statistics of text units in the English Wikipedia.", "chunk_type": "fixed", "chunker": "fixed_recursive_v1"}
{"chunk_id": "drag_fixed_p3_c15", "pdf_name": "drag", "page_number": 3, "char_count": 913, "text": "# words\nPassages\n41,393,528\n58.5\nSentences\n114,219,127\n21.0\nPropositions\n256,885,003\n11.2\nTable 1: Statistics of text units in the English Wikipedia. resolved into its full mention, the Leaning Tower\nof Pisa, in the first proposition. We expect each\nproposition to describe exactly one atomic fact, and\nso our intuition is that propositions would suitably\nwork as a retrieval unit for information-seeking\nquestions. 3\nFACTOIDWIKI: Proposition-Level\nIndex and Retrieval for Wikipedia\nWe empirically compare passages, sentences, and\npropositions as retrieval units on Wikipedia, a\ncommonly-used retrieval source for knowledge-\nintensive NLP tasks (Petroni et al., 2021). To allow\na fair comparison across granularities, we process\nan English Wikipedia dump from 2021-10-13, as\nused by Bohnet et al. (2022). We segment each doc-\nument text into three different granularities: pas-\nsages, sentences, and propositions.", "chunk_type": "fixed", "chunker": "fixed_recursive_v1"}
{"chunk_id": "drag_fixed_p3_c16", "pdf_name": "drag", "page_number": 3, "char_count": 984, "text": "We segment each doc-\nument text into three different granularities: pas-\nsages, sentences, and propositions. We include the\ndetails on passage- and sentence-level segmenta-\ntion of the corpus in Appendix A. Parsing Passage to Propositions. To segment\nthe Wikipedia pages into propositions, we finetune\na text generation model, which we refer to as the\nPropositionizer. The Propositionizer takes a pas-\nsage as input and generates the list of propositions\nwithin the passage. Following Chen et al. (2023b), we train the\nPropositionizer with a two-step distillation process. We first prompt GPT-4 (OpenAI, 2023) with an\ninstruction containing the proposition definition\nand 1-shot demonstration. We include the details\nof the prompt in Figure 8. We start with a set of\n42k passages and use GPT-4 to generate the seed\nset of paragraph-to-proposition pairs. Next, we\nuse the seed set to finetune a Flan-T5-large model\n(Chung et al., 2022). We refer to the processed\ncorpus as FACTOIDWIKI.", "chunk_type": "fixed", "chunker": "fixed_recursive_v1"}
{"chunk_id": "drag_fixed_p3_c17", "pdf_name": "drag", "page_number": 3, "char_count": 353, "text": "We refer to the processed\ncorpus as FACTOIDWIKI. The statistics of FAC-\nTOIDWIKI are shown in Table 1. Quality Analysis. We conduct a manual error\nanalysis to understand the quality of propositions\ngenerated by GPT-4 and the Propositionizer. While\nthere does not exist a fixed standard on deciding\na ground truth set of propositions for a passage,\n15161", "chunk_type": "fixed", "chunker": "fixed_recursive_v1"}
{"chunk_id": "drag_semantic_p3_c18", "pdf_name": "drag", "page_number": 3, "char_count": 320, "text": "granularity impacts retrieval and downstream\ntask performance. We observe that the retrieval\nunits have a significant impact on performance. • We introduce FACTOIDWIKI, a processed En-\nglish Wikipedia dump, where each page is seg-\nmented into multiple granularities: passages, sen-\ntences, and our proposed propositions.", "chunk_type": "semantic", "chunker": "semantic_adjacent_v1"}
{"chunk_id": "drag_semantic_p3_c19", "pdf_name": "drag", "page_number": 3, "char_count": 592, "text": "• We propose retrieval by proposition as an alter-\nnative strategy, which achieves better retrieval\nand QA accuracy and generalization performance\n(with unsupervised retriever), compared to pas-\nsage or sentence as retrieval unit. 2\nProposition as a Retrieval Unit\nThe goal of our study is to understand how the gran-\nularity of a retrieval corpus influences the dense\nretrieval models’ performance empirically. Aside\nfrom commonly-used retrieval units such as 100-\nword passages (Karpukhin et al., 2020) or sen-\ntences, we propose using proposition as an alterna-\ntive retrieval unit choice.", "chunk_type": "semantic", "chunker": "semantic_adjacent_v1"}
{"chunk_id": "drag_semantic_p3_c20", "pdf_name": "drag", "page_number": 3, "char_count": 301, "text": "Here, propositions repre-\nsent atomic expressions of meanings in text (Min\net al., 2023) with three defining principles below. 1. Each proposition should correspond to a distinct\npiece of meaning in text, where the composition\nof all propositions would represent the seman-\ntics of the entire text. 2.", "chunk_type": "semantic", "chunker": "semantic_adjacent_v1"}
{"chunk_id": "drag_semantic_p3_c21", "pdf_name": "drag", "page_number": 3, "char_count": 529, "text": "A proposition should be minimal, i.e. it cannot\nbe further split into separate propositions. 3. A proposition should be contextualized and self-\ncontained (Choi et al., 2021). A proposition\nshould include all the necessary context from the\ntext (e.g. coreference) to interpret its meaning. The use of proposition as a retrieval unit is in-\nspired by a recent line of work (Min et al., 2023;\nKamoi et al., 2023; Chen et al., 2023a,b), which\nfinds success in representing and evaluating text\nsemantics at the level of propositions.", "chunk_type": "semantic", "chunker": "semantic_adjacent_v1"}
{"chunk_id": "drag_semantic_p3_c22", "pdf_name": "drag", "page_number": 3, "char_count": 353, "text": "We demon-\nstrate the concept of proposition and how a passage\ncan be split into a set of propositions by an example\non the left side of Figure 2. The passage contains\nthree propositions, each of which corresponds to\na distinct factoid about the Leaning Tower of Pisa:\nthe angle before the restoration, the current angle,\nand the horizontal displacement.", "chunk_type": "semantic", "chunker": "semantic_adjacent_v1"}
{"chunk_id": "drag_semantic_p3_c23", "pdf_name": "drag", "page_number": 3, "char_count": 367, "text": "Within each proposition, necessary context from\nthe passage is incorporated so that the meaning of\nthe proposition can be interpreted independently of\nthe original text, e.g. the reference of the tower is\n# units\nAvg. # words\nPassages\n41,393,528\n58.5\nSentences\n114,219,127\n21.0\nPropositions\n256,885,003\n11.2\nTable 1: Statistics of text units in the English Wikipedia.", "chunk_type": "semantic", "chunker": "semantic_adjacent_v1"}
{"chunk_id": "drag_semantic_p3_c24", "pdf_name": "drag", "page_number": 3, "char_count": 646, "text": "resolved into its full mention, the Leaning Tower\nof Pisa, in the first proposition. We expect each\nproposition to describe exactly one atomic fact, and\nso our intuition is that propositions would suitably\nwork as a retrieval unit for information-seeking\nquestions. 3\nFACTOIDWIKI: Proposition-Level\nIndex and Retrieval for Wikipedia\nWe empirically compare passages, sentences, and\npropositions as retrieval units on Wikipedia, a\ncommonly-used retrieval source for knowledge-\nintensive NLP tasks (Petroni et al., 2021). To allow\na fair comparison across granularities, we process\nan English Wikipedia dump from 2021-10-13, as\nused by Bohnet et al.", "chunk_type": "semantic", "chunker": "semantic_adjacent_v1"}
{"chunk_id": "drag_semantic_p3_c25", "pdf_name": "drag", "page_number": 3, "char_count": 376, "text": "(2022). We segment each doc-\nument text into three different granularities: pas-\nsages, sentences, and propositions. We include the\ndetails on passage- and sentence-level segmenta-\ntion of the corpus in Appendix A. Parsing Passage to Propositions. To segment\nthe Wikipedia pages into propositions, we finetune\na text generation model, which we refer to as the\nPropositionizer.", "chunk_type": "semantic", "chunker": "semantic_adjacent_v1"}
{"chunk_id": "drag_semantic_p3_c26", "pdf_name": "drag", "page_number": 3, "char_count": 324, "text": "The Propositionizer takes a pas-\nsage as input and generates the list of propositions\nwithin the passage. Following Chen et al. (2023b), we train the\nPropositionizer with a two-step distillation process. We first prompt GPT-4 (OpenAI, 2023) with an\ninstruction containing the proposition definition\nand 1-shot demonstration.", "chunk_type": "semantic", "chunker": "semantic_adjacent_v1"}
{"chunk_id": "drag_semantic_p3_c27", "pdf_name": "drag", "page_number": 3, "char_count": 595, "text": "We include the details\nof the prompt in Figure 8. We start with a set of\n42k passages and use GPT-4 to generate the seed\nset of paragraph-to-proposition pairs. Next, we\nuse the seed set to finetune a Flan-T5-large model\n(Chung et al., 2022). We refer to the processed\ncorpus as FACTOIDWIKI. The statistics of FAC-\nTOIDWIKI are shown in Table 1. Quality Analysis. We conduct a manual error\nanalysis to understand the quality of propositions\ngenerated by GPT-4 and the Propositionizer. While\nthere does not exist a fixed standard on deciding\na ground truth set of propositions for a passage,\n15161", "chunk_type": "semantic", "chunker": "semantic_adjacent_v1"}
{"chunk_id": "drag_fixed_p4_c18", "pdf_name": "drag", "page_number": 4, "char_count": 893, "text": "GPT-4\nPropositionizer\nNot Faithful\n0.7% (3/408)\n1.3% (6/445)\nNot Minimal\n2.9% (12/408)\n2.0% (9/445)\nNot Stand-alone\n4.9% (20/408)\n3.1% (14/445)\nTable 2: Frequency of errors occurred in the generated\npropositions. Most generated propositions are faithful,\nwhile a small portion of them are not stand-alone. we estimate the frequency of error cases where\n(1) a proposition is not fully supported by the pas-\nsage, (2) a proposition can be further split into\nseparate propositions, and (3) propositions are not\nself-contained, respectively (Table 2). On a random\nsample of 50 passages, we observe that almost all\npropositions generated by both models are faithful,\nwhile a small portion of the propositions are not\nstand-alone. 4\nExperimental Settings\nTo evaluate the impact of the three retrieval unit\nchoices, we conduct experiments on five differ-\nent open-domain QA datasets with FACTOIDWIKI.", "chunk_type": "fixed", "chunker": "fixed_recursive_v1"}
{"chunk_id": "drag_fixed_p4_c19", "pdf_name": "drag", "page_number": 4, "char_count": 939, "text": "4\nExperimental Settings\nTo evaluate the impact of the three retrieval unit\nchoices, we conduct experiments on five differ-\nent open-domain QA datasets with FACTOIDWIKI. With each dataset, we evaluate both passage re-\ntrieval and downstream QA performance when\ndense retrievers work with Wikipedia indexed in\ndifferent granularities. 4.1\nOpen-Domain QA Datasets\nWe experiment on five different open-domain QA\ndatasets with Wikipedia as the retrieval source: Nat-\nural Questions (NQ, Kwiatkowski et al., 2019),\nTriviaQA (TQA, Joshi et al., 2017), Web Ques-\ntions (WebQ, Berant et al., 2013), SQuAD (Ra-\njpurkar et al., 2016), and Entity Questions (EQ,\nSciavolino et al., 2021). 4.2\nDense Retrieval Models\nWe compare the performance of the four following\nsupervised or unsupervised dense retriever mod-\nels. Here, supervised models refer to ones that\nhave used human-labeled query-passage pairs as\nsupervision during training, and vice versa.", "chunk_type": "fixed", "chunker": "fixed_recursive_v1"}
{"chunk_id": "drag_fixed_p4_c20", "pdf_name": "drag", "page_number": 4, "char_count": 981, "text": "Here, supervised models refer to ones that\nhave used human-labeled query-passage pairs as\nsupervision during training, and vice versa. • SimCSE (Gao et al., 2021) is a BERT-base (De-\nvlin et al., 2019) encoder trained on unlabeled\nsentences sampled randomly from Wikipedia. SimCSE can be transferred to use as an unsu-\npervised retriever (Chen et al., 2023b). • Contriever (Izacard et al., 2022) is an unsuper-\nvised retriever, instantiated with a BERT-base\nencoder. Contriever is contrastively trained by\nsegment pairs constructed from unlabeled docu-\nments from Wikipedia and web crawl data. • DPR (Karpukhin et al., 2020) is a dual-encoder\nBERT-base model fine-tuned on passage retrieval\ntasks directly using the question-passage pair la-\nbels from NQ, TQA, WebQ and SQuAD. • GTR (Ni et al., 2022) is a T5-base encoder (Raf-\nfel et al., 2020) pretrained on online forum QA\ndata, and fine-tuned with question-passage pair\nlabels on MS MARCO (Nguyen et al., 2016) and\nNQ datasets.", "chunk_type": "fixed", "chunker": "fixed_recursive_v1"}
{"chunk_id": "drag_fixed_p4_c21", "pdf_name": "drag", "page_number": 4, "char_count": 961, "text": "R (Ni et al., 2022) is a T5-base encoder (Raf-\nfel et al., 2020) pretrained on online forum QA\ndata, and fine-tuned with question-passage pair\nlabels on MS MARCO (Nguyen et al., 2016) and\nNQ datasets. 4.3\nPassage Retrieval Evaluation\nWe evaluate the retrieval performance at the pas-\nsage level when the corpus is indexed at the pas-\nsage, sentence, or proposition level respectively. For sentence and proposition level retrieval, we\nfollow the setting introduced in Lee et al. (2021b),\nwhere the score of the passage is based on the max-\nimum similarity score between the query and all\nsentences or propositions in a passage. In practice,\nwe first retrieve a slightly larger number of text\nunits, then map each unit to the source passage,\nand eventually return the top-k unique passages. We use Passage Recall@k as our evaluation metric,\nwhich is defined as the percentage of questions for\nwhich the correct answer is found within the top-k\nretrieved passages.", "chunk_type": "fixed", "chunker": "fixed_recursive_v1"}
{"chunk_id": "drag_fixed_p4_c22", "pdf_name": "drag", "page_number": 4, "char_count": 939, "text": "We use Passage Recall@k as our evaluation metric,\nwhich is defined as the percentage of questions for\nwhich the correct answer is found within the top-k\nretrieved passages. To further understand how different retrieved\npassages affect the downstream QA. We use Fusion-\nin-Decoder (FiD, Izacard and Grave, 2021) model\nto extract answers from retrieved passages. We use\na T5-large sized FiD model trained on NQ dataset\nin our experiments. The exact match (EM) score\ncomputes the percentage of questions for which the\npredicted answer exactly matches the ground truth. 4.4\nOpen-domain QA Evaluation on\nRetrieval-Augmented Language Models\nAnother aspect of the choice of granularity lies\nin what units should be used in the prompt for\nretrieval-augmented language models. For large\nlanguage models, retrieval-augmented generation\nis achieved by prepending retrieved units to user in-\nstruction and taking them as the input for language\nmodels.", "chunk_type": "fixed", "chunker": "fixed_recursive_v1"}
{"chunk_id": "drag_fixed_p4_c23", "pdf_name": "drag", "page_number": 4, "char_count": 466, "text": "For large\nlanguage models, retrieval-augmented generation\nis achieved by prepending retrieved units to user in-\nstruction and taking them as the input for language\nmodels. We aim to understand the implications of\nusing retrieved units of different granularity within\nthe same computational budget at inference time. To fairly compare using different granularity in the\nprompts under the same computation budget, we\nset a token length limit for retrieved units. 15162", "chunk_type": "fixed", "chunker": "fixed_recursive_v1"}
{"chunk_id": "drag_semantic_p4_c28", "pdf_name": "drag", "page_number": 4, "char_count": 305, "text": "GPT-4\nPropositionizer\nNot Faithful\n0.7% (3/408)\n1.3% (6/445)\nNot Minimal\n2.9% (12/408)\n2.0% (9/445)\nNot Stand-alone\n4.9% (20/408)\n3.1% (14/445)\nTable 2: Frequency of errors occurred in the generated\npropositions. Most generated propositions are faithful,\nwhile a small portion of them are not stand-alone.", "chunk_type": "semantic", "chunker": "semantic_adjacent_v1"}
{"chunk_id": "drag_semantic_p4_c29", "pdf_name": "drag", "page_number": 4, "char_count": 418, "text": "we estimate the frequency of error cases where\n(1) a proposition is not fully supported by the pas-\nsage, (2) a proposition can be further split into\nseparate propositions, and (3) propositions are not\nself-contained, respectively (Table 2). On a random\nsample of 50 passages, we observe that almost all\npropositions generated by both models are faithful,\nwhile a small portion of the propositions are not\nstand-alone.", "chunk_type": "semantic", "chunker": "semantic_adjacent_v1"}
{"chunk_id": "drag_semantic_p4_c30", "pdf_name": "drag", "page_number": 4, "char_count": 675, "text": "4\nExperimental Settings\nTo evaluate the impact of the three retrieval unit\nchoices, we conduct experiments on five differ-\nent open-domain QA datasets with FACTOIDWIKI. With each dataset, we evaluate both passage re-\ntrieval and downstream QA performance when\ndense retrievers work with Wikipedia indexed in\ndifferent granularities. 4.1\nOpen-Domain QA Datasets\nWe experiment on five different open-domain QA\ndatasets with Wikipedia as the retrieval source: Nat-\nural Questions (NQ, Kwiatkowski et al., 2019),\nTriviaQA (TQA, Joshi et al., 2017), Web Ques-\ntions (WebQ, Berant et al., 2013), SQuAD (Ra-\njpurkar et al., 2016), and Entity Questions (EQ,\nSciavolino et al., 2021).", "chunk_type": "semantic", "chunker": "semantic_adjacent_v1"}
{"chunk_id": "drag_semantic_p4_c31", "pdf_name": "drag", "page_number": 4, "char_count": 402, "text": "4.2\nDense Retrieval Models\nWe compare the performance of the four following\nsupervised or unsupervised dense retriever mod-\nels. Here, supervised models refer to ones that\nhave used human-labeled query-passage pairs as\nsupervision during training, and vice versa. • SimCSE (Gao et al., 2021) is a BERT-base (De-\nvlin et al., 2019) encoder trained on unlabeled\nsentences sampled randomly from Wikipedia.", "chunk_type": "semantic", "chunker": "semantic_adjacent_v1"}
{"chunk_id": "drag_semantic_p4_c32", "pdf_name": "drag", "page_number": 4, "char_count": 319, "text": "SimCSE can be transferred to use as an unsu-\npervised retriever (Chen et al., 2023b). • Contriever (Izacard et al., 2022) is an unsuper-\nvised retriever, instantiated with a BERT-base\nencoder. Contriever is contrastively trained by\nsegment pairs constructed from unlabeled docu-\nments from Wikipedia and web crawl data.", "chunk_type": "semantic", "chunker": "semantic_adjacent_v1"}
{"chunk_id": "drag_semantic_p4_c33", "pdf_name": "drag", "page_number": 4, "char_count": 387, "text": "• DPR (Karpukhin et al., 2020) is a dual-encoder\nBERT-base model fine-tuned on passage retrieval\ntasks directly using the question-passage pair la-\nbels from NQ, TQA, WebQ and SQuAD. • GTR (Ni et al., 2022) is a T5-base encoder (Raf-\nfel et al., 2020) pretrained on online forum QA\ndata, and fine-tuned with question-passage pair\nlabels on MS MARCO (Nguyen et al., 2016) and\nNQ datasets.", "chunk_type": "semantic", "chunker": "semantic_adjacent_v1"}
{"chunk_id": "drag_semantic_p4_c34", "pdf_name": "drag", "page_number": 4, "char_count": 425, "text": "4.3\nPassage Retrieval Evaluation\nWe evaluate the retrieval performance at the pas-\nsage level when the corpus is indexed at the pas-\nsage, sentence, or proposition level respectively. For sentence and proposition level retrieval, we\nfollow the setting introduced in Lee et al. (2021b),\nwhere the score of the passage is based on the max-\nimum similarity score between the query and all\nsentences or propositions in a passage.", "chunk_type": "semantic", "chunker": "semantic_adjacent_v1"}
{"chunk_id": "drag_semantic_p4_c35", "pdf_name": "drag", "page_number": 4, "char_count": 334, "text": "In practice,\nwe first retrieve a slightly larger number of text\nunits, then map each unit to the source passage,\nand eventually return the top-k unique passages. We use Passage Recall@k as our evaluation metric,\nwhich is defined as the percentage of questions for\nwhich the correct answer is found within the top-k\nretrieved passages.", "chunk_type": "semantic", "chunker": "semantic_adjacent_v1"}
{"chunk_id": "drag_semantic_p4_c36", "pdf_name": "drag", "page_number": 4, "char_count": 392, "text": "To further understand how different retrieved\npassages affect the downstream QA. We use Fusion-\nin-Decoder (FiD, Izacard and Grave, 2021) model\nto extract answers from retrieved passages. We use\na T5-large sized FiD model trained on NQ dataset\nin our experiments. The exact match (EM) score\ncomputes the percentage of questions for which the\npredicted answer exactly matches the ground truth.", "chunk_type": "semantic", "chunker": "semantic_adjacent_v1"}
{"chunk_id": "drag_semantic_p4_c37", "pdf_name": "drag", "page_number": 4, "char_count": 668, "text": "4.4\nOpen-domain QA Evaluation on\nRetrieval-Augmented Language Models\nAnother aspect of the choice of granularity lies\nin what units should be used in the prompt for\nretrieval-augmented language models. For large\nlanguage models, retrieval-augmented generation\nis achieved by prepending retrieved units to user in-\nstruction and taking them as the input for language\nmodels. We aim to understand the implications of\nusing retrieved units of different granularity within\nthe same computational budget at inference time. To fairly compare using different granularity in the\nprompts under the same computation budget, we\nset a token length limit for retrieved units. 15162", "chunk_type": "semantic", "chunker": "semantic_adjacent_v1"}
{"chunk_id": "drag_fixed_p5_c24", "pdf_name": "drag", "page_number": 5, "char_count": 47, "text": "Retriever\nGranularity\nNQ\nTQA\nWebQ\nSQuAD\nEQ\nAvg.", "chunk_type": "fixed", "chunker": "fixed_recursive_v1"}
{"chunk_id": "drag_fixed_p5_c25", "pdf_name": "drag", "page_number": 5, "char_count": 1224, "text": "Retriever\nGranularity\nNQ\nTQA\nWebQ\nSQuAD\nEQ\nAvg. R@5\nR@20\nR@5\nR@20\nR@5\nR@20\nR@5\nR@20\nR@5\nR@20\nR@5\nR@20\nUnsupervised Dense Retrievers\nSimCSE\nPassage\n28.8\n44.3\n44.9\n59.4\n39.8\n56.0\n29.5\n45.5\n28.4\n40.3\n34.3\n49.1\nSentence\n35.5\n53.1\n50.5\n64.3\n45.3\n64.1\n37.1\n52.3\n36.3\n50.1\n40.9\n56.8\nProposition\n41.1\n58.9\n52.4\n66.5\n50.0\n66.8\n38.7\n53.9\n49.5\n62.2\n46.3\n61.7\nContriever\nPassage\n42.5\n63.8\n58.1\n73.7\n37.1\n60.6\n40.8\n59.8\n36.3\n56.3\n43.0\n62.8\nSentence\n46.4\n66.8\n60.6\n75.7\n41.7\n63.1\n45.1\n63.5\n42.7\n61.3\n47.3\n66.1\nProposition\n50.1\n70.0\n65.1\n77.9\n45.9\n66.8\n50.7\n67.7\n51.7\n70.1\n52.7\n70.5\nSupervised Dense Retrievers\nDPR\nPassage\n66.0\n78.0\n71.6\n80.2\n62.9\n74.9\n38.3\n53.9\n47.5\n60.4\n57.3\n69.5\nSentence\n66.0\n78.0\n71.8\n80.5\n64.1\n74.4\n40.3\n55.9\n53.7\n66.0\n59.2\n71.0\nProposition\n65.4\n77.7\n70.7\n79.6\n62.8\n75.1\n41.4\n57.2\n59.4\n71.3\n59.9\n72.2\nGTR\nPassage\n66.3\n78.4\n70.1\n79.4\n63.3\n76.5\n54.4\n68.1\n71.7\n80.5\n65.2\n76.6\nSentence\n66.4\n79.4\n71.6\n80.9\n62.2\n76.8\n60.9\n73.4\n72.5\n81.3\n66.7\n78.4\nProposition\n66.5\n79.6\n72.2\n80.9\n63.2\n77.4\n63.3\n75.0\n74.9\n83.0\n68.0\n79.2\nTable 3: Passage retrieval performance (Recall@k = 5, 20) on five different open-domain QA datasets when\npre-trained dense retrievers work with the three different granularity from the retrieval corpus.", "chunk_type": "fixed", "chunker": "fixed_recursive_v1"}
{"chunk_id": "drag_fixed_p5_c26", "pdf_name": "drag", "page_number": 5, "char_count": 954, "text": "ble 3: Passage retrieval performance (Recall@k = 5, 20) on five different open-domain QA datasets when\npre-trained dense retrievers work with the three different granularity from the retrieval corpus. Underline denotes\ncases where the training split of the target dataset was included in the training data of the dense retriever. For this reason, we follow an evaluation setup\nwhere the maximum number of retrieved tokens\nis capped at l = 100 or 500, i.e. only the top\nl tokens from passage, sentence, or proposition\nlevel retrieval are fed into the language model as\ninput. We evaluate the percentage of questions for\nwhich the predicted answer exactly matches (EM)\nthe ground truth. We denote our metric as EM @\nl tokens. We use LLaMA-2-7B (Touvron et al.,\n2023) in our evaluation. To ensure the model’s out-\nput aligns with the format of each dataset, we em-\nploy in-context learning, incorporating four-shot\ndemonstrations as illustrated in Figure 9.", "chunk_type": "fixed", "chunker": "fixed_recursive_v1"}
{"chunk_id": "drag_fixed_p5_c27", "pdf_name": "drag", "page_number": 5, "char_count": 939, "text": "To ensure the model’s out-\nput aligns with the format of each dataset, we em-\nploy in-context learning, incorporating four-shot\ndemonstrations as illustrated in Figure 9. 5\nHow Does Granularity Influence\nPassage Retrieval? In this section, we report and discuss how index-\ning the corpus at various granularity influences the\npassage retrieval performance. Surprisingly, de-\nspite all of the dense retrieval models being trained\non only passage-level documents, all the models\ndemonstrate on-par or superior performance when\nthe corpus is indexed at the proposition level. Our\nresults suggest that indexing the corpus at the finer-\ngrained units improves the cross-task generaliza-\ntion on passage retrieval. 5.1\nPassage Retrieval Performance\nWe report our evaluation results in Table 3. We\nobserve that retrieval by propositions outperforms\nretrieval by sentences or passages on most tasks for\nboth unsupervised and supervised retrievers.", "chunk_type": "fixed", "chunker": "fixed_recursive_v1"}
{"chunk_id": "drag_fixed_p5_c28", "pdf_name": "drag", "page_number": 5, "char_count": 971, "text": "We\nobserve that retrieval by propositions outperforms\nretrieval by sentences or passages on most tasks for\nboth unsupervised and supervised retrievers. With all dense retrievers tested, proposition-\nlevel retrieval consistently outperforms sentence\nand passage-level retrieval on average across the\nfive datasets. With the unsupervised retrievers, i.e. SimCSE and Contriever, we see an averaged Re-\ncall@5 improvement of +12.0 and +9.3 (35.0%\nand 22.5% relative improvement) on five datasets. With the supervised retrievers, proposition-level\nretrieval still shows an advantage on average, yet\nthe sizes of improvements are smaller. We hypothe-\nsize that this is due to these retrievers being trained\non query-passage pairs. For instance, with DPR,\nwhich have been trained on NQ, TQA, WebQ, and\nSQuAD, we observe that proposition and sentence\nlevel retrieval perform slightly worse compared to\npassage level on three out of the four datasets, with\nthe exception of SQuAD.", "chunk_type": "fixed", "chunker": "fixed_recursive_v1"}
{"chunk_id": "drag_fixed_p5_c29", "pdf_name": "drag", "page_number": 5, "char_count": 884, "text": "d on NQ, TQA, WebQ, and\nSQuAD, we observe that proposition and sentence\nlevel retrieval perform slightly worse compared to\npassage level on three out of the four datasets, with\nthe exception of SQuAD. As shown in Table 3, all\nsupervised retrievers demonstrate comparable per-\nformance across three levels of retrieval granularity\nin NQ, TQA, and WebQ. However, on datasets that the retriever model has\nnot seen during training, we observe that retrieval\nby proposition demonstrates a clear advantage. For\ninstance, most notably on SQuAD or EntityQues-\ntions, we observe that proposition-based retrieval\nsignificantly outperforms the other two granulari-\nties. We see 25% Recall@5 relative improvement\non EntityQuestions with relatively weak retrievers\nlike DPR. Furthermore, the Recall@5 of retrieval\nby proposition on SQuAD improved most on GTR,\nwith 16% relative improvements. 15163", "chunk_type": "fixed", "chunker": "fixed_recursive_v1"}
{"chunk_id": "drag_semantic_p5_c38", "pdf_name": "drag", "page_number": 5, "char_count": 1224, "text": "Retriever\nGranularity\nNQ\nTQA\nWebQ\nSQuAD\nEQ\nAvg. R@5\nR@20\nR@5\nR@20\nR@5\nR@20\nR@5\nR@20\nR@5\nR@20\nR@5\nR@20\nUnsupervised Dense Retrievers\nSimCSE\nPassage\n28.8\n44.3\n44.9\n59.4\n39.8\n56.0\n29.5\n45.5\n28.4\n40.3\n34.3\n49.1\nSentence\n35.5\n53.1\n50.5\n64.3\n45.3\n64.1\n37.1\n52.3\n36.3\n50.1\n40.9\n56.8\nProposition\n41.1\n58.9\n52.4\n66.5\n50.0\n66.8\n38.7\n53.9\n49.5\n62.2\n46.3\n61.7\nContriever\nPassage\n42.5\n63.8\n58.1\n73.7\n37.1\n60.6\n40.8\n59.8\n36.3\n56.3\n43.0\n62.8\nSentence\n46.4\n66.8\n60.6\n75.7\n41.7\n63.1\n45.1\n63.5\n42.7\n61.3\n47.3\n66.1\nProposition\n50.1\n70.0\n65.1\n77.9\n45.9\n66.8\n50.7\n67.7\n51.7\n70.1\n52.7\n70.5\nSupervised Dense Retrievers\nDPR\nPassage\n66.0\n78.0\n71.6\n80.2\n62.9\n74.9\n38.3\n53.9\n47.5\n60.4\n57.3\n69.5\nSentence\n66.0\n78.0\n71.8\n80.5\n64.1\n74.4\n40.3\n55.9\n53.7\n66.0\n59.2\n71.0\nProposition\n65.4\n77.7\n70.7\n79.6\n62.8\n75.1\n41.4\n57.2\n59.4\n71.3\n59.9\n72.2\nGTR\nPassage\n66.3\n78.4\n70.1\n79.4\n63.3\n76.5\n54.4\n68.1\n71.7\n80.5\n65.2\n76.6\nSentence\n66.4\n79.4\n71.6\n80.9\n62.2\n76.8\n60.9\n73.4\n72.5\n81.3\n66.7\n78.4\nProposition\n66.5\n79.6\n72.2\n80.9\n63.2\n77.4\n63.3\n75.0\n74.9\n83.0\n68.0\n79.2\nTable 3: Passage retrieval performance (Recall@k = 5, 20) on five different open-domain QA datasets when\npre-trained dense retrievers work with the three different granularity from the retrieval corpus.", "chunk_type": "semantic", "chunker": "semantic_adjacent_v1"}
{"chunk_id": "drag_semantic_p5_c39", "pdf_name": "drag", "page_number": 5, "char_count": 373, "text": "Underline denotes\ncases where the training split of the target dataset was included in the training data of the dense retriever. For this reason, we follow an evaluation setup\nwhere the maximum number of retrieved tokens\nis capped at l = 100 or 500, i.e. only the top\nl tokens from passage, sentence, or proposition\nlevel retrieval are fed into the language model as\ninput.", "chunk_type": "semantic", "chunker": "semantic_adjacent_v1"}
{"chunk_id": "drag_semantic_p5_c40", "pdf_name": "drag", "page_number": 5, "char_count": 379, "text": "We evaluate the percentage of questions for\nwhich the predicted answer exactly matches (EM)\nthe ground truth. We denote our metric as EM @\nl tokens. We use LLaMA-2-7B (Touvron et al.,\n2023) in our evaluation. To ensure the model’s out-\nput aligns with the format of each dataset, we em-\nploy in-context learning, incorporating four-shot\ndemonstrations as illustrated in Figure 9.", "chunk_type": "semantic", "chunker": "semantic_adjacent_v1"}
{"chunk_id": "drag_semantic_p5_c41", "pdf_name": "drag", "page_number": 5, "char_count": 616, "text": "5\nHow Does Granularity Influence\nPassage Retrieval? In this section, we report and discuss how index-\ning the corpus at various granularity influences the\npassage retrieval performance. Surprisingly, de-\nspite all of the dense retrieval models being trained\non only passage-level documents, all the models\ndemonstrate on-par or superior performance when\nthe corpus is indexed at the proposition level. Our\nresults suggest that indexing the corpus at the finer-\ngrained units improves the cross-task generaliza-\ntion on passage retrieval. 5.1\nPassage Retrieval Performance\nWe report our evaluation results in Table 3.", "chunk_type": "semantic", "chunker": "semantic_adjacent_v1"}
{"chunk_id": "drag_semantic_p5_c42", "pdf_name": "drag", "page_number": 5, "char_count": 313, "text": "We\nobserve that retrieval by propositions outperforms\nretrieval by sentences or passages on most tasks for\nboth unsupervised and supervised retrievers. With all dense retrievers tested, proposition-\nlevel retrieval consistently outperforms sentence\nand passage-level retrieval on average across the\nfive datasets.", "chunk_type": "semantic", "chunker": "semantic_adjacent_v1"}
{"chunk_id": "drag_semantic_p5_c43", "pdf_name": "drag", "page_number": 5, "char_count": 318, "text": "With the unsupervised retrievers, i.e. SimCSE and Contriever, we see an averaged Re-\ncall@5 improvement of +12.0 and +9.3 (35.0%\nand 22.5% relative improvement) on five datasets. With the supervised retrievers, proposition-level\nretrieval still shows an advantage on average, yet\nthe sizes of improvements are smaller.", "chunk_type": "semantic", "chunker": "semantic_adjacent_v1"}
{"chunk_id": "drag_semantic_p5_c44", "pdf_name": "drag", "page_number": 5, "char_count": 638, "text": "We hypothe-\nsize that this is due to these retrievers being trained\non query-passage pairs. For instance, with DPR,\nwhich have been trained on NQ, TQA, WebQ, and\nSQuAD, we observe that proposition and sentence\nlevel retrieval perform slightly worse compared to\npassage level on three out of the four datasets, with\nthe exception of SQuAD. As shown in Table 3, all\nsupervised retrievers demonstrate comparable per-\nformance across three levels of retrieval granularity\nin NQ, TQA, and WebQ. However, on datasets that the retriever model has\nnot seen during training, we observe that retrieval\nby proposition demonstrates a clear advantage.", "chunk_type": "semantic", "chunker": "semantic_adjacent_v1"}
{"chunk_id": "drag_semantic_p5_c45", "pdf_name": "drag", "page_number": 5, "char_count": 383, "text": "For\ninstance, most notably on SQuAD or EntityQues-\ntions, we observe that proposition-based retrieval\nsignificantly outperforms the other two granulari-\nties. We see 25% Recall@5 relative improvement\non EntityQuestions with relatively weak retrievers\nlike DPR. Furthermore, the Recall@5 of retrieval\nby proposition on SQuAD improved most on GTR,\nwith 16% relative improvements. 15163", "chunk_type": "semantic", "chunker": "semantic_adjacent_v1"}
{"chunk_id": "drag_fixed_p6_c30", "pdf_name": "drag", "page_number": 6, "char_count": 703, "text": "101\n102\n103\nFrequency\n20\n40\n60\nRecall@5\nSimCSE\n101\n102\n103\nFrequency\n20\n40\n60\nRecall@5\nContriever\n101\n102\n103\nFrequency\n40\n50\n60\nRecall@5\nDPR\n101\n102\n103\nFrequency\n50\n60\n70\n80\nRecall@5\nGTR\nPassage\nSentence\nProposition\nFigure 3: Document retrieval recall vs. the frequency of the target entity in each question from the Entity Questions\ndataset. The frequency of each entity (i.e. smaller value ⇒less common entities, and vice versa) is estimated by\nthe frequency of the entity in its top-1000 passage retrieved by BM25. On queries with less common entities, we\nobserve that retrieving by proposition shows a larger advantage over retrieval by proposition. Retriever\nGranularity\nNQ\nTQA\nWebQ\nSQuAD\nEQ\nAvg.", "chunk_type": "fixed", "chunker": "fixed_recursive_v1"}
{"chunk_id": "drag_fixed_p6_c31", "pdf_name": "drag", "page_number": 6, "char_count": 1265, "text": "Retriever\nGranularity\nNQ\nTQA\nWebQ\nSQuAD\nEQ\nAvg. top-5\ntop-20\ntop-5\ntop-20\ntop-5\ntop-20\ntop-5\ntop-20\ntop-5\ntop-20\ntop-5\ntop-20\nUnsupervised Dense Retrievers\nSimCSE\nPassage\n16.6\n23.6\n32.3\n40.8\n15.5\n19.1\n14.6\n20.7\n16.1\n20.3\n19.0\n24.9\nSentence\n20.7\n28.1\n36.0\n44.5\n18.5\n21.9\n19.6\n25.8\n19.9\n25.1\n23.0\n29.1\nProposition\n24.5\n33.1\n37.5\n46.2\n19.7\n23.0\n21.4\n27.6\n26.8\n32.0\n26.0\n32.4\nContriever\nPassage\n23.2\n35.1\n40.8\n50.8\n16.3\n22.1\n23.9\n32.7\n20.2\n27.9\n24.9\n33.7\nSentence\n26.0\n36.8\n43.4\n52.9\n18.4\n23.9\n26.7\n34.7\n23.7\n30.3\n27.6\n35.7\nProposition\n28.9\n39.2\n47.2\n55.6\n19.5\n25.2\n30.8\n37.6\n28.8\n35.8\n31.1\n38.7\nSupervised Dense Retrievers\nDPR\nPassage\n41.1\n45.6\n50.6\n57.0\n23.7\n25.5\n18.8\n25.4\n25.3\n29.7\n31.9\n36.6\nSentence\n40.3\n45.6\n51.7\n57.6\n24.0\n26.9\n21.1\n27.4\n28.6\n32.9\n33.1\n38.1\nProposition\n39.7\n45.2\n51.0\n56.8\n24.3\n27.5\n22.2\n28.3\n32.0\n36.0\n33.9\n38.8\nGTR\nPassage\n39.8\n46.1\n49.7\n55.9\n23.0\n25.9\n29.9\n35.1\n37.8\n39.6\n36.0\n40.5\nSentence\n39.4\n45.9\n51.7\n58.0\n23.2\n26.1\n35.7\n39.1\n38.0\n39.9\n37.6\n41.8\nProposition\n40.0\n46.9\n52.5\n58.4\n24.2\n26.5\n37.8\n40.4\n39.2\n41.0\n38.7\n42.6\nTable 4: Open-domain QA performance (Exact Match) using Fusion-in-Decoder model (Izacard and Grave, 2021)\nto extract answer from top-5 and top-20 passages retrieved on the index of passages, sentences, and propositions.", "chunk_type": "fixed", "chunker": "fixed_recursive_v1"}
{"chunk_id": "drag_fixed_p6_c32", "pdf_name": "drag", "page_number": 6, "char_count": 968, "text": "n QA performance (Exact Match) using Fusion-in-Decoder model (Izacard and Grave, 2021)\nto extract answer from top-5 and top-20 passages retrieved on the index of passages, sentences, and propositions. 5.2\nRetrieval on Finer-grained Index ⇒\nBetter Cross-Task Generalization\nOur results show the advantage of retrieval on\nproposition-level index in cross-task generalization\nsettings. We observe that on SQuAD and Entity\nQuestions, retrieval on the proposition-level index\nbrings more performance gain over the passage-\nlevel index and sentence-level index. To better understand where the improvements\ncan be attributed, we conduct an additional analysis\non Entity Questions. As Entity Questions features\nquestions targeting the properties of longer-tail enti-\nties, we study how the retrieval performance under\nthree different granularities is affected by the occu-\nrance of the target entity in question, i.e. whether\nthe entity appears frequently in Wikipedia or not.", "chunk_type": "fixed", "chunker": "fixed_recursive_v1"}
{"chunk_id": "drag_fixed_p6_c33", "pdf_name": "drag", "page_number": 6, "char_count": 959, "text": "whether\nthe entity appears frequently in Wikipedia or not. We estimate the frequency of each entity with the\nfollowing method. Given the surface form of an en-\ntity, we use BM25 to retrieve the top 1000 relevant\npassages from Wikipedia. We use the number of\noccurrences of the entity in its relevant passages as\nan estimate of its frequency. With the 20,000 test\nqueries, around 25% of the target entities have an\nfrequency value of less or equal to 3. Figure 3 shows the passage retrieval perfor-\nmance vs. the frequency of the target entity in\neach question. Across all four dense retrievers,\nwe observe that retrieving by proposition shows a\nmuch larger advantage over retrieving by passages\nwith questions targeting less common entities. As\nthe frequency of entities increases, the performance\ngap decreases. Our findings indicate that the per-\nformance gain from retrieval by proposition can\nmostly be attributed to queries for long-tailed infor-\nmation.", "chunk_type": "fixed", "chunker": "fixed_recursive_v1"}
{"chunk_id": "drag_fixed_p6_c34", "pdf_name": "drag", "page_number": 6, "char_count": 546, "text": "Our findings indicate that the per-\nformance gain from retrieval by proposition can\nmostly be attributed to queries for long-tailed infor-\nmation. This echoes our observation that retrieval\non proposition-level index improves the cross-task\ngeneralization performance of dense retrievers. 5.3\nHigher Passage Recall ⇒Higher\nDownstream QA Accuracy\nTo further understand whether the passage retrieval\non a finer-grained index achieves higher down-\nstream QA performance, we extract the answer\nfrom the retrieved passage by a QA reader, Fusion-\n15164", "chunk_type": "fixed", "chunker": "fixed_recursive_v1"}
{"chunk_id": "drag_semantic_p6_c46", "pdf_name": "drag", "page_number": 6, "char_count": 519, "text": "101\n102\n103\nFrequency\n20\n40\n60\nRecall@5\nSimCSE\n101\n102\n103\nFrequency\n20\n40\n60\nRecall@5\nContriever\n101\n102\n103\nFrequency\n40\n50\n60\nRecall@5\nDPR\n101\n102\n103\nFrequency\n50\n60\n70\n80\nRecall@5\nGTR\nPassage\nSentence\nProposition\nFigure 3: Document retrieval recall vs. the frequency of the target entity in each question from the Entity Questions\ndataset. The frequency of each entity (i.e. smaller value ⇒less common entities, and vice versa) is estimated by\nthe frequency of the entity in its top-1000 passage retrieved by BM25.", "chunk_type": "semantic", "chunker": "semantic_adjacent_v1"}
{"chunk_id": "drag_semantic_p6_c47", "pdf_name": "drag", "page_number": 6, "char_count": 1401, "text": "On queries with less common entities, we\nobserve that retrieving by proposition shows a larger advantage over retrieval by proposition. Retriever\nGranularity\nNQ\nTQA\nWebQ\nSQuAD\nEQ\nAvg. top-5\ntop-20\ntop-5\ntop-20\ntop-5\ntop-20\ntop-5\ntop-20\ntop-5\ntop-20\ntop-5\ntop-20\nUnsupervised Dense Retrievers\nSimCSE\nPassage\n16.6\n23.6\n32.3\n40.8\n15.5\n19.1\n14.6\n20.7\n16.1\n20.3\n19.0\n24.9\nSentence\n20.7\n28.1\n36.0\n44.5\n18.5\n21.9\n19.6\n25.8\n19.9\n25.1\n23.0\n29.1\nProposition\n24.5\n33.1\n37.5\n46.2\n19.7\n23.0\n21.4\n27.6\n26.8\n32.0\n26.0\n32.4\nContriever\nPassage\n23.2\n35.1\n40.8\n50.8\n16.3\n22.1\n23.9\n32.7\n20.2\n27.9\n24.9\n33.7\nSentence\n26.0\n36.8\n43.4\n52.9\n18.4\n23.9\n26.7\n34.7\n23.7\n30.3\n27.6\n35.7\nProposition\n28.9\n39.2\n47.2\n55.6\n19.5\n25.2\n30.8\n37.6\n28.8\n35.8\n31.1\n38.7\nSupervised Dense Retrievers\nDPR\nPassage\n41.1\n45.6\n50.6\n57.0\n23.7\n25.5\n18.8\n25.4\n25.3\n29.7\n31.9\n36.6\nSentence\n40.3\n45.6\n51.7\n57.6\n24.0\n26.9\n21.1\n27.4\n28.6\n32.9\n33.1\n38.1\nProposition\n39.7\n45.2\n51.0\n56.8\n24.3\n27.5\n22.2\n28.3\n32.0\n36.0\n33.9\n38.8\nGTR\nPassage\n39.8\n46.1\n49.7\n55.9\n23.0\n25.9\n29.9\n35.1\n37.8\n39.6\n36.0\n40.5\nSentence\n39.4\n45.9\n51.7\n58.0\n23.2\n26.1\n35.7\n39.1\n38.0\n39.9\n37.6\n41.8\nProposition\n40.0\n46.9\n52.5\n58.4\n24.2\n26.5\n37.8\n40.4\n39.2\n41.0\n38.7\n42.6\nTable 4: Open-domain QA performance (Exact Match) using Fusion-in-Decoder model (Izacard and Grave, 2021)\nto extract answer from top-5 and top-20 passages retrieved on the index of passages, sentences, and propositions.", "chunk_type": "semantic", "chunker": "semantic_adjacent_v1"}
{"chunk_id": "drag_semantic_p6_c48", "pdf_name": "drag", "page_number": 6, "char_count": 354, "text": "5.2\nRetrieval on Finer-grained Index ⇒\nBetter Cross-Task Generalization\nOur results show the advantage of retrieval on\nproposition-level index in cross-task generalization\nsettings. We observe that on SQuAD and Entity\nQuestions, retrieval on the proposition-level index\nbrings more performance gain over the passage-\nlevel index and sentence-level index.", "chunk_type": "semantic", "chunker": "semantic_adjacent_v1"}
{"chunk_id": "drag_semantic_p6_c49", "pdf_name": "drag", "page_number": 6, "char_count": 353, "text": "To better understand where the improvements\ncan be attributed, we conduct an additional analysis\non Entity Questions. As Entity Questions features\nquestions targeting the properties of longer-tail enti-\nties, we study how the retrieval performance under\nthree different granularities is affected by the occu-\nrance of the target entity in question, i.e.", "chunk_type": "semantic", "chunker": "semantic_adjacent_v1"}
{"chunk_id": "drag_semantic_p6_c50", "pdf_name": "drag", "page_number": 6, "char_count": 341, "text": "whether\nthe entity appears frequently in Wikipedia or not. We estimate the frequency of each entity with the\nfollowing method. Given the surface form of an en-\ntity, we use BM25 to retrieve the top 1000 relevant\npassages from Wikipedia. We use the number of\noccurrences of the entity in its relevant passages as\nan estimate of its frequency.", "chunk_type": "semantic", "chunker": "semantic_adjacent_v1"}
{"chunk_id": "drag_semantic_p6_c51", "pdf_name": "drag", "page_number": 6, "char_count": 399, "text": "With the 20,000 test\nqueries, around 25% of the target entities have an\nfrequency value of less or equal to 3. Figure 3 shows the passage retrieval perfor-\nmance vs. the frequency of the target entity in\neach question. Across all four dense retrievers,\nwe observe that retrieving by proposition shows a\nmuch larger advantage over retrieving by passages\nwith questions targeting less common entities.", "chunk_type": "semantic", "chunker": "semantic_adjacent_v1"}
{"chunk_id": "drag_semantic_p6_c52", "pdf_name": "drag", "page_number": 6, "char_count": 617, "text": "As\nthe frequency of entities increases, the performance\ngap decreases. Our findings indicate that the per-\nformance gain from retrieval by proposition can\nmostly be attributed to queries for long-tailed infor-\nmation. This echoes our observation that retrieval\non proposition-level index improves the cross-task\ngeneralization performance of dense retrievers. 5.3\nHigher Passage Recall ⇒Higher\nDownstream QA Accuracy\nTo further understand whether the passage retrieval\non a finer-grained index achieves higher down-\nstream QA performance, we extract the answer\nfrom the retrieved passage by a QA reader, Fusion-\n15164", "chunk_type": "semantic", "chunker": "semantic_adjacent_v1"}
{"chunk_id": "drag_fixed_p7_c35", "pdf_name": "drag", "page_number": 7, "char_count": 867, "text": "in-decoder. The results are shown in Table 4. Retrieval by proposition-level index achieves the\nhighest average exact match (EM) on all four re-\ntriever models. Apart from limited exceptions, the\nproposition-level index achieves the highest EM\nfor most retrieval tasks and on most datasets. We\nobserve that the trend of downstream QA perfor-\nmance is highly consistent with passage retrieval\nrecall, suggesting higher passage recall implies bet-\nter downstream QA performance. 6\nHow Does Granularity Influence\nRetrieval-Augmented LMs? In this section, we study how the choice of different\ngranularity used in the prompts affects the retrieval-\naugmented generation across open-domain QA\ntasks. To fairly compare different granularity with\nthe same computation budget, we limit the num-\nber of retrieved tokens for input to the language\nmodel at l = 100 or 500 tokens.", "chunk_type": "fixed", "chunker": "fixed_recursive_v1"}
{"chunk_id": "drag_fixed_p7_c36", "pdf_name": "drag", "page_number": 7, "char_count": 900, "text": "To fairly compare different granularity with\nthe same computation budget, we limit the num-\nber of retrieved tokens for input to the language\nmodel at l = 100 or 500 tokens. Our results sug-\ngest that retrieval by finer-grained units enables a\nhigher density of question-related information in\nthe prompts, leading to better performance. 6.1\nOpen-domain QA Performance\nTable 5 shows the evaluation results with LLaMA-\n2-7B as the language model. Across different re-\ntrievers, we observe higher QA performance in\nterms of the EM@l metric on average when using\npropositions as the retrieval unit. Using propositions rather than passages in the\nprompts, the four dense retrievers—SimCSE, Con-\nRetriever, DPR, and GTR—improve by +4.1, +3.2,\n+2.7, and +2.8 in the EM@500 score. The improve-\nments for using sentences over passages for the\nfour retrieval models are +2.4, +2.1, +2, and +1.6,\nrespectively.", "chunk_type": "fixed", "chunker": "fixed_recursive_v1"}
{"chunk_id": "drag_fixed_p7_c37", "pdf_name": "drag", "page_number": 7, "char_count": 924, "text": "The improve-\nments for using sentences over passages for the\nfour retrieval models are +2.4, +2.1, +2, and +1.6,\nrespectively. It is interesting to note that in the\nLLaMA-2-7B model, the QA accuracy on TQA\nand WebQ is not sensitive to retrieval type. The\nhighest improvements over the closed-book setting\nare only +4.9 and +3.2, achieved by GTR with\npropositions. Nevertheless, we observe that using\nsentences and propositions in the prompts results\nin higher performance than using passages for all\nretrieval models on these two datasets. The results\nsuggest that using finer-grained units in the prompts\nis beneficial to retrieval-augmented generation. 6.2\nFiner-grained Granularity ⇒Higher\nDensity of Question-Related Information\nIntuitively, compared to sentences or passages as\nretrieval units, the advantage of propositions is that\nthe retrieved propositions have a higher density\nof relevant information to the query.", "chunk_type": "fixed", "chunker": "fixed_recursive_v1"}
{"chunk_id": "drag_fixed_p7_c38", "pdf_name": "drag", "page_number": 7, "char_count": 913, "text": "ormation\nIntuitively, compared to sentences or passages as\nretrieval units, the advantage of propositions is that\nthe retrieved propositions have a higher density\nof relevant information to the query. With finer-\ngrained retrieval units, the correct answer to the\nquery would more likely appear in the top-l re-\ntrieved words by a dense retriever. We illustrate this phenomenon by an analysis\nshown in Figure 4. Here, we investigate the posi-\ntion at which the ground truth answer appears in\nthe top-l retrieved words. Specifically, we calcu-\nlate the recall of the gold answer within the initial l\nretrieved words with GTR working with Wikipedia\nindexed in three different granularities. We show the results in Figure 4 and 7 with l\nranging from 0 to 500 across all five datasets. For\na fixed word retrieval budget, proposition retrieval\nshows a higher success rate than sentence and pas-\nsage retrieval methods.", "chunk_type": "fixed", "chunker": "fixed_recursive_v1"}
{"chunk_id": "drag_fixed_p7_c39", "pdf_name": "drag", "page_number": 7, "char_count": 862, "text": "For\na fixed word retrieval budget, proposition retrieval\nshows a higher success rate than sentence and pas-\nsage retrieval methods. The largest improvement of\nproposition retrieval over passage retrieval occurs\nwithin the range of 100-200 words, which corre-\nsponds to roughly 10 propositions, 5 sentences, or\n2 passages. As word count increases, the recall rate\nof the three granularities converges, encompassing\nall relevant information. 7\nRelated Work\nRecent works on dense retrievers typically adopt\na dual-encoder architecture (Yih et al., 2011;\nReimers and Gurevych, 2019; Karpukhin et al.,\n2020; Ni et al., 2022). With dual-encoders,\neach query and document is encoded into a low-\ndimensional feature vector respectively, and their\nrelevance is measured by a non-parametric similar-\nity function between the embedding vectors (Muss-\nmann and Ermon, 2016).", "chunk_type": "fixed", "chunker": "fixed_recursive_v1"}
{"chunk_id": "drag_fixed_p7_c40", "pdf_name": "drag", "page_number": 7, "char_count": 971, "text": "nt is encoded into a low-\ndimensional feature vector respectively, and their\nrelevance is measured by a non-parametric similar-\nity function between the embedding vectors (Muss-\nmann and Ermon, 2016). Due to the limited expres-\nsivity from the similarity function, dual encoder\nmodels often generalize poorly to new tasks with\nscarce training data (Thakur et al., 2021). Previous\nstudies use techniques such as data augmentation\n(Wang et al., 2022; Yu et al., 2023a; Izacard et al.,\n2022; Gao and Callan, 2022; Lin et al., 2023; Dai\net al., 2023), continual pre-training (Chang et al.,\n2020; Sachan et al., 2021; Oguz et al., 2022), task-\naware training (Xin et al., 2022; Cheng et al., 2023),\nhybrid sparse-dense retrieval (Luan et al., 2021;\nChen et al., 2022), or mixed strategy retrieval (Ma\net al., 2022, 2023) and so on to improve cross-task\ngeneralization performance of dense retrievers. The motivation of our work echoes in part with\nmulti-vector retrieval, e.g.", "chunk_type": "fixed", "chunker": "fixed_recursive_v1"}
{"chunk_id": "drag_fixed_p7_c41", "pdf_name": "drag", "page_number": 7, "char_count": 197, "text": "The motivation of our work echoes in part with\nmulti-vector retrieval, e.g. ColBERT (Khattab and\nZaharia, 2020), DensePhrase (Lee et al., 2021a,b),\nME-BERT (Luan et al., 2021), and MVR (Zhang\n15165", "chunk_type": "fixed", "chunker": "fixed_recursive_v1"}
{"chunk_id": "drag_semantic_p7_c53", "pdf_name": "drag", "page_number": 7, "char_count": 476, "text": "in-decoder. The results are shown in Table 4. Retrieval by proposition-level index achieves the\nhighest average exact match (EM) on all four re-\ntriever models. Apart from limited exceptions, the\nproposition-level index achieves the highest EM\nfor most retrieval tasks and on most datasets. We\nobserve that the trend of downstream QA perfor-\nmance is highly consistent with passage retrieval\nrecall, suggesting higher passage recall implies bet-\nter downstream QA performance.", "chunk_type": "semantic", "chunker": "semantic_adjacent_v1"}
{"chunk_id": "drag_semantic_p7_c54", "pdf_name": "drag", "page_number": 7, "char_count": 390, "text": "6\nHow Does Granularity Influence\nRetrieval-Augmented LMs? In this section, we study how the choice of different\ngranularity used in the prompts affects the retrieval-\naugmented generation across open-domain QA\ntasks. To fairly compare different granularity with\nthe same computation budget, we limit the num-\nber of retrieved tokens for input to the language\nmodel at l = 100 or 500 tokens.", "chunk_type": "semantic", "chunker": "semantic_adjacent_v1"}
{"chunk_id": "drag_semantic_p7_c55", "pdf_name": "drag", "page_number": 7, "char_count": 421, "text": "Our results sug-\ngest that retrieval by finer-grained units enables a\nhigher density of question-related information in\nthe prompts, leading to better performance. 6.1\nOpen-domain QA Performance\nTable 5 shows the evaluation results with LLaMA-\n2-7B as the language model. Across different re-\ntrievers, we observe higher QA performance in\nterms of the EM@l metric on average when using\npropositions as the retrieval unit.", "chunk_type": "semantic", "chunker": "semantic_adjacent_v1"}
{"chunk_id": "drag_semantic_p7_c56", "pdf_name": "drag", "page_number": 7, "char_count": 304, "text": "Using propositions rather than passages in the\nprompts, the four dense retrievers—SimCSE, Con-\nRetriever, DPR, and GTR—improve by +4.1, +3.2,\n+2.7, and +2.8 in the EM@500 score. The improve-\nments for using sentences over passages for the\nfour retrieval models are +2.4, +2.1, +2, and +1.6,\nrespectively.", "chunk_type": "semantic", "chunker": "semantic_adjacent_v1"}
{"chunk_id": "drag_semantic_p7_c57", "pdf_name": "drag", "page_number": 7, "char_count": 527, "text": "It is interesting to note that in the\nLLaMA-2-7B model, the QA accuracy on TQA\nand WebQ is not sensitive to retrieval type. The\nhighest improvements over the closed-book setting\nare only +4.9 and +3.2, achieved by GTR with\npropositions. Nevertheless, we observe that using\nsentences and propositions in the prompts results\nin higher performance than using passages for all\nretrieval models on these two datasets. The results\nsuggest that using finer-grained units in the prompts\nis beneficial to retrieval-augmented generation.", "chunk_type": "semantic", "chunker": "semantic_adjacent_v1"}
{"chunk_id": "drag_semantic_p7_c58", "pdf_name": "drag", "page_number": 7, "char_count": 416, "text": "6.2\nFiner-grained Granularity ⇒Higher\nDensity of Question-Related Information\nIntuitively, compared to sentences or passages as\nretrieval units, the advantage of propositions is that\nthe retrieved propositions have a higher density\nof relevant information to the query. With finer-\ngrained retrieval units, the correct answer to the\nquery would more likely appear in the top-l re-\ntrieved words by a dense retriever.", "chunk_type": "semantic", "chunker": "semantic_adjacent_v1"}
{"chunk_id": "drag_semantic_p7_c59", "pdf_name": "drag", "page_number": 7, "char_count": 340, "text": "We illustrate this phenomenon by an analysis\nshown in Figure 4. Here, we investigate the posi-\ntion at which the ground truth answer appears in\nthe top-l retrieved words. Specifically, we calcu-\nlate the recall of the gold answer within the initial l\nretrieved words with GTR working with Wikipedia\nindexed in three different granularities.", "chunk_type": "semantic", "chunker": "semantic_adjacent_v1"}
{"chunk_id": "drag_semantic_p7_c60", "pdf_name": "drag", "page_number": 7, "char_count": 414, "text": "We show the results in Figure 4 and 7 with l\nranging from 0 to 500 across all five datasets. For\na fixed word retrieval budget, proposition retrieval\nshows a higher success rate than sentence and pas-\nsage retrieval methods. The largest improvement of\nproposition retrieval over passage retrieval occurs\nwithin the range of 100-200 words, which corre-\nsponds to roughly 10 propositions, 5 sentences, or\n2 passages.", "chunk_type": "semantic", "chunker": "semantic_adjacent_v1"}
{"chunk_id": "drag_semantic_p7_c61", "pdf_name": "drag", "page_number": 7, "char_count": 710, "text": "As word count increases, the recall rate\nof the three granularities converges, encompassing\nall relevant information. 7\nRelated Work\nRecent works on dense retrievers typically adopt\na dual-encoder architecture (Yih et al., 2011;\nReimers and Gurevych, 2019; Karpukhin et al.,\n2020; Ni et al., 2022). With dual-encoders,\neach query and document is encoded into a low-\ndimensional feature vector respectively, and their\nrelevance is measured by a non-parametric similar-\nity function between the embedding vectors (Muss-\nmann and Ermon, 2016). Due to the limited expres-\nsivity from the similarity function, dual encoder\nmodels often generalize poorly to new tasks with\nscarce training data (Thakur et al., 2021).", "chunk_type": "semantic", "chunker": "semantic_adjacent_v1"}
{"chunk_id": "drag_semantic_p7_c62", "pdf_name": "drag", "page_number": 7, "char_count": 722, "text": "Previous\nstudies use techniques such as data augmentation\n(Wang et al., 2022; Yu et al., 2023a; Izacard et al.,\n2022; Gao and Callan, 2022; Lin et al., 2023; Dai\net al., 2023), continual pre-training (Chang et al.,\n2020; Sachan et al., 2021; Oguz et al., 2022), task-\naware training (Xin et al., 2022; Cheng et al., 2023),\nhybrid sparse-dense retrieval (Luan et al., 2021;\nChen et al., 2022), or mixed strategy retrieval (Ma\net al., 2022, 2023) and so on to improve cross-task\ngeneralization performance of dense retrievers. The motivation of our work echoes in part with\nmulti-vector retrieval, e.g. ColBERT (Khattab and\nZaharia, 2020), DensePhrase (Lee et al., 2021a,b),\nME-BERT (Luan et al., 2021), and MVR (Zhang\n15165", "chunk_type": "semantic", "chunker": "semantic_adjacent_v1"}
{"chunk_id": "drag_fixed_p8_c42", "pdf_name": "drag", "page_number": 8, "char_count": 47, "text": "Retriever\nGranularity\nNQ\nTQA\nWebQ\nSQuAD\nEQ\nAvg.", "chunk_type": "fixed", "chunker": "fixed_recursive_v1"}
{"chunk_id": "drag_fixed_p8_c43", "pdf_name": "drag", "page_number": 8, "char_count": 1188, "text": "Retriever\nGranularity\nNQ\nTQA\nWebQ\nSQuAD\nEQ\nAvg. EM\nEM\nEM\nEM\nEM\nEM\n@100\n@500\n@100\n@500\n@100\n@500\n@100\n@500\n@100\n@500\n@100\n@500\nClosed-book\n23.4\n57.4\n25.9\n13.0\n23.2\n28.6\nUnsupervised Dense Retrievers\nSimCSE\nPassage\n20.5\n22.9\n49.7\n52.9\n24.5\n24.6\n13.7\n16.6\n20.7\n25.5\n25.8\n28.5\nSentence\n21.1\n24.3\n52.1\n54.2\n24.2\n26.1\n17.7\n21.5\n22.9\n28.3\n27.6\n30.9\nProposition\n22.0\n26.0\n51.0\n53.9\n23.5\n27.0\n18.6\n22.7\n25.9\n33.6\n28.2\n32.6\nContriever\nPassage\n24.5\n28.7\n54.7\n57.9\n25.7\n26.9\n17.7\n24.2\n25.6\n32.5\n29.6\n34.1\nSentence\n25.0\n30.2\n56.3\n59.2\n26.8\n29.2\n22.5\n28.1\n26.1\n34.1\n31.3\n36.2\nProposition\n25.8\n30.3\n56.8\n60.0\n26.8\n29.9\n24.8\n29.7\n27.1\n36.5\n32.3\n37.3\nSupervised Dense Retrievers\nDPR\nPassage\n30.6\n33.7\n56.5\n60.3\n25.0\n26.8\n14.2\n18.9\n26.4\n31.6\n30.6\n34.3\nSentence\n32.5\n34.1\n58.3\n61.7\n25.4\n28.0\n17.6\n22.1\n29.8\n35.6\n32.7\n36.3\nProposition\n31.5\n33.8\n57.6\n60.6\n27.1\n28.2\n18.2\n22.6\n32.9\n39.7\n33.5\n37.0\nGTR\nPassage\n30.0\n33.9\n56.9\n60.0\n24.5\n25.9\n21.5\n27.4\n42.2\n45.3\n35.0\n38.5\nSentence\n30.9\n34.0\n58.9\n61.9\n24.5\n27.0\n29.8\n31.7\n42.9\n45.9\n37.4\n40.1\nProposition\n32.1\n33.8\n58.8\n62.3\n25.7\n29.1\n32.5\n33.1\n43.0\n48.1\n38.4\n41.3\nTable 5: Open-domain QA performance (EM = Exact Match) with LLaMA-2-7B model (Touvron et al., 2023).", "chunk_type": "fixed", "chunker": "fixed_recursive_v1"}
{"chunk_id": "drag_fixed_p8_c44", "pdf_name": "drag", "page_number": 8, "char_count": 837, "text": ".8\n31.7\n42.9\n45.9\n37.4\n40.1\nProposition\n32.1\n33.8\n58.8\n62.3\n25.7\n29.1\n32.5\n33.1\n43.0\n48.1\n38.4\n41.3\nTable 5: Open-domain QA performance (EM = Exact Match) with LLaMA-2-7B model (Touvron et al., 2023). The\ncontext in the prompts is constructed by passage, sentence, or propositions limiting at l = 100 or 500 tokens. We\nprompt the LLaMA-2-7B model with four-shot demonstrations for each test case. 0\n200\n400\n#Words\n50\n60\n70\nRecall (%)\nGTR / NQ\n0\n200\n400\n#Words\n60\n70\nRecall (%)\nGTR / TQA\n0\n200\n400\n#Words\n50\n60\n70\nRecall (%)\nGTR / WebQ\n0\n200\n400\n#Words\n40\n50\n60\nRecall (%)\nGTR / SQuAD\n0\n200\n400\n#Words\n60\n70\n80\nRecall (%)\nGTR / EQ\nPassage\nSentence\nProposition\nFigure 4: Recall of the gold answer in the retrieved text limited to first k words for the GTR retriever. Finer-grained\nretrieval has a higher recall across all numbers of words.", "chunk_type": "fixed", "chunker": "fixed_recursive_v1"}
{"chunk_id": "drag_fixed_p8_c45", "pdf_name": "drag", "page_number": 8, "char_count": 906, "text": "Finer-grained\nretrieval has a higher recall across all numbers of words. et al., 2022), where the retrieval model learns to\nencode a candidate retrieval unit into multiple vec-\ntors to increase model expressivity and improve\nretrieval granularity (Seo et al., 2019; Humeau\net al., 2019). Our work instead focuses on the\nsetting where we do not update the dense retriever\nmodel or its parameters. We show that indexing\nthe retrieval corpus by different granularity can be\na simple and orthogonal strategy for improving the\ngeneralization of dense retrievers at inference time. In line with generating retrieval units from the\noriginal corpus, Sarthi et al. (2024) propose using\ngenerative summaries as additional retrieval units\nalongside the original text, enhancing queries with\ndocument-level understanding. In contrast, our\nwork generates propositions to improve queries\nrelated to long-tailed entities.", "chunk_type": "fixed", "chunker": "fixed_recursive_v1"}
{"chunk_id": "drag_fixed_p8_c46", "pdf_name": "drag", "page_number": 8, "char_count": 820, "text": "In contrast, our\nwork generates propositions to improve queries\nrelated to long-tailed entities. These approaches are\ncomplementary, as they address different aspects\nof retrieval enhancement. The use of propositions as a unit of text rep-\nresentation dates back to the Pyramid method in\nsummarization evaluation (Nenkova and Passon-\nneau, 2004), where a model-generated summary\nis evaluated by each proposition. Proposition ex-\ntraction from text has been a long-standing task,\nwith earlier formulations focusing on a structured\nrepresentation of propositions (Etzioni et al., 2008;\nGildea and Jurafsky, 2000). More recent studies\nhave found success in extracting free-text propo-\nsitions via few-shot prompting with LLMs (Min\net al., 2023; Kamoi et al., 2023), or fine-tuning\ncompact-sized models (Chen et al., 2023b).", "chunk_type": "fixed", "chunker": "fixed_recursive_v1"}
{"chunk_id": "drag_fixed_p8_c47", "pdf_name": "drag", "page_number": 8, "char_count": 696, "text": "ent studies\nhave found success in extracting free-text propo-\nsitions via few-shot prompting with LLMs (Min\net al., 2023; Kamoi et al., 2023), or fine-tuning\ncompact-sized models (Chen et al., 2023b). Retrieve-then-read, or more broadly retrieval\naugmented generation, has recently emerged as\na popular paradigm for open-domain question an-\nswering (Lewis et al., 2021; Jiang et al., 2023; Asai\net al., 2023). While earlier works provide up to\nthe top 100 retrieved passages for the downstream\nreader (Izacard and Grave, 2021; Kedia et al.,\n2022), the amount of context allowed is signifi-\ncantly reduced when using recent large language\nmodels (Touvron et al., 2023; Yu et al., 2023b), due\n15166", "chunk_type": "fixed", "chunker": "fixed_recursive_v1"}
{"chunk_id": "drag_semantic_p8_c63", "pdf_name": "drag", "page_number": 8, "char_count": 1188, "text": "Retriever\nGranularity\nNQ\nTQA\nWebQ\nSQuAD\nEQ\nAvg. EM\nEM\nEM\nEM\nEM\nEM\n@100\n@500\n@100\n@500\n@100\n@500\n@100\n@500\n@100\n@500\n@100\n@500\nClosed-book\n23.4\n57.4\n25.9\n13.0\n23.2\n28.6\nUnsupervised Dense Retrievers\nSimCSE\nPassage\n20.5\n22.9\n49.7\n52.9\n24.5\n24.6\n13.7\n16.6\n20.7\n25.5\n25.8\n28.5\nSentence\n21.1\n24.3\n52.1\n54.2\n24.2\n26.1\n17.7\n21.5\n22.9\n28.3\n27.6\n30.9\nProposition\n22.0\n26.0\n51.0\n53.9\n23.5\n27.0\n18.6\n22.7\n25.9\n33.6\n28.2\n32.6\nContriever\nPassage\n24.5\n28.7\n54.7\n57.9\n25.7\n26.9\n17.7\n24.2\n25.6\n32.5\n29.6\n34.1\nSentence\n25.0\n30.2\n56.3\n59.2\n26.8\n29.2\n22.5\n28.1\n26.1\n34.1\n31.3\n36.2\nProposition\n25.8\n30.3\n56.8\n60.0\n26.8\n29.9\n24.8\n29.7\n27.1\n36.5\n32.3\n37.3\nSupervised Dense Retrievers\nDPR\nPassage\n30.6\n33.7\n56.5\n60.3\n25.0\n26.8\n14.2\n18.9\n26.4\n31.6\n30.6\n34.3\nSentence\n32.5\n34.1\n58.3\n61.7\n25.4\n28.0\n17.6\n22.1\n29.8\n35.6\n32.7\n36.3\nProposition\n31.5\n33.8\n57.6\n60.6\n27.1\n28.2\n18.2\n22.6\n32.9\n39.7\n33.5\n37.0\nGTR\nPassage\n30.0\n33.9\n56.9\n60.0\n24.5\n25.9\n21.5\n27.4\n42.2\n45.3\n35.0\n38.5\nSentence\n30.9\n34.0\n58.9\n61.9\n24.5\n27.0\n29.8\n31.7\n42.9\n45.9\n37.4\n40.1\nProposition\n32.1\n33.8\n58.8\n62.3\n25.7\n29.1\n32.5\n33.1\n43.0\n48.1\n38.4\n41.3\nTable 5: Open-domain QA performance (EM = Exact Match) with LLaMA-2-7B model (Touvron et al., 2023).", "chunk_type": "semantic", "chunker": "semantic_adjacent_v1"}
{"chunk_id": "drag_semantic_p8_c64", "pdf_name": "drag", "page_number": 8, "char_count": 636, "text": "The\ncontext in the prompts is constructed by passage, sentence, or propositions limiting at l = 100 or 500 tokens. We\nprompt the LLaMA-2-7B model with four-shot demonstrations for each test case. 0\n200\n400\n#Words\n50\n60\n70\nRecall (%)\nGTR / NQ\n0\n200\n400\n#Words\n60\n70\nRecall (%)\nGTR / TQA\n0\n200\n400\n#Words\n50\n60\n70\nRecall (%)\nGTR / WebQ\n0\n200\n400\n#Words\n40\n50\n60\nRecall (%)\nGTR / SQuAD\n0\n200\n400\n#Words\n60\n70\n80\nRecall (%)\nGTR / EQ\nPassage\nSentence\nProposition\nFigure 4: Recall of the gold answer in the retrieved text limited to first k words for the GTR retriever. Finer-grained\nretrieval has a higher recall across all numbers of words.", "chunk_type": "semantic", "chunker": "semantic_adjacent_v1"}
{"chunk_id": "drag_semantic_p8_c65", "pdf_name": "drag", "page_number": 8, "char_count": 322, "text": "et al., 2022), where the retrieval model learns to\nencode a candidate retrieval unit into multiple vec-\ntors to increase model expressivity and improve\nretrieval granularity (Seo et al., 2019; Humeau\net al., 2019). Our work instead focuses on the\nsetting where we do not update the dense retriever\nmodel or its parameters.", "chunk_type": "semantic", "chunker": "semantic_adjacent_v1"}
{"chunk_id": "drag_semantic_p8_c66", "pdf_name": "drag", "page_number": 8, "char_count": 413, "text": "We show that indexing\nthe retrieval corpus by different granularity can be\na simple and orthogonal strategy for improving the\ngeneralization of dense retrievers at inference time. In line with generating retrieval units from the\noriginal corpus, Sarthi et al. (2024) propose using\ngenerative summaries as additional retrieval units\nalongside the original text, enhancing queries with\ndocument-level understanding.", "chunk_type": "semantic", "chunker": "semantic_adjacent_v1"}
{"chunk_id": "drag_semantic_p8_c67", "pdf_name": "drag", "page_number": 8, "char_count": 611, "text": "In contrast, our\nwork generates propositions to improve queries\nrelated to long-tailed entities. These approaches are\ncomplementary, as they address different aspects\nof retrieval enhancement. The use of propositions as a unit of text rep-\nresentation dates back to the Pyramid method in\nsummarization evaluation (Nenkova and Passon-\nneau, 2004), where a model-generated summary\nis evaluated by each proposition. Proposition ex-\ntraction from text has been a long-standing task,\nwith earlier formulations focusing on a structured\nrepresentation of propositions (Etzioni et al., 2008;\nGildea and Jurafsky, 2000).", "chunk_type": "semantic", "chunker": "semantic_adjacent_v1"}
{"chunk_id": "drag_semantic_p8_c68", "pdf_name": "drag", "page_number": 8, "char_count": 704, "text": "More recent studies\nhave found success in extracting free-text propo-\nsitions via few-shot prompting with LLMs (Min\net al., 2023; Kamoi et al., 2023), or fine-tuning\ncompact-sized models (Chen et al., 2023b). Retrieve-then-read, or more broadly retrieval\naugmented generation, has recently emerged as\na popular paradigm for open-domain question an-\nswering (Lewis et al., 2021; Jiang et al., 2023; Asai\net al., 2023). While earlier works provide up to\nthe top 100 retrieved passages for the downstream\nreader (Izacard and Grave, 2021; Kedia et al.,\n2022), the amount of context allowed is signifi-\ncantly reduced when using recent large language\nmodels (Touvron et al., 2023; Yu et al., 2023b), due\n15166", "chunk_type": "semantic", "chunker": "semantic_adjacent_v1"}
{"chunk_id": "drag_fixed_p9_c48", "pdf_name": "drag", "page_number": 9, "char_count": 969, "text": "to the limited context window length and inability\nto reason over long context (Liu et al., 2023). Re-\ncent efforts try to improve the quality of the reader\ncontext by filtering or compressing the retrieved\ndocuments (Wang et al., 2023; Xu et al., 2023). Our work offers a new perspective by changing\nthe retrieval granularity, in order to achiev greater\ninformation density with a fixed context length. 8\nConclusion\nThis paper studies how the choice of granularity\nfor indexing a corpus, as well as the granularity\nused in the prompts, influences retrieval and down-\nstream QA performance. Our results show that\nretrieval by propositions outperforms passage-level\nand sentence-level retrieval on passage retrieval\nand downstream QA across five open-domain QA\ndatasets. Our analysis shows that indexing a corpus\nwith finer-grained units enhances the cross-task\ngeneralization of dense retrievers and increases\nthe density of question-related information in the\nprompts.", "chunk_type": "fixed", "chunker": "fixed_recursive_v1"}
{"chunk_id": "drag_fixed_p9_c49", "pdf_name": "drag", "page_number": 9, "char_count": 985, "text": "Our analysis shows that indexing a corpus\nwith finer-grained units enhances the cross-task\ngeneralization of dense retrievers and increases\nthe density of question-related information in the\nprompts. We hope that FACTOIDWIKI and our find-\nings will facilitate future research on information\nretrieval and retrieval-augmented generation. Limitations\nThe scope of our current study on the granular-\nity of retrieval corpus has the following limita-\ntions. (1) Retrieval Corpus – Our study only fo-\ncuses on Wikipedia as the retrieval corpus, due to\nthe fact that most open-domain QA datasets adopt\nWikipedia as the retrieval corpus. (2) Types of\ndense retrievers evaluated – In the current version\nof the paper, we evaluate 6 types of popular dense\nretrievers, most of which follow the bi- or dual-\nencoder architecture. In future versions, we will\ninclude and discuss results on a broader range of\ndense retrievers. (3) Language – Our current study\nis limited to English Wikipedia only.", "chunk_type": "fixed", "chunker": "fixed_recursive_v1"}
{"chunk_id": "drag_fixed_p9_c50", "pdf_name": "drag", "page_number": 9, "char_count": 887, "text": "(3) Language – Our current study\nis limited to English Wikipedia only. We leave the\nexploration on other languages to future work. Ethical Considerations\nThis article follows the ACL Code of Ethics. Our\nwork is a foundational research on information\nretrieval. To the best of our knowledge, we do\nnot find obvious risks related to malicious harmful\neffects, environmental impact, fairness considera-\ntions, or privacy considerations. Acknowledgements\nThe authors sincerely appreciate anonymous re-\nviewers for helpful discussions and comments. The\nauthors would like to thank Xuanyu Ben Zhou,\nRuixin Hong, Ning Dai, and Linfeng Shen for valu-\nable feedback on the project. Xinran Zhao is sup-\nported by the ONR Award N000142312840. References\nZeynep Akkalyoncu Yilmaz, Wei Yang, Haotian Zhang,\nand Jimmy Lin. 2019. Cross-domain modeling of\nsentence-level evidence for document retrieval.", "chunk_type": "fixed", "chunker": "fixed_recursive_v1"}
{"chunk_id": "drag_fixed_p9_c51", "pdf_name": "drag", "page_number": 9, "char_count": 981, "text": "Cross-domain modeling of\nsentence-level evidence for document retrieval. In\nProceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing and the\n9th International Joint Conference on Natural Lan-\nguage Processing (EMNLP-IJCNLP), pages 3490–\n3496, Hong Kong, China. Association for Computa-\ntional Linguistics. Akari Asai, Zeqiu Wu, Yizhong Wang, Avirup Sil, and\nHannaneh Hajishirzi. 2023. Self-rag: Learning to re-\ntrieve, generate, and critique through self-reflection. Jonathan Berant, Andrew Chou, Roy Frostig, and Percy\nLiang. 2013. Semantic parsing on freebase from\nquestion-answer pairs. In Proceedings of the 2013\nconference on empirical methods in natural language\nprocessing, pages 1533–1544. Bernd Bohnet, Vinh Q Tran, Pat Verga, Roee Aharoni,\nDaniel Andor, Livio Baldini Soares, Jacob Eisenstein,\nKuzman Ganchev, Jonathan Herzig, Kai Hui, et al. 2022. Attributed question answering: Evaluation and\nmodeling for attributed large language models.", "chunk_type": "fixed", "chunker": "fixed_recursive_v1"}
{"chunk_id": "drag_fixed_p9_c52", "pdf_name": "drag", "page_number": 9, "char_count": 906, "text": "Attributed question answering: Evaluation and\nmodeling for attributed large language models. arXiv\npreprint arXiv:2212.08037. Wei-Cheng Chang, X Yu Felix, Yin-Wen Chang, Yim-\ning Yang, and Sanjiv Kumar. 2020. Pre-training tasks\nfor embedding-based large-scale retrieval. In Inter-\nnational Conference on Learning Representations. Sihao Chen, Senaka Buthpitiya, Alex Fabrikant, Dan\nRoth, and Tal Schuster. 2023a. PropSegmEnt: A\nlarge-scale corpus for proposition-level segmentation\nand entailment recognition. In Findings of the As-\nsociation for Computational Linguistics: ACL 2023,\npages 8874–8893, Toronto, Canada. Association for\nComputational Linguistics. Sihao Chen, Hongming Zhang, Tong Chen, Ben Zhou,\nWenhao Yu, Dian Yu, Baolin Peng, Hongwei Wang,\nDan Roth, and Dong Yu. 2023b. Sub-sentence en-\ncoder: Contrastive learning of propositional semantic\nrepresentations. arXiv preprint arXiv:2311.04335.", "chunk_type": "fixed", "chunker": "fixed_recursive_v1"}
{"chunk_id": "drag_fixed_p9_c53", "pdf_name": "drag", "page_number": 9, "char_count": 328, "text": "arXiv preprint arXiv:2311.04335. Xilun Chen, Kushal Lakhotia, Barlas Oguz, Anchit\nGupta, Patrick Lewis, Stan Peshterliev, Yashar\nMehdad, Sonal Gupta, and Wen-tau Yih. 2022. Salient phrase aware dense retrieval: Can a dense\nretriever imitate a sparse one? In Findings of the\nAssociation for Computational Linguistics: EMNLP\n15167", "chunk_type": "fixed", "chunker": "fixed_recursive_v1"}
{"chunk_id": "drag_semantic_p9_c69", "pdf_name": "drag", "page_number": 9, "char_count": 769, "text": "to the limited context window length and inability\nto reason over long context (Liu et al., 2023). Re-\ncent efforts try to improve the quality of the reader\ncontext by filtering or compressing the retrieved\ndocuments (Wang et al., 2023; Xu et al., 2023). Our work offers a new perspective by changing\nthe retrieval granularity, in order to achiev greater\ninformation density with a fixed context length. 8\nConclusion\nThis paper studies how the choice of granularity\nfor indexing a corpus, as well as the granularity\nused in the prompts, influences retrieval and down-\nstream QA performance. Our results show that\nretrieval by propositions outperforms passage-level\nand sentence-level retrieval on passage retrieval\nand downstream QA across five open-domain QA\ndatasets.", "chunk_type": "semantic", "chunker": "semantic_adjacent_v1"}
{"chunk_id": "drag_semantic_p9_c70", "pdf_name": "drag", "page_number": 9, "char_count": 336, "text": "Our analysis shows that indexing a corpus\nwith finer-grained units enhances the cross-task\ngeneralization of dense retrievers and increases\nthe density of question-related information in the\nprompts. We hope that FACTOIDWIKI and our find-\nings will facilitate future research on information\nretrieval and retrieval-augmented generation.", "chunk_type": "semantic", "chunker": "semantic_adjacent_v1"}
{"chunk_id": "drag_semantic_p9_c71", "pdf_name": "drag", "page_number": 9, "char_count": 577, "text": "Limitations\nThe scope of our current study on the granular-\nity of retrieval corpus has the following limita-\ntions. (1) Retrieval Corpus – Our study only fo-\ncuses on Wikipedia as the retrieval corpus, due to\nthe fact that most open-domain QA datasets adopt\nWikipedia as the retrieval corpus. (2) Types of\ndense retrievers evaluated – In the current version\nof the paper, we evaluate 6 types of popular dense\nretrievers, most of which follow the bi- or dual-\nencoder architecture. In future versions, we will\ninclude and discuss results on a broader range of\ndense retrievers.", "chunk_type": "semantic", "chunker": "semantic_adjacent_v1"}
{"chunk_id": "drag_semantic_p9_c72", "pdf_name": "drag", "page_number": 9, "char_count": 433, "text": "(3) Language – Our current study\nis limited to English Wikipedia only. We leave the\nexploration on other languages to future work. Ethical Considerations\nThis article follows the ACL Code of Ethics. Our\nwork is a foundational research on information\nretrieval. To the best of our knowledge, we do\nnot find obvious risks related to malicious harmful\neffects, environmental impact, fairness considera-\ntions, or privacy considerations.", "chunk_type": "semantic", "chunker": "semantic_adjacent_v1"}
{"chunk_id": "drag_semantic_p9_c73", "pdf_name": "drag", "page_number": 9, "char_count": 374, "text": "Acknowledgements\nThe authors sincerely appreciate anonymous re-\nviewers for helpful discussions and comments. The\nauthors would like to thank Xuanyu Ben Zhou,\nRuixin Hong, Ning Dai, and Linfeng Shen for valu-\nable feedback on the project. Xinran Zhao is sup-\nported by the ONR Award N000142312840. References\nZeynep Akkalyoncu Yilmaz, Wei Yang, Haotian Zhang,\nand Jimmy Lin.", "chunk_type": "semantic", "chunker": "semantic_adjacent_v1"}
{"chunk_id": "drag_semantic_p9_c74", "pdf_name": "drag", "page_number": 9, "char_count": 341, "text": "2019. Cross-domain modeling of\nsentence-level evidence for document retrieval. In\nProceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing and the\n9th International Joint Conference on Natural Lan-\nguage Processing (EMNLP-IJCNLP), pages 3490–\n3496, Hong Kong, China. Association for Computa-\ntional Linguistics.", "chunk_type": "semantic", "chunker": "semantic_adjacent_v1"}
{"chunk_id": "drag_semantic_p9_c75", "pdf_name": "drag", "page_number": 9, "char_count": 391, "text": "Akari Asai, Zeqiu Wu, Yizhong Wang, Avirup Sil, and\nHannaneh Hajishirzi. 2023. Self-rag: Learning to re-\ntrieve, generate, and critique through self-reflection. Jonathan Berant, Andrew Chou, Roy Frostig, and Percy\nLiang. 2013. Semantic parsing on freebase from\nquestion-answer pairs. In Proceedings of the 2013\nconference on empirical methods in natural language\nprocessing, pages 1533–1544.", "chunk_type": "semantic", "chunker": "semantic_adjacent_v1"}
{"chunk_id": "drag_semantic_p9_c76", "pdf_name": "drag", "page_number": 9, "char_count": 363, "text": "Bernd Bohnet, Vinh Q Tran, Pat Verga, Roee Aharoni,\nDaniel Andor, Livio Baldini Soares, Jacob Eisenstein,\nKuzman Ganchev, Jonathan Herzig, Kai Hui, et al. 2022. Attributed question answering: Evaluation and\nmodeling for attributed large language models. arXiv\npreprint arXiv:2212.08037. Wei-Cheng Chang, X Yu Felix, Yin-Wen Chang, Yim-\ning Yang, and Sanjiv Kumar.", "chunk_type": "semantic", "chunker": "semantic_adjacent_v1"}
{"chunk_id": "drag_semantic_p9_c77", "pdf_name": "drag", "page_number": 9, "char_count": 305, "text": "2020. Pre-training tasks\nfor embedding-based large-scale retrieval. In Inter-\nnational Conference on Learning Representations. Sihao Chen, Senaka Buthpitiya, Alex Fabrikant, Dan\nRoth, and Tal Schuster. 2023a. PropSegmEnt: A\nlarge-scale corpus for proposition-level segmentation\nand entailment recognition.", "chunk_type": "semantic", "chunker": "semantic_adjacent_v1"}
{"chunk_id": "drag_semantic_p9_c78", "pdf_name": "drag", "page_number": 9, "char_count": 364, "text": "In Findings of the As-\nsociation for Computational Linguistics: ACL 2023,\npages 8874–8893, Toronto, Canada. Association for\nComputational Linguistics. Sihao Chen, Hongming Zhang, Tong Chen, Ben Zhou,\nWenhao Yu, Dian Yu, Baolin Peng, Hongwei Wang,\nDan Roth, and Dong Yu. 2023b. Sub-sentence en-\ncoder: Contrastive learning of propositional semantic\nrepresentations.", "chunk_type": "semantic", "chunker": "semantic_adjacent_v1"}
{"chunk_id": "drag_semantic_p9_c79", "pdf_name": "drag", "page_number": 9, "char_count": 328, "text": "arXiv preprint arXiv:2311.04335. Xilun Chen, Kushal Lakhotia, Barlas Oguz, Anchit\nGupta, Patrick Lewis, Stan Peshterliev, Yashar\nMehdad, Sonal Gupta, and Wen-tau Yih. 2022. Salient phrase aware dense retrieval: Can a dense\nretriever imitate a sparse one? In Findings of the\nAssociation for Computational Linguistics: EMNLP\n15167", "chunk_type": "semantic", "chunker": "semantic_adjacent_v1"}
{"chunk_id": "drag_fixed_p10_c54", "pdf_name": "drag", "page_number": 10, "char_count": 921, "text": "2022, pages 250–262, Abu Dhabi, United Arab Emi-\nrates. Association for Computational Linguistics. Hao Cheng, Hao Fang, Xiaodong Liu, and Jianfeng\nGao. 2023. Task-aware specialization for efficient\nand robust dense retrieval for open-domain question\nanswering. In Proceedings of the 61st Annual Meet-\ning of the Association for Computational Linguistics\n(Volume 2: Short Papers), pages 1864–1875, Toronto,\nCanada. Association for Computational Linguistics. Eunsol Choi, Jennimaria Palomaki, Matthew Lamm,\nTom Kwiatkowski, Dipanjan Das, and Michael\nCollins. 2021. Decontextualization: Making sen-\ntences stand-alone. Transactions of the Association\nfor Computational Linguistics, 9:447–461. Hyung Won Chung, Le Hou, Shayne Longpre, Barret\nZoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi\nWang, Mostafa Dehghani, Siddhartha Brahma, et al. 2022. Scaling instruction-finetuned language models. arXiv preprint arXiv:2210.11416.", "chunk_type": "fixed", "chunker": "fixed_recursive_v1"}
{"chunk_id": "drag_fixed_p10_c55", "pdf_name": "drag", "page_number": 10, "char_count": 991, "text": "arXiv preprint arXiv:2210.11416. Zhuyun Dai, Vincent Y Zhao, Ji Ma, Yi Luan, Jianmo\nNi, Jing Lu, Anton Bakalov, Kelvin Guu, Keith Hall,\nand Ming-Wei Chang. 2023. Promptagator: Few-\nshot dense retrieval from 8 examples. In The Eleventh\nInternational Conference on Learning Representa-\ntions. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n4171–4186, Minneapolis, Minnesota. Association for\nComputational Linguistics. Oren Etzioni, Michele Banko, Stephen Soderland, and\nDaniel S Weld. 2008. Open information extrac-\ntion from the web. Communications of the ACM,\n51(12):68–74. Luyu Gao and Jamie Callan. 2022. Unsupervised cor-\npus aware language model pre-training for dense pas-\nsage retrieval.", "chunk_type": "fixed", "chunker": "fixed_recursive_v1"}
{"chunk_id": "drag_fixed_p10_c56", "pdf_name": "drag", "page_number": 10, "char_count": 890, "text": "Unsupervised cor-\npus aware language model pre-training for dense pas-\nsage retrieval. In Proceedings of the 60th Annual\nMeeting of the Association for Computational Lin-\nguistics (Volume 1: Long Papers), pages 2843–2853,\nDublin, Ireland. Association for Computational Lin-\nguistics. Tianyu Gao, Xingcheng Yao, and Danqi Chen. 2021. Simcse: Simple contrastive learning of sentence em-\nbeddings. arXiv preprint arXiv:2104.08821. Daniel Gildea and Daniel Jurafsky. 2000. Automatic\nlabeling of semantic roles. In Proceedings of the 38th\nAnnual Meeting of the Association for Computational\nLinguistics, pages 512–520, Hong Kong. Association\nfor Computational Linguistics. Samuel Humeau, Kurt Shuster, Marie-Anne Lachaux,\nand Jason Weston. 2019. Poly-encoders: Trans-\nformer architectures and pre-training strategies for\nfast and accurate multi-sentence scoring. arXiv\npreprint arXiv:1905.01969.", "chunk_type": "fixed", "chunker": "fixed_recursive_v1"}
{"chunk_id": "drag_fixed_p10_c57", "pdf_name": "drag", "page_number": 10, "char_count": 963, "text": "arXiv\npreprint arXiv:1905.01969. Gautier Izacard, Mathilde Caron, Lucas Hosseini, Sebas-\ntian Riedel, Piotr Bojanowski, Armand Joulin, and\nEdouard Grave. 2022. Unsupervised dense informa-\ntion retrieval with contrastive learning. Transactions\non Machine Learning Research. Gautier Izacard and Edouard Grave. 2021. Leveraging\npassage retrieval with generative models for open do-\nmain question answering. In Proceedings of the 16th\nConference of the European Chapter of the Associ-\nation for Computational Linguistics: Main Volume,\npages 874–880, Online. Association for Computa-\ntional Linguistics. Zhengbao Jiang, Frank F. Xu, Luyu Gao, Zhiqing Sun,\nQian Liu, Jane Dwivedi-Yu, Yiming Yang, Jamie\nCallan, and Graham Neubig. 2023. Active retrieval\naugmented generation. Mandar Joshi, Eunsol Choi, Daniel S Weld, and Luke\nZettlemoyer. 2017. Triviaqa: A large scale distantly\nsupervised challenge dataset for reading comprehen-\nsion. arXiv preprint arXiv:1705.03551.", "chunk_type": "fixed", "chunker": "fixed_recursive_v1"}
{"chunk_id": "drag_fixed_p10_c58", "pdf_name": "drag", "page_number": 10, "char_count": 992, "text": "arXiv preprint arXiv:1705.03551. Ryo Kamoi, Tanya Goyal, Juan Diego Rodriguez, and\nGreg Durrett. 2023. Wice: Real-world entailment for\nclaims in wikipedia. In Proceedings of the 2023 Con-\nference on Empirical Methods in Natural Language\nProcessing. Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick\nLewis, Ledell Wu, Sergey Edunov, Danqi Chen, and\nWen-tau Yih. 2020. Dense passage retrieval for open-\ndomain question answering. In Proceedings of the\n2020 Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP), pages 6769–6781,\nOnline. Association for Computational Linguistics. Akhil Kedia, Mohd Abbas Zaidi, and Haejun Lee. 2022. FiE: Building a global probability space by leverag-\ning early fusion in encoder for open-domain question\nanswering. In Proceedings of the 2022 Conference\non Empirical Methods in Natural Language Process-\ning, pages 4246–4260, Abu Dhabi, United Arab Emi-\nrates. Association for Computational Linguistics. Omar Khattab and Matei Zaharia. 2020.", "chunk_type": "fixed", "chunker": "fixed_recursive_v1"}
{"chunk_id": "drag_fixed_p10_c59", "pdf_name": "drag", "page_number": 10, "char_count": 557, "text": "2020. Colbert: Effi-\ncient and effective passage search via contextualized\nlate interaction over bert. In Proceedings of the 43rd\nInternational ACM SIGIR conference on research\nand development in Information Retrieval, pages 39–\n48. Tom Kwiatkowski, Jennimaria Palomaki, Olivia Red-\nfield, Michael Collins, Ankur Parikh, Chris Alberti,\nDanielle Epstein, Illia Polosukhin, Jacob Devlin, Ken-\nton Lee, et al. 2019. Natural questions: a benchmark\nfor question answering research. Transactions of the\nAssociation for Computational Linguistics, 7:453–\n466. 15168", "chunk_type": "fixed", "chunker": "fixed_recursive_v1"}
{"chunk_id": "drag_semantic_p10_c80", "pdf_name": "drag", "page_number": 10, "char_count": 456, "text": "2022, pages 250–262, Abu Dhabi, United Arab Emi-\nrates. Association for Computational Linguistics. Hao Cheng, Hao Fang, Xiaodong Liu, and Jianfeng\nGao. 2023. Task-aware specialization for efficient\nand robust dense retrieval for open-domain question\nanswering. In Proceedings of the 61st Annual Meet-\ning of the Association for Computational Linguistics\n(Volume 2: Short Papers), pages 1864–1875, Toronto,\nCanada. Association for Computational Linguistics.", "chunk_type": "semantic", "chunker": "semantic_adjacent_v1"}
{"chunk_id": "drag_semantic_p10_c81", "pdf_name": "drag", "page_number": 10, "char_count": 378, "text": "Eunsol Choi, Jennimaria Palomaki, Matthew Lamm,\nTom Kwiatkowski, Dipanjan Das, and Michael\nCollins. 2021. Decontextualization: Making sen-\ntences stand-alone. Transactions of the Association\nfor Computational Linguistics, 9:447–461. Hyung Won Chung, Le Hou, Shayne Longpre, Barret\nZoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi\nWang, Mostafa Dehghani, Siddhartha Brahma, et al.", "chunk_type": "semantic", "chunker": "semantic_adjacent_v1"}
{"chunk_id": "drag_semantic_p10_c82", "pdf_name": "drag", "page_number": 10, "char_count": 343, "text": "2022. Scaling instruction-finetuned language models. arXiv preprint arXiv:2210.11416. Zhuyun Dai, Vincent Y Zhao, Ji Ma, Yi Luan, Jianmo\nNi, Jing Lu, Anton Bakalov, Kelvin Guu, Keith Hall,\nand Ming-Wei Chang. 2023. Promptagator: Few-\nshot dense retrieval from 8 examples. In The Eleventh\nInternational Conference on Learning Representa-\ntions.", "chunk_type": "semantic", "chunker": "semantic_adjacent_v1"}
{"chunk_id": "drag_semantic_p10_c83", "pdf_name": "drag", "page_number": 10, "char_count": 422, "text": "Jacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n4171–4186, Minneapolis, Minnesota. Association for\nComputational Linguistics.", "chunk_type": "semantic", "chunker": "semantic_adjacent_v1"}
{"chunk_id": "drag_semantic_p10_c84", "pdf_name": "drag", "page_number": 10, "char_count": 474, "text": "Oren Etzioni, Michele Banko, Stephen Soderland, and\nDaniel S Weld. 2008. Open information extrac-\ntion from the web. Communications of the ACM,\n51(12):68–74. Luyu Gao and Jamie Callan. 2022. Unsupervised cor-\npus aware language model pre-training for dense pas-\nsage retrieval. In Proceedings of the 60th Annual\nMeeting of the Association for Computational Lin-\nguistics (Volume 1: Long Papers), pages 2843–2853,\nDublin, Ireland. Association for Computational Lin-\nguistics.", "chunk_type": "semantic", "chunker": "semantic_adjacent_v1"}
{"chunk_id": "drag_semantic_p10_c85", "pdf_name": "drag", "page_number": 10, "char_count": 383, "text": "Tianyu Gao, Xingcheng Yao, and Danqi Chen. 2021. Simcse: Simple contrastive learning of sentence em-\nbeddings. arXiv preprint arXiv:2104.08821. Daniel Gildea and Daniel Jurafsky. 2000. Automatic\nlabeling of semantic roles. In Proceedings of the 38th\nAnnual Meeting of the Association for Computational\nLinguistics, pages 512–520, Hong Kong. Association\nfor Computational Linguistics.", "chunk_type": "semantic", "chunker": "semantic_adjacent_v1"}
{"chunk_id": "drag_semantic_p10_c86", "pdf_name": "drag", "page_number": 10, "char_count": 343, "text": "Samuel Humeau, Kurt Shuster, Marie-Anne Lachaux,\nand Jason Weston. 2019. Poly-encoders: Trans-\nformer architectures and pre-training strategies for\nfast and accurate multi-sentence scoring. arXiv\npreprint arXiv:1905.01969. Gautier Izacard, Mathilde Caron, Lucas Hosseini, Sebas-\ntian Riedel, Piotr Bojanowski, Armand Joulin, and\nEdouard Grave.", "chunk_type": "semantic", "chunker": "semantic_adjacent_v1"}
{"chunk_id": "drag_semantic_p10_c87", "pdf_name": "drag", "page_number": 10, "char_count": 444, "text": "2022. Unsupervised dense informa-\ntion retrieval with contrastive learning. Transactions\non Machine Learning Research. Gautier Izacard and Edouard Grave. 2021. Leveraging\npassage retrieval with generative models for open do-\nmain question answering. In Proceedings of the 16th\nConference of the European Chapter of the Associ-\nation for Computational Linguistics: Main Volume,\npages 874–880, Online. Association for Computa-\ntional Linguistics.", "chunk_type": "semantic", "chunker": "semantic_adjacent_v1"}
{"chunk_id": "drag_semantic_p10_c88", "pdf_name": "drag", "page_number": 10, "char_count": 331, "text": "Zhengbao Jiang, Frank F. Xu, Luyu Gao, Zhiqing Sun,\nQian Liu, Jane Dwivedi-Yu, Yiming Yang, Jamie\nCallan, and Graham Neubig. 2023. Active retrieval\naugmented generation. Mandar Joshi, Eunsol Choi, Daniel S Weld, and Luke\nZettlemoyer. 2017. Triviaqa: A large scale distantly\nsupervised challenge dataset for reading comprehen-\nsion.", "chunk_type": "semantic", "chunker": "semantic_adjacent_v1"}
{"chunk_id": "drag_semantic_p10_c89", "pdf_name": "drag", "page_number": 10, "char_count": 362, "text": "arXiv preprint arXiv:1705.03551. Ryo Kamoi, Tanya Goyal, Juan Diego Rodriguez, and\nGreg Durrett. 2023. Wice: Real-world entailment for\nclaims in wikipedia. In Proceedings of the 2023 Con-\nference on Empirical Methods in Natural Language\nProcessing. Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick\nLewis, Ledell Wu, Sergey Edunov, Danqi Chen, and\nWen-tau Yih.", "chunk_type": "semantic", "chunker": "semantic_adjacent_v1"}
{"chunk_id": "drag_semantic_p10_c90", "pdf_name": "drag", "page_number": 10, "char_count": 403, "text": "2020. Dense passage retrieval for open-\ndomain question answering. In Proceedings of the\n2020 Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP), pages 6769–6781,\nOnline. Association for Computational Linguistics. Akhil Kedia, Mohd Abbas Zaidi, and Haejun Lee. 2022. FiE: Building a global probability space by leverag-\ning early fusion in encoder for open-domain question\nanswering.", "chunk_type": "semantic", "chunker": "semantic_adjacent_v1"}
{"chunk_id": "drag_semantic_p10_c91", "pdf_name": "drag", "page_number": 10, "char_count": 322, "text": "In Proceedings of the 2022 Conference\non Empirical Methods in Natural Language Process-\ning, pages 4246–4260, Abu Dhabi, United Arab Emi-\nrates. Association for Computational Linguistics. Omar Khattab and Matei Zaharia. 2020. Colbert: Effi-\ncient and effective passage search via contextualized\nlate interaction over bert.", "chunk_type": "semantic", "chunker": "semantic_adjacent_v1"}
{"chunk_id": "drag_semantic_p10_c92", "pdf_name": "drag", "page_number": 10, "char_count": 454, "text": "In Proceedings of the 43rd\nInternational ACM SIGIR conference on research\nand development in Information Retrieval, pages 39–\n48. Tom Kwiatkowski, Jennimaria Palomaki, Olivia Red-\nfield, Michael Collins, Ankur Parikh, Chris Alberti,\nDanielle Epstein, Illia Polosukhin, Jacob Devlin, Ken-\nton Lee, et al. 2019. Natural questions: a benchmark\nfor question answering research. Transactions of the\nAssociation for Computational Linguistics, 7:453–\n466. 15168", "chunk_type": "semantic", "chunker": "semantic_adjacent_v1"}
{"chunk_id": "drag_fixed_p11_c60", "pdf_name": "drag", "page_number": 11, "char_count": 903, "text": "Jaewoong Lee, Heejoon Lee, Hwanhee Lee, and Ky-\nomin Jung. 2021a. Learning to select question-\nrelevant relations for visual question answering. In\nProceedings of the Third Workshop on Multimodal\nArtificial Intelligence, pages 87–96, Mexico City,\nMexico. Association for Computational Linguistics. Jinhyuk Lee, Alexander Wettig, and Danqi Chen. 2021b. Phrase retrieval learns passage retrieval, too. In Pro-\nceedings of the 2021 Conference on Empirical Meth-\nods in Natural Language Processing, pages 3661–\n3672, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics. Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio\nPetroni, Vladimir Karpukhin, Naman Goyal, Hein-\nrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rock-\ntäschel, et al. 2020. Retrieval-augmented generation\nfor knowledge-intensive nlp tasks. Advances in Neu-\nral Information Processing Systems, 33:9459–9474.", "chunk_type": "fixed", "chunker": "fixed_recursive_v1"}
{"chunk_id": "drag_fixed_p11_c61", "pdf_name": "drag", "page_number": 11, "char_count": 947, "text": "Advances in Neu-\nral Information Processing Systems, 33:9459–9474. Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio\nPetroni, Vladimir Karpukhin, Naman Goyal, Hein-\nrich Küttler, Mike Lewis, Wen tau Yih, Tim Rock-\ntäschel, Sebastian Riedel, and Douwe Kiela. 2021. Retrieval-augmented generation for knowledge-\nintensive nlp tasks. Sheng-Chieh Lin, Akari Asai, Minghan Li, Barlas Oguz,\nJimmy Lin, Yashar Mehdad, Wen-tau Yih, and Xilun\nChen. 2023. How to Train Your DRAGON: Di-\nverse Augmentation Towards Generalizable Dense\nRetrieval. In Proceedings of the 2023 Conference on\nEmpirical Methods in Natural Language Processing. Nelson F. Liu, Kevin Lin, John Hewitt, Ashwin Paran-\njape, Michele Bevilacqua, Fabio Petroni, and Percy\nLiang. 2023. Lost in the middle: How language\nmodels use long contexts. Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2020.", "chunk_type": "fixed", "chunker": "fixed_recursive_v1"}
{"chunk_id": "drag_fixed_p11_c62", "pdf_name": "drag", "page_number": 11, "char_count": 965, "text": "2020. Ro{bert}a: A robustly optimized {bert} pretraining\napproach. Yi Luan, Jacob Eisenstein, Kristina Toutanova, and\nMichael Collins. 2021. Sparse, dense, and attentional\nrepresentations for text retrieval. Transactions of the\nAssociation for Computational Linguistics, 9:329–\n345. Kaixin Ma, Hao Cheng, Xiaodong Liu, Eric Nyberg,\nand Jianfeng Gao. 2022. Open-domain question an-\nswering via chain of reasoning over heterogeneous\nknowledge. In Findings of the Association for Com-\nputational Linguistics: EMNLP 2022, pages 5360–\n5374, Abu Dhabi, United Arab Emirates. Association\nfor Computational Linguistics. Kaixin Ma, Hao Cheng, Yu Zhang, Xiaodong Liu, Eric\nNyberg, and Jianfeng Gao. 2023. Chain-of-skills:\nA configurable model for open-domain question an-\nswering. In Proceedings of the 61st Annual Meet-\ning of the Association for Computational Linguistics\n(Volume 1: Long Papers), pages 1599–1618, Toronto,\nCanada. Association for Computational Linguistics.", "chunk_type": "fixed", "chunker": "fixed_recursive_v1"}
{"chunk_id": "drag_fixed_p11_c63", "pdf_name": "drag", "page_number": 11, "char_count": 952, "text": "Association for Computational Linguistics. Sewon Min, Kalpesh Krishna, Xinxi Lyu, Mike\nLewis, Wen-tau Yih, Pang Wei Koh, Mohit Iyyer,\nLuke Zettlemoyer, and Hannaneh Hajishirzi. 2023. FActScore: Fine-grained atomic evaluation of factual\nprecision in long form text generation. In Proceed-\nings of the 2023 Conference on Empirical Methods\nin Natural Language Processing. Sewon Min, Julian Michael, Hannaneh Hajishirzi, and\nLuke Zettlemoyer. 2020. AmbigQA: Answering am-\nbiguous open-domain questions. In Proceedings of\nthe 2020 Conference on Empirical Methods in Nat-\nural Language Processing (EMNLP), pages 5783–\n5797, Online. Association for Computational Lin-\nguistics. Stephen Mussmann and Stefano Ermon. 2016. Learning\nand inference via maximum inner product search. In International Conference on Machine Learning,\npages 2587–2596. PMLR. Ani Nenkova and Rebecca Passonneau. 2004. Evaluat-\ning content selection in summarization: The pyramid\nmethod.", "chunk_type": "fixed", "chunker": "fixed_recursive_v1"}
{"chunk_id": "drag_fixed_p11_c64", "pdf_name": "drag", "page_number": 11, "char_count": 878, "text": "Evaluat-\ning content selection in summarization: The pyramid\nmethod. In Proceedings of the Human Language\nTechnology Conference of the North American Chap-\nter of the Association for Computational Linguistics:\nHLT-NAACL 2004, pages 145–152, Boston, Mas-\nsachusetts, USA. Association for Computational Lin-\nguistics. Benjamin Newman, Luca Soldaini, Raymond Fok, Ar-\nman Cohan, and Kyle Lo. 2023. A controllable\nqa-based framework for decontextualization. arXiv\npreprint arXiv:2305.14772. Tri Nguyen, Mir Rosenberg, Xia Song, Jianfeng\nGao, Saurabh Tiwary, Rangan Majumder, and\nLi Deng. 2016. MS MARCO: A human gener-\nated machine reading comprehension dataset. CoRR,\nabs/1611.09268. Jianmo Ni, Chen Qu, Jing Lu, Zhuyun Dai, Gustavo\nHernandez Abrego, Ji Ma, Vincent Zhao, Yi Luan,\nKeith Hall, Ming-Wei Chang, and Yinfei Yang. 2022. Large dual encoders are generalizable retrievers.", "chunk_type": "fixed", "chunker": "fixed_recursive_v1"}
{"chunk_id": "drag_fixed_p11_c65", "pdf_name": "drag", "page_number": 11, "char_count": 900, "text": "Large dual encoders are generalizable retrievers. In\nProceedings of the 2022 Conference on Empirical\nMethods in Natural Language Processing, pages\n9844–9855, Abu Dhabi, United Arab Emirates. As-\nsociation for Computational Linguistics. Barlas Oguz, Kushal Lakhotia, Anchit Gupta, Patrick\nLewis, Vladimir Karpukhin, Aleksandra Piktus,\nXilun Chen, Sebastian Riedel, Scott Yih, Sonal\nGupta, and Yashar Mehdad. 2022. Domain-matched\npre-training tasks for dense retrieval. In Findings\nof the Association for Computational Linguistics:\nNAACL 2022, pages 1524–1534, Seattle, United\nStates. Association for Computational Linguistics. OpenAI. 2023. Gpt-4 technical report. ArXiv,\nabs/2303.08774. Fabio Petroni, Aleksandra Piktus, Angela Fan, Patrick\nLewis, Majid Yazdani, Nicola De Cao, James Thorne,\nYacine Jernite, Vladimir Karpukhin, Jean Maillard,\nVassilis Plachouras, Tim Rocktäschel, and Sebastian\n15169", "chunk_type": "fixed", "chunker": "fixed_recursive_v1"}
{"chunk_id": "drag_semantic_p11_c93", "pdf_name": "drag", "page_number": 11, "char_count": 344, "text": "Jaewoong Lee, Heejoon Lee, Hwanhee Lee, and Ky-\nomin Jung. 2021a. Learning to select question-\nrelevant relations for visual question answering. In\nProceedings of the Third Workshop on Multimodal\nArtificial Intelligence, pages 87–96, Mexico City,\nMexico. Association for Computational Linguistics. Jinhyuk Lee, Alexander Wettig, and Danqi Chen.", "chunk_type": "semantic", "chunker": "semantic_adjacent_v1"}
{"chunk_id": "drag_semantic_p11_c94", "pdf_name": "drag", "page_number": 11, "char_count": 419, "text": "2021b. Phrase retrieval learns passage retrieval, too. In Pro-\nceedings of the 2021 Conference on Empirical Meth-\nods in Natural Language Processing, pages 3661–\n3672, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics. Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio\nPetroni, Vladimir Karpukhin, Naman Goyal, Hein-\nrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rock-\ntäschel, et al.", "chunk_type": "semantic", "chunker": "semantic_adjacent_v1"}
{"chunk_id": "drag_semantic_p11_c95", "pdf_name": "drag", "page_number": 11, "char_count": 332, "text": "2020. Retrieval-augmented generation\nfor knowledge-intensive nlp tasks. Advances in Neu-\nral Information Processing Systems, 33:9459–9474. Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio\nPetroni, Vladimir Karpukhin, Naman Goyal, Hein-\nrich Küttler, Mike Lewis, Wen tau Yih, Tim Rock-\ntäschel, Sebastian Riedel, and Douwe Kiela.", "chunk_type": "semantic", "chunker": "semantic_adjacent_v1"}
{"chunk_id": "drag_semantic_p11_c96", "pdf_name": "drag", "page_number": 11, "char_count": 366, "text": "2021. Retrieval-augmented generation for knowledge-\nintensive nlp tasks. Sheng-Chieh Lin, Akari Asai, Minghan Li, Barlas Oguz,\nJimmy Lin, Yashar Mehdad, Wen-tau Yih, and Xilun\nChen. 2023. How to Train Your DRAGON: Di-\nverse Augmentation Towards Generalizable Dense\nRetrieval. In Proceedings of the 2023 Conference on\nEmpirical Methods in Natural Language Processing.", "chunk_type": "semantic", "chunker": "semantic_adjacent_v1"}
{"chunk_id": "drag_semantic_p11_c97", "pdf_name": "drag", "page_number": 11, "char_count": 313, "text": "Nelson F. Liu, Kevin Lin, John Hewitt, Ashwin Paran-\njape, Michele Bevilacqua, Fabio Petroni, and Percy\nLiang. 2023. Lost in the middle: How language\nmodels use long contexts. Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov.", "chunk_type": "semantic", "chunker": "semantic_adjacent_v1"}
{"chunk_id": "drag_semantic_p11_c98", "pdf_name": "drag", "page_number": 11, "char_count": 349, "text": "2020. Ro{bert}a: A robustly optimized {bert} pretraining\napproach. Yi Luan, Jacob Eisenstein, Kristina Toutanova, and\nMichael Collins. 2021. Sparse, dense, and attentional\nrepresentations for text retrieval. Transactions of the\nAssociation for Computational Linguistics, 9:329–\n345. Kaixin Ma, Hao Cheng, Xiaodong Liu, Eric Nyberg,\nand Jianfeng Gao.", "chunk_type": "semantic", "chunker": "semantic_adjacent_v1"}
{"chunk_id": "drag_semantic_p11_c99", "pdf_name": "drag", "page_number": 11, "char_count": 338, "text": "2022. Open-domain question an-\nswering via chain of reasoning over heterogeneous\nknowledge. In Findings of the Association for Com-\nputational Linguistics: EMNLP 2022, pages 5360–\n5374, Abu Dhabi, United Arab Emirates. Association\nfor Computational Linguistics. Kaixin Ma, Hao Cheng, Yu Zhang, Xiaodong Liu, Eric\nNyberg, and Jianfeng Gao.", "chunk_type": "semantic", "chunker": "semantic_adjacent_v1"}
{"chunk_id": "drag_semantic_p11_c100", "pdf_name": "drag", "page_number": 11, "char_count": 410, "text": "2023. Chain-of-skills:\nA configurable model for open-domain question an-\nswering. In Proceedings of the 61st Annual Meet-\ning of the Association for Computational Linguistics\n(Volume 1: Long Papers), pages 1599–1618, Toronto,\nCanada. Association for Computational Linguistics. Sewon Min, Kalpesh Krishna, Xinxi Lyu, Mike\nLewis, Wen-tau Yih, Pang Wei Koh, Mohit Iyyer,\nLuke Zettlemoyer, and Hannaneh Hajishirzi.", "chunk_type": "semantic", "chunker": "semantic_adjacent_v1"}
{"chunk_id": "drag_semantic_p11_c101", "pdf_name": "drag", "page_number": 11, "char_count": 321, "text": "2023. FActScore: Fine-grained atomic evaluation of factual\nprecision in long form text generation. In Proceed-\nings of the 2023 Conference on Empirical Methods\nin Natural Language Processing. Sewon Min, Julian Michael, Hannaneh Hajishirzi, and\nLuke Zettlemoyer. 2020. AmbigQA: Answering am-\nbiguous open-domain questions.", "chunk_type": "semantic", "chunker": "semantic_adjacent_v1"}
{"chunk_id": "drag_semantic_p11_c102", "pdf_name": "drag", "page_number": 11, "char_count": 336, "text": "In Proceedings of\nthe 2020 Conference on Empirical Methods in Nat-\nural Language Processing (EMNLP), pages 5783–\n5797, Online. Association for Computational Lin-\nguistics. Stephen Mussmann and Stefano Ermon. 2016. Learning\nand inference via maximum inner product search. In International Conference on Machine Learning,\npages 2587–2596.", "chunk_type": "semantic", "chunker": "semantic_adjacent_v1"}
{"chunk_id": "drag_semantic_p11_c103", "pdf_name": "drag", "page_number": 11, "char_count": 318, "text": "PMLR. Ani Nenkova and Rebecca Passonneau. 2004. Evaluat-\ning content selection in summarization: The pyramid\nmethod. In Proceedings of the Human Language\nTechnology Conference of the North American Chap-\nter of the Association for Computational Linguistics:\nHLT-NAACL 2004, pages 145–152, Boston, Mas-\nsachusetts, USA.", "chunk_type": "semantic", "chunker": "semantic_adjacent_v1"}
{"chunk_id": "drag_semantic_p11_c104", "pdf_name": "drag", "page_number": 11, "char_count": 312, "text": "Association for Computational Lin-\nguistics. Benjamin Newman, Luca Soldaini, Raymond Fok, Ar-\nman Cohan, and Kyle Lo. 2023. A controllable\nqa-based framework for decontextualization. arXiv\npreprint arXiv:2305.14772. Tri Nguyen, Mir Rosenberg, Xia Song, Jianfeng\nGao, Saurabh Tiwary, Rangan Majumder, and\nLi Deng.", "chunk_type": "semantic", "chunker": "semantic_adjacent_v1"}
{"chunk_id": "drag_semantic_p11_c105", "pdf_name": "drag", "page_number": 11, "char_count": 435, "text": "2016. MS MARCO: A human gener-\nated machine reading comprehension dataset. CoRR,\nabs/1611.09268. Jianmo Ni, Chen Qu, Jing Lu, Zhuyun Dai, Gustavo\nHernandez Abrego, Ji Ma, Vincent Zhao, Yi Luan,\nKeith Hall, Ming-Wei Chang, and Yinfei Yang. 2022. Large dual encoders are generalizable retrievers. In\nProceedings of the 2022 Conference on Empirical\nMethods in Natural Language Processing, pages\n9844–9855, Abu Dhabi, United Arab Emirates.", "chunk_type": "semantic", "chunker": "semantic_adjacent_v1"}
{"chunk_id": "drag_semantic_p11_c106", "pdf_name": "drag", "page_number": 11, "char_count": 709, "text": "As-\nsociation for Computational Linguistics. Barlas Oguz, Kushal Lakhotia, Anchit Gupta, Patrick\nLewis, Vladimir Karpukhin, Aleksandra Piktus,\nXilun Chen, Sebastian Riedel, Scott Yih, Sonal\nGupta, and Yashar Mehdad. 2022. Domain-matched\npre-training tasks for dense retrieval. In Findings\nof the Association for Computational Linguistics:\nNAACL 2022, pages 1524–1534, Seattle, United\nStates. Association for Computational Linguistics. OpenAI. 2023. Gpt-4 technical report. ArXiv,\nabs/2303.08774. Fabio Petroni, Aleksandra Piktus, Angela Fan, Patrick\nLewis, Majid Yazdani, Nicola De Cao, James Thorne,\nYacine Jernite, Vladimir Karpukhin, Jean Maillard,\nVassilis Plachouras, Tim Rocktäschel, and Sebastian\n15169", "chunk_type": "semantic", "chunker": "semantic_adjacent_v1"}
{"chunk_id": "drag_fixed_p12_c66", "pdf_name": "drag", "page_number": 12, "char_count": 833, "text": "Riedel. 2021. KILT: a benchmark for knowledge\nintensive language tasks. In Proceedings of the 2021\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies, pages 2523–2544, Online. Association for Computational Linguistics. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J Liu. 2020. Exploring the limits\nof transfer learning with a unified text-to-text trans-\nformer. The Journal of Machine Learning Research,\n21(1):5485–5551. Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and\nPercy Liang. 2016. Squad: 100,000+ questions\nfor machine comprehension of text. arXiv preprint\narXiv:1606.05250. Nils Reimers and Iryna Gurevych. 2019. Sentence-\nBERT: Sentence embeddings using Siamese BERT-\nnetworks.", "chunk_type": "fixed", "chunker": "fixed_recursive_v1"}
{"chunk_id": "drag_fixed_p12_c67", "pdf_name": "drag", "page_number": 12, "char_count": 964, "text": "Sentence-\nBERT: Sentence embeddings using Siamese BERT-\nnetworks. In Proceedings of the 2019 Conference on\nEmpirical Methods in Natural Language Processing\nand the 9th International Joint Conference on Natu-\nral Language Processing (EMNLP-IJCNLP), pages\n3982–3992, Hong Kong, China. Association for Com-\nputational Linguistics. Devendra Sachan, Mostofa Patwary, Mohammad\nShoeybi, Neel Kant, Wei Ping, William L. Hamil-\nton, and Bryan Catanzaro. 2021. End-to-end training\nof neural retrievers for open-domain question answer-\ning. In Proceedings of the 59th Annual Meeting of the\nAssociation for Computational Linguistics and the\n11th International Joint Conference on Natural Lan-\nguage Processing (Volume 1: Long Papers), pages\n6648–6662, Online. Association for Computational\nLinguistics. Keshav Santhanam, Omar Khattab, Jon Saad-Falcon,\nChristopher Potts, and Matei Zaharia. 2022. Col-\nBERTv2:\nEffective and efficient retrieval via\nlightweight late interaction.", "chunk_type": "fixed", "chunker": "fixed_recursive_v1"}
{"chunk_id": "drag_fixed_p12_c68", "pdf_name": "drag", "page_number": 12, "char_count": 902, "text": "Col-\nBERTv2:\nEffective and efficient retrieval via\nlightweight late interaction. In Proceedings of the\n2022 Conference of the North American Chapter of\nthe Association for Computational Linguistics: Hu-\nman Language Technologies, pages 3715–3734, Seat-\ntle, United States. Association for Computational\nLinguistics. Parth Sarthi, Salman Abdullah, Aditi Tuli, Shubh\nKhanna, Anna Goldie, and Christopher D Manning. 2024. RAPTOR: Recursive abstractive processing\nfor tree-organized retrieval. In The Twelfth Interna-\ntional Conference on Learning Representations. Christopher Sciavolino, Zexuan Zhong, Jinhyuk Lee,\nand Danqi Chen. 2021. Simple entity-centric ques-\ntions challenge dense retrievers. arXiv preprint\narXiv:2109.08535. Minjoon Seo, Jinhyuk Lee, Tom Kwiatkowski, Ankur\nParikh, Ali Farhadi, and Hannaneh Hajishirzi. 2019. Real-time open-domain question answering with\ndense-sparse phrase index.", "chunk_type": "fixed", "chunker": "fixed_recursive_v1"}
{"chunk_id": "drag_fixed_p12_c69", "pdf_name": "drag", "page_number": 12, "char_count": 803, "text": "Real-time open-domain question answering with\ndense-sparse phrase index. In Proceedings of the\n57th Annual Meeting of the Association for Computa-\ntional Linguistics, pages 4430–4441, Florence, Italy. Association for Computational Linguistics. Freda Shi, Xinyun Chen, Kanishka Misra, Nathan\nScales, David Dohan, Ed H Chi, Nathanael Schärli,\nand Denny Zhou. 2023. Large language models can\nbe easily distracted by irrelevant context. In Inter-\nnational Conference on Machine Learning, pages\n31210–31227. PMLR. Nandan Thakur, Nils Reimers, Andreas Rücklé, Ab-\nhishek Srivastava, and Iryna Gurevych. 2021. Beir:\nA heterogeneous benchmark for zero-shot evaluation\nof information retrieval models. In Thirty-fifth Con-\nference on Neural Information Processing Systems\nDatasets and Benchmarks Track (Round 2).", "chunk_type": "fixed", "chunker": "fixed_recursive_v1"}
{"chunk_id": "drag_fixed_p12_c70", "pdf_name": "drag", "page_number": 12, "char_count": 1161, "text": "In Thirty-fifth Con-\nference on Neural Information Processing Systems\nDatasets and Benchmarks Track (Round 2). Hugo Touvron, Louis Martin, Kevin Stone, Peter Al-\nbert, Amjad Almahairi, Yasmine Babaei, Nikolay\nBashlykov, Soumya Batra, Prajjwal Bhargava, Shruti\nBhosale, Dan Bikel, Lukas Blecher, Cristian Canton\nFerrer, Moya Chen, Guillem Cucurull, David Esiobu,\nJude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller,\nCynthia Gao, Vedanuj Goswami, Naman Goyal, An-\nthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan\nInan, Marcin Kardas, Viktor Kerkez, Madian Khabsa,\nIsabel Kloumann, Artem Korenev, Punit Singh Koura,\nMarie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Di-\nana Liskovich, Yinghai Lu, Yuning Mao, Xavier Mar-\ntinet, Todor Mihaylov, Pushkar Mishra, Igor Moly-\nbog, Yixin Nie, Andrew Poulton, Jeremy Reizen-\nstein, Rashi Rungta, Kalyan Saladi, Alan Schelten,\nRuan Silva, Eric Michael Smith, Ranjan Subrama-\nnian, Xiaoqing Ellen Tan, Binh Tang, Ross Tay-\nlor, Adina Williams, Jian Xiang Kuan, Puxin Xu,\nZheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan,\nMelanie Kambadur, Sharan Narang, Aurelien Ro-\ndriguez, Robert Stojnic, Sergey Edunov, and Thomas\nScialom.", "chunk_type": "fixed", "chunker": "fixed_recursive_v1"}
{"chunk_id": "drag_fixed_p12_c71", "pdf_name": "drag", "page_number": 12, "char_count": 975, "text": "Adina Williams, Jian Xiang Kuan, Puxin Xu,\nZheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan,\nMelanie Kambadur, Sharan Narang, Aurelien Ro-\ndriguez, Robert Stojnic, Sergey Edunov, and Thomas\nScialom. 2023. Llama 2: Open foundation and fine-\ntuned chat models. Kexin Wang, Nandan Thakur, Nils Reimers, and Iryna\nGurevych. 2022. GPL: Generative pseudo labeling\nfor unsupervised domain adaptation of dense retrieval. In Proceedings of the 2022 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\npages 2345–2360, Seattle, United States. Association\nfor Computational Linguistics. Zhiruo Wang, Jun Araki, Zhengbao Jiang, Md Rizwan\nParvez, and Graham Neubig. 2023. Learning to filter\ncontext for retrieval-augmented generation. Ji Xin, Chenyan Xiong, Ashwin Srinivasan, Ankita\nSharma, Damien Jose, and Paul Bennett. 2022. Zero-\nshot dense retrieval with momentum adversarial do-\nmain invariant representations.", "chunk_type": "fixed", "chunker": "fixed_recursive_v1"}
{"chunk_id": "drag_fixed_p12_c72", "pdf_name": "drag", "page_number": 12, "char_count": 384, "text": "Zero-\nshot dense retrieval with momentum adversarial do-\nmain invariant representations. In Findings of the As-\nsociation for Computational Linguistics: ACL 2022,\npages 4008–4020, Dublin, Ireland. Association for\nComputational Linguistics. Fangyuan Xu, Weijia Shi, and Eunsol Choi. 2023. Re-\ncomp: Improving retrieval-augmented lms with com-\npression and selective augmentation. 15170", "chunk_type": "fixed", "chunker": "fixed_recursive_v1"}
{"chunk_id": "drag_semantic_p12_c107", "pdf_name": "drag", "page_number": 12, "char_count": 411, "text": "Riedel. 2021. KILT: a benchmark for knowledge\nintensive language tasks. In Proceedings of the 2021\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies, pages 2523–2544, Online. Association for Computational Linguistics. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J Liu.", "chunk_type": "semantic", "chunker": "semantic_adjacent_v1"}
{"chunk_id": "drag_semantic_p12_c108", "pdf_name": "drag", "page_number": 12, "char_count": 316, "text": "2020. Exploring the limits\nof transfer learning with a unified text-to-text trans-\nformer. The Journal of Machine Learning Research,\n21(1):5485–5551. Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and\nPercy Liang. 2016. Squad: 100,000+ questions\nfor machine comprehension of text. arXiv preprint\narXiv:1606.05250.", "chunk_type": "semantic", "chunker": "semantic_adjacent_v1"}
{"chunk_id": "drag_semantic_p12_c109", "pdf_name": "drag", "page_number": 12, "char_count": 321, "text": "Nils Reimers and Iryna Gurevych. 2019. Sentence-\nBERT: Sentence embeddings using Siamese BERT-\nnetworks. In Proceedings of the 2019 Conference on\nEmpirical Methods in Natural Language Processing\nand the 9th International Joint Conference on Natu-\nral Language Processing (EMNLP-IJCNLP), pages\n3982–3992, Hong Kong, China.", "chunk_type": "semantic", "chunker": "semantic_adjacent_v1"}
{"chunk_id": "drag_semantic_p12_c110", "pdf_name": "drag", "page_number": 12, "char_count": 507, "text": "Association for Com-\nputational Linguistics. Devendra Sachan, Mostofa Patwary, Mohammad\nShoeybi, Neel Kant, Wei Ping, William L. Hamil-\nton, and Bryan Catanzaro. 2021. End-to-end training\nof neural retrievers for open-domain question answer-\ning. In Proceedings of the 59th Annual Meeting of the\nAssociation for Computational Linguistics and the\n11th International Joint Conference on Natural Lan-\nguage Processing (Volume 1: Long Papers), pages\n6648–6662, Online. Association for Computational\nLinguistics.", "chunk_type": "semantic", "chunker": "semantic_adjacent_v1"}
{"chunk_id": "drag_semantic_p12_c111", "pdf_name": "drag", "page_number": 12, "char_count": 408, "text": "Keshav Santhanam, Omar Khattab, Jon Saad-Falcon,\nChristopher Potts, and Matei Zaharia. 2022. Col-\nBERTv2:\nEffective and efficient retrieval via\nlightweight late interaction. In Proceedings of the\n2022 Conference of the North American Chapter of\nthe Association for Computational Linguistics: Hu-\nman Language Technologies, pages 3715–3734, Seat-\ntle, United States. Association for Computational\nLinguistics.", "chunk_type": "semantic", "chunker": "semantic_adjacent_v1"}
{"chunk_id": "drag_semantic_p12_c112", "pdf_name": "drag", "page_number": 12, "char_count": 311, "text": "Parth Sarthi, Salman Abdullah, Aditi Tuli, Shubh\nKhanna, Anna Goldie, and Christopher D Manning. 2024. RAPTOR: Recursive abstractive processing\nfor tree-organized retrieval. In The Twelfth Interna-\ntional Conference on Learning Representations. Christopher Sciavolino, Zexuan Zhong, Jinhyuk Lee,\nand Danqi Chen.", "chunk_type": "semantic", "chunker": "semantic_adjacent_v1"}
{"chunk_id": "drag_semantic_p12_c113", "pdf_name": "drag", "page_number": 12, "char_count": 445, "text": "2021. Simple entity-centric ques-\ntions challenge dense retrievers. arXiv preprint\narXiv:2109.08535. Minjoon Seo, Jinhyuk Lee, Tom Kwiatkowski, Ankur\nParikh, Ali Farhadi, and Hannaneh Hajishirzi. 2019. Real-time open-domain question answering with\ndense-sparse phrase index. In Proceedings of the\n57th Annual Meeting of the Association for Computa-\ntional Linguistics, pages 4430–4441, Florence, Italy. Association for Computational Linguistics.", "chunk_type": "semantic", "chunker": "semantic_adjacent_v1"}
{"chunk_id": "drag_semantic_p12_c114", "pdf_name": "drag", "page_number": 12, "char_count": 352, "text": "Freda Shi, Xinyun Chen, Kanishka Misra, Nathan\nScales, David Dohan, Ed H Chi, Nathanael Schärli,\nand Denny Zhou. 2023. Large language models can\nbe easily distracted by irrelevant context. In Inter-\nnational Conference on Machine Learning, pages\n31210–31227. PMLR. Nandan Thakur, Nils Reimers, Andreas Rücklé, Ab-\nhishek Srivastava, and Iryna Gurevych.", "chunk_type": "semantic", "chunker": "semantic_adjacent_v1"}
{"chunk_id": "drag_semantic_p12_c115", "pdf_name": "drag", "page_number": 12, "char_count": 1257, "text": "2021. Beir:\nA heterogeneous benchmark for zero-shot evaluation\nof information retrieval models. In Thirty-fifth Con-\nference on Neural Information Processing Systems\nDatasets and Benchmarks Track (Round 2). Hugo Touvron, Louis Martin, Kevin Stone, Peter Al-\nbert, Amjad Almahairi, Yasmine Babaei, Nikolay\nBashlykov, Soumya Batra, Prajjwal Bhargava, Shruti\nBhosale, Dan Bikel, Lukas Blecher, Cristian Canton\nFerrer, Moya Chen, Guillem Cucurull, David Esiobu,\nJude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller,\nCynthia Gao, Vedanuj Goswami, Naman Goyal, An-\nthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan\nInan, Marcin Kardas, Viktor Kerkez, Madian Khabsa,\nIsabel Kloumann, Artem Korenev, Punit Singh Koura,\nMarie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Di-\nana Liskovich, Yinghai Lu, Yuning Mao, Xavier Mar-\ntinet, Todor Mihaylov, Pushkar Mishra, Igor Moly-\nbog, Yixin Nie, Andrew Poulton, Jeremy Reizen-\nstein, Rashi Rungta, Kalyan Saladi, Alan Schelten,\nRuan Silva, Eric Michael Smith, Ranjan Subrama-\nnian, Xiaoqing Ellen Tan, Binh Tang, Ross Tay-\nlor, Adina Williams, Jian Xiang Kuan, Puxin Xu,\nZheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan,\nMelanie Kambadur, Sharan Narang, Aurelien Ro-\ndriguez, Robert Stojnic, Sergey Edunov, and Thomas\nScialom.", "chunk_type": "semantic", "chunker": "semantic_adjacent_v1"}
{"chunk_id": "drag_semantic_p12_c116", "pdf_name": "drag", "page_number": 12, "char_count": 446, "text": "2023. Llama 2: Open foundation and fine-\ntuned chat models. Kexin Wang, Nandan Thakur, Nils Reimers, and Iryna\nGurevych. 2022. GPL: Generative pseudo labeling\nfor unsupervised domain adaptation of dense retrieval. In Proceedings of the 2022 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\npages 2345–2360, Seattle, United States. Association\nfor Computational Linguistics.", "chunk_type": "semantic", "chunker": "semantic_adjacent_v1"}
{"chunk_id": "drag_semantic_p12_c117", "pdf_name": "drag", "page_number": 12, "char_count": 624, "text": "Zhiruo Wang, Jun Araki, Zhengbao Jiang, Md Rizwan\nParvez, and Graham Neubig. 2023. Learning to filter\ncontext for retrieval-augmented generation. Ji Xin, Chenyan Xiong, Ashwin Srinivasan, Ankita\nSharma, Damien Jose, and Paul Bennett. 2022. Zero-\nshot dense retrieval with momentum adversarial do-\nmain invariant representations. In Findings of the As-\nsociation for Computational Linguistics: ACL 2022,\npages 4008–4020, Dublin, Ireland. Association for\nComputational Linguistics. Fangyuan Xu, Weijia Shi, and Eunsol Choi. 2023. Re-\ncomp: Improving retrieval-augmented lms with com-\npression and selective augmentation. 15170", "chunk_type": "semantic", "chunker": "semantic_adjacent_v1"}
{"chunk_id": "drag_fixed_p13_c73", "pdf_name": "drag", "page_number": 13, "char_count": 941, "text": "Yinfei Yang, Daniel Cer, Amin Ahmad, Mandy Guo,\nJax Law, Noah Constant, Gustavo Hernandez Abrego,\nSteve Yuan, Chris Tar, Yun-Hsuan Sung, et al. 2020. Multilingual universal sentence encoder for semantic\nretrieval. In Proceedings of the 58th Annual Meet-\ning of the Association for Computational Linguistics:\nSystem Demonstrations, pages 87–94. Wen-tau Yih, Kristina Toutanova, John C. Platt, and\nChristopher Meek. 2011. Learning discriminative\nprojections for text similarity measures. In Proceed-\nings of the Fifteenth Conference on Computational\nNatural Language Learning, pages 247–256, Port-\nland, Oregon, USA. Association for Computational\nLinguistics. Wenhao Yu, Dan Iter, Shuohang Wang, Yichong Xu,\nMingxuan Ju, Soumya Sanyal, Chenguang Zhu,\nMichael Zeng, and Meng Jiang. 2023a. Generate\nrather than retrieve: Large language models are\nstrong context generators. In The Eleventh Inter-\nnational Conference on Learning Representations.", "chunk_type": "fixed", "chunker": "fixed_recursive_v1"}
{"chunk_id": "drag_fixed_p13_c74", "pdf_name": "drag", "page_number": 13, "char_count": 801, "text": "In The Eleventh Inter-\nnational Conference on Learning Representations. Wenhao Yu, Hongming Zhang, Xiaoman Pan, Kaixin\nMa, Hongwei Wang, and Dong Yu. 2023b. Chain-of-\nnote: Enhancing robustness in retrieval-augmented\nlanguage models. arXiv preprint arXiv:2311.09210. Shunyu Zhang, Yaobo Liang, Ming Gong, Daxin Jiang,\nand Nan Duan. 2022. Multi-view document repre-\nsentation learning for open-domain dense retrieval. In Proceedings of the 60th Annual Meeting of the\nAssociation for Computational Linguistics (Volume\n1: Long Papers), pages 5990–6000, Dublin, Ireland. Association for Computational Linguistics. Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q. Weinberger, and Yoav Artzi. 2020. Bertscore: Eval-\nuating text generation with bert. In International\nConference on Learning Representations.", "chunk_type": "fixed", "chunker": "fixed_recursive_v1"}
{"chunk_id": "drag_fixed_p13_c75", "pdf_name": "drag", "page_number": 13, "char_count": 936, "text": "In International\nConference on Learning Representations. A\nRetrieval Corpus Processing\nThe English Wikipedia dump used in this study, re-\nleased by Bohnet et al., 2022, was selected because\nit has been filtered to remove figures, tables, and\nlists, and is organized into paragraphs. The dump\ndates back to October 13, 2021. We have seg-\nmented Wikipedia into three retrieval units for this\nstudy: 100-word passage chunks, sentences, and\npropositions. Paragraphs are divided into 100-word\npassage chunks using a greedy method. We divide\nonly at the end of sentences to ensure each passage\nchunk contains complete sentences. As we process\nthe paragraph, we add sentences one by one. If in-\ncluding the next sentence causes the passage chunk\nto exceed 100 words, we start a new passage chunk\nwith that sentence. However, if the final passage\nchunk is shorter than 50 words, we merge it with\nthe previous one to avoid overly small segments.", "chunk_type": "fixed", "chunker": "fixed_recursive_v1"}
{"chunk_id": "drag_fixed_p13_c76", "pdf_name": "drag", "page_number": 13, "char_count": 942, "text": "However, if the final passage\nchunk is shorter than 50 words, we merge it with\nthe previous one to avoid overly small segments. Each passage is further segmented into sentences\nusing the widely used Python SpaCy 1 en_cor\ne_web_lg model. Additionally, each passage is\ndecomposed into propositions by our Proposition-\nizer model. Decomposing the entire Wikipedia\ncorpus requires approximately 500 GPU hours on\nNVIDIA P100 GPUs using the default implementa-\ntion in the transformers2 package. We decomposed\n6 million pages into 41 million passages, 114 mil-\nlion sentences, and 257 million propositions. On\naverage, a passage contains 6.3 propositions, and a\nsentence contains 2.3 propositions. B\nTraining the Propositionizer\nWe generated a list of propositions from a given\nparagraph using GPT-4 with a prompt, as shown in\nFigure 8. After filtering, 42,857 pairs were used to\nfine-tune a Flan-T5-Large model. We named the\nmodel Propositionizer.", "chunk_type": "fixed", "chunker": "fixed_recursive_v1"}
{"chunk_id": "drag_fixed_p13_c77", "pdf_name": "drag", "page_number": 13, "char_count": 677, "text": "We named the\nmodel Propositionizer. The AdamW optimizer was\nused with a batch size of 64, learning rate of 1e-4,\nweight decay of 1e-4, and 3 epochs. To compare the proposition generation perfor-\nmance of different models, we set up a development\nset and an evaluation metric. The development set\ncontains an additional 1,000 pairs collected by GPT-\n4 using the same approach as the training set. We\nevaluated the quality of the predicted propositions\nby the F1 score of two sets of propositions. Mo-\ntivated by the F1 score of two sets of tokens in\nBertScore, we designed the F1 score for two sets of\n1https://spacy.io/\n2https://huggingface.co/docs/\ntransformers/en/index\n15171", "chunk_type": "fixed", "chunker": "fixed_recursive_v1"}
{"chunk_id": "drag_semantic_p13_c118", "pdf_name": "drag", "page_number": 13, "char_count": 343, "text": "Yinfei Yang, Daniel Cer, Amin Ahmad, Mandy Guo,\nJax Law, Noah Constant, Gustavo Hernandez Abrego,\nSteve Yuan, Chris Tar, Yun-Hsuan Sung, et al. 2020. Multilingual universal sentence encoder for semantic\nretrieval. In Proceedings of the 58th Annual Meet-\ning of the Association for Computational Linguistics:\nSystem Demonstrations, pages 87–94.", "chunk_type": "semantic", "chunker": "semantic_adjacent_v1"}
{"chunk_id": "drag_semantic_p13_c119", "pdf_name": "drag", "page_number": 13, "char_count": 313, "text": "Wen-tau Yih, Kristina Toutanova, John C. Platt, and\nChristopher Meek. 2011. Learning discriminative\nprojections for text similarity measures. In Proceed-\nings of the Fifteenth Conference on Computational\nNatural Language Learning, pages 247–256, Port-\nland, Oregon, USA. Association for Computational\nLinguistics.", "chunk_type": "semantic", "chunker": "semantic_adjacent_v1"}
{"chunk_id": "drag_semantic_p13_c120", "pdf_name": "drag", "page_number": 13, "char_count": 361, "text": "Wenhao Yu, Dan Iter, Shuohang Wang, Yichong Xu,\nMingxuan Ju, Soumya Sanyal, Chenguang Zhu,\nMichael Zeng, and Meng Jiang. 2023a. Generate\nrather than retrieve: Large language models are\nstrong context generators. In The Eleventh Inter-\nnational Conference on Learning Representations. Wenhao Yu, Hongming Zhang, Xiaoman Pan, Kaixin\nMa, Hongwei Wang, and Dong Yu.", "chunk_type": "semantic", "chunker": "semantic_adjacent_v1"}
{"chunk_id": "drag_semantic_p13_c121", "pdf_name": "drag", "page_number": 13, "char_count": 459, "text": "2023b. Chain-of-\nnote: Enhancing robustness in retrieval-augmented\nlanguage models. arXiv preprint arXiv:2311.09210. Shunyu Zhang, Yaobo Liang, Ming Gong, Daxin Jiang,\nand Nan Duan. 2022. Multi-view document repre-\nsentation learning for open-domain dense retrieval. In Proceedings of the 60th Annual Meeting of the\nAssociation for Computational Linguistics (Volume\n1: Long Papers), pages 5990–6000, Dublin, Ireland. Association for Computational Linguistics.", "chunk_type": "semantic", "chunker": "semantic_adjacent_v1"}
{"chunk_id": "drag_semantic_p13_c122", "pdf_name": "drag", "page_number": 13, "char_count": 417, "text": "Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q. Weinberger, and Yoav Artzi. 2020. Bertscore: Eval-\nuating text generation with bert. In International\nConference on Learning Representations. A\nRetrieval Corpus Processing\nThe English Wikipedia dump used in this study, re-\nleased by Bohnet et al., 2022, was selected because\nit has been filtered to remove figures, tables, and\nlists, and is organized into paragraphs.", "chunk_type": "semantic", "chunker": "semantic_adjacent_v1"}
{"chunk_id": "drag_semantic_p13_c123", "pdf_name": "drag", "page_number": 13, "char_count": 397, "text": "The dump\ndates back to October 13, 2021. We have seg-\nmented Wikipedia into three retrieval units for this\nstudy: 100-word passage chunks, sentences, and\npropositions. Paragraphs are divided into 100-word\npassage chunks using a greedy method. We divide\nonly at the end of sentences to ensure each passage\nchunk contains complete sentences. As we process\nthe paragraph, we add sentences one by one.", "chunk_type": "semantic", "chunker": "semantic_adjacent_v1"}
{"chunk_id": "drag_semantic_p13_c124", "pdf_name": "drag", "page_number": 13, "char_count": 364, "text": "If in-\ncluding the next sentence causes the passage chunk\nto exceed 100 words, we start a new passage chunk\nwith that sentence. However, if the final passage\nchunk is shorter than 50 words, we merge it with\nthe previous one to avoid overly small segments. Each passage is further segmented into sentences\nusing the widely used Python SpaCy 1 en_cor\ne_web_lg model.", "chunk_type": "semantic", "chunker": "semantic_adjacent_v1"}
{"chunk_id": "drag_semantic_p13_c125", "pdf_name": "drag", "page_number": 13, "char_count": 363, "text": "Additionally, each passage is\ndecomposed into propositions by our Proposition-\nizer model. Decomposing the entire Wikipedia\ncorpus requires approximately 500 GPU hours on\nNVIDIA P100 GPUs using the default implementa-\ntion in the transformers2 package. We decomposed\n6 million pages into 41 million passages, 114 mil-\nlion sentences, and 257 million propositions.", "chunk_type": "semantic", "chunker": "semantic_adjacent_v1"}
{"chunk_id": "drag_semantic_p13_c126", "pdf_name": "drag", "page_number": 13, "char_count": 305, "text": "On\naverage, a passage contains 6.3 propositions, and a\nsentence contains 2.3 propositions. B\nTraining the Propositionizer\nWe generated a list of propositions from a given\nparagraph using GPT-4 with a prompt, as shown in\nFigure 8. After filtering, 42,857 pairs were used to\nfine-tune a Flan-T5-Large model.", "chunk_type": "semantic", "chunker": "semantic_adjacent_v1"}
{"chunk_id": "drag_semantic_p13_c127", "pdf_name": "drag", "page_number": 13, "char_count": 677, "text": "We named the\nmodel Propositionizer. The AdamW optimizer was\nused with a batch size of 64, learning rate of 1e-4,\nweight decay of 1e-4, and 3 epochs. To compare the proposition generation perfor-\nmance of different models, we set up a development\nset and an evaluation metric. The development set\ncontains an additional 1,000 pairs collected by GPT-\n4 using the same approach as the training set. We\nevaluated the quality of the predicted propositions\nby the F1 score of two sets of propositions. Mo-\ntivated by the F1 score of two sets of tokens in\nBertScore, we designed the F1 score for two sets of\n1https://spacy.io/\n2https://huggingface.co/docs/\ntransformers/en/index\n15171", "chunk_type": "semantic", "chunker": "semantic_adjacent_v1"}
{"chunk_id": "drag_fixed_p14_c78", "pdf_name": "drag", "page_number": 14, "char_count": 946, "text": "propositions. Let P = {p1, ..., pn} denote the set\nof labeled propositions and ˆP = {ˆp1, ..., ˆpm} the\nset of predicted propositions. We use sim(pi, ˆpj)\nto represent the similarity between two proposi-\ntions. Theoretically, any text similarity metric can\nbe used. We chose BertScore (Zhang et al., 2020)\nwith roberta-large (Liu et al., 2020) configuration\nas our sim function since we wanted our metric\nto reflect the semantic difference between proposi-\ntions. We define\nRecall =\n1\n|P|\nX\npi∈P\nmax\nˆpj∈ˆP\nsim(pi, ˆpj)\nPrecision =\n1\n| ˆP|\nX\nˆpj∈ˆP\nmax\npi∈P sim(pi, ˆpj)\nF1 = 2 · Precision · Recall\nPrecision + Recall\nHere is a figurative explanation of the F1 score:\nRecall represents the percentage of propositions\nin the labeled set that are similar to those in the\ngenerated set, Precision represents the percentage\nof propositions in the generated set that are similar\nto the labeled set, and F1 is the harmonic mean of\nRecall and Precision.", "chunk_type": "fixed", "chunker": "fixed_recursive_v1"}
{"chunk_id": "drag_fixed_p14_c79", "pdf_name": "drag", "page_number": 14, "char_count": 945, "text": "imilar to those in the\ngenerated set, Precision represents the percentage\nof propositions in the generated set that are similar\nto the labeled set, and F1 is the harmonic mean of\nRecall and Precision. F1 is 1 if the two sets are\nexactly the same, and 0 if any two propositions are\nsemantically different. We conducted a comparative analysis of base-\nsize and large-size Flan-T5 models, which were\ntrained using varying amounts of data (shown in\nFigure 5). Our findings suggest that larger models,\ncoupled with extensive training data, yield better\nresults. The Propositionizer presented in this paper\nattained an F1 score of 0.822. Upon manually\nreviewing the generated propositions, we found\nthem to be satisfactory. 5000\n7500\n10000\n12500\n15000\n17500\nNumber of training samples\n76\n77\n78\n79\n80\n81\nF1\nflan-t5-base\nflan-t5-large\nFigure 5: Performance of proposition-level decompo-\nsition by models with different sizes and number of\ntraining data.", "chunk_type": "fixed", "chunker": "fixed_recursive_v1"}
{"chunk_id": "drag_fixed_p14_c80", "pdf_name": "drag", "page_number": 14, "char_count": 817, "text": "17500\nNumber of training samples\n76\n77\n78\n79\n80\n81\nF1\nflan-t5-base\nflan-t5-large\nFigure 5: Performance of proposition-level decompo-\nsition by models with different sizes and number of\ntraining data. C\nQuality Analysis of Generated\nPropositions\nWe collected propositions generated from 50 ran-\ndomly selected passages. There are 408 and 445\npropositions generated by GPT-4 and Proposition-\nizer, respectively. The propositions and passages\nwere provided to an expert without knowing which\nmodel generated each proposition. The expert an-\nnotated three scores from different perspectives for\neach proposition: (1) whether the proposition is\nfully supported by the passage, (2) whether the\nproposition is minimal and cannot be further split\ninto separate propositions, and (3) whether the\nproposition is self-contained.", "chunk_type": "fixed", "chunker": "fixed_recursive_v1"}
{"chunk_id": "drag_fixed_p14_c81", "pdf_name": "drag", "page_number": 14, "char_count": 948, "text": "her the proposition is\nfully supported by the passage, (2) whether the\nproposition is minimal and cannot be further split\ninto separate propositions, and (3) whether the\nproposition is self-contained. The scores range\nfrom 1 to 3, where 1 means \"no,\" 2 means \"maybe,\"\nand 3 means \"yes.\" We report the number of cases\nwhere the annotation was \"no.\" The detailed in-\nstructions are provided in Table 8. D\nOffline Indexing\nWe used the pyserini and faiss packages\nto encode retrieval units into embeddings. We\nexploited multiple GPUs to encode each text\nunit in groups of 1M units with a batch size\nof 64. After preprocessing the embeddings,\nwe used an exact search for the inner product\n(faiss.IndexFlatIP) in all experiments. The\nplain index of FACTOIDWIKIis approximately\n768GB in size. To reduce memory pressure, the\nembeddings are split into 8 shards. An approximate\nnearest neighbor search is conducted per shard be-\nfore aggregating all results.", "chunk_type": "fixed", "chunker": "fixed_recursive_v1"}
{"chunk_id": "drag_fixed_p14_c82", "pdf_name": "drag", "page_number": 14, "char_count": 862, "text": "An approximate\nnearest neighbor search is conducted per shard be-\nfore aggregating all results. Although the number of propositions is six times\nthat of passages, using efficient indexing tech-\nniques can enable sub-linear search times relative\nto the total count of vectors. Moreover, utilizing\nGPU parallelism and distributed indexes signifi-\ncantly decreases the online search time. As a result,\nwith proper implementation, we can make propo-\nsition retrieval a practically viable and efficient\noption. E\nRetriever Models and QA Models\nWe used transformers and sentence-tra\nnsformers packages for the model implementa-\ntion. We used the following checkpoints released\non HuggingFace: SimCSE (princeton-nlp/u\nnsup-simcse-bert-base-uncased), Con-\ntriever (facebook/contriever), DPR (fac\nebook/dpr-ctx_encoder-multiset-ba\nse, facebook/dpr-question_encoder-\n15172", "chunk_type": "fixed", "chunker": "fixed_recursive_v1"}
{"chunk_id": "drag_semantic_p14_c128", "pdf_name": "drag", "page_number": 14, "char_count": 463, "text": "propositions. Let P = {p1, ..., pn} denote the set\nof labeled propositions and ˆP = {ˆp1, ..., ˆpm} the\nset of predicted propositions. We use sim(pi, ˆpj)\nto represent the similarity between two proposi-\ntions. Theoretically, any text similarity metric can\nbe used. We chose BertScore (Zhang et al., 2020)\nwith roberta-large (Liu et al., 2020) configuration\nas our sim function since we wanted our metric\nto reflect the semantic difference between proposi-\ntions.", "chunk_type": "semantic", "chunker": "semantic_adjacent_v1"}
{"chunk_id": "drag_semantic_p14_c129", "pdf_name": "drag", "page_number": 14, "char_count": 482, "text": "We define\nRecall =\n1\n|P|\nX\npi∈P\nmax\nˆpj∈ˆP\nsim(pi, ˆpj)\nPrecision =\n1\n| ˆP|\nX\nˆpj∈ˆP\nmax\npi∈P sim(pi, ˆpj)\nF1 = 2 · Precision · Recall\nPrecision + Recall\nHere is a figurative explanation of the F1 score:\nRecall represents the percentage of propositions\nin the labeled set that are similar to those in the\ngenerated set, Precision represents the percentage\nof propositions in the generated set that are similar\nto the labeled set, and F1 is the harmonic mean of\nRecall and Precision.", "chunk_type": "semantic", "chunker": "semantic_adjacent_v1"}
{"chunk_id": "drag_semantic_p14_c130", "pdf_name": "drag", "page_number": 14, "char_count": 355, "text": "F1 is 1 if the two sets are\nexactly the same, and 0 if any two propositions are\nsemantically different. We conducted a comparative analysis of base-\nsize and large-size Flan-T5 models, which were\ntrained using varying amounts of data (shown in\nFigure 5). Our findings suggest that larger models,\ncoupled with extensive training data, yield better\nresults.", "chunk_type": "semantic", "chunker": "semantic_adjacent_v1"}
{"chunk_id": "drag_semantic_p14_c131", "pdf_name": "drag", "page_number": 14, "char_count": 388, "text": "The Propositionizer presented in this paper\nattained an F1 score of 0.822. Upon manually\nreviewing the generated propositions, we found\nthem to be satisfactory. 5000\n7500\n10000\n12500\n15000\n17500\nNumber of training samples\n76\n77\n78\n79\n80\n81\nF1\nflan-t5-base\nflan-t5-large\nFigure 5: Performance of proposition-level decompo-\nsition by models with different sizes and number of\ntraining data.", "chunk_type": "semantic", "chunker": "semantic_adjacent_v1"}
{"chunk_id": "drag_semantic_p14_c132", "pdf_name": "drag", "page_number": 14, "char_count": 617, "text": "C\nQuality Analysis of Generated\nPropositions\nWe collected propositions generated from 50 ran-\ndomly selected passages. There are 408 and 445\npropositions generated by GPT-4 and Proposition-\nizer, respectively. The propositions and passages\nwere provided to an expert without knowing which\nmodel generated each proposition. The expert an-\nnotated three scores from different perspectives for\neach proposition: (1) whether the proposition is\nfully supported by the passage, (2) whether the\nproposition is minimal and cannot be further split\ninto separate propositions, and (3) whether the\nproposition is self-contained.", "chunk_type": "semantic", "chunker": "semantic_adjacent_v1"}
{"chunk_id": "drag_semantic_p14_c133", "pdf_name": "drag", "page_number": 14, "char_count": 301, "text": "The scores range\nfrom 1 to 3, where 1 means \"no,\" 2 means \"maybe,\"\nand 3 means \"yes.\" We report the number of cases\nwhere the annotation was \"no.\" The detailed in-\nstructions are provided in Table 8. D\nOffline Indexing\nWe used the pyserini and faiss packages\nto encode retrieval units into embeddings.", "chunk_type": "semantic", "chunker": "semantic_adjacent_v1"}
{"chunk_id": "drag_semantic_p14_c134", "pdf_name": "drag", "page_number": 14, "char_count": 349, "text": "We\nexploited multiple GPUs to encode each text\nunit in groups of 1M units with a batch size\nof 64. After preprocessing the embeddings,\nwe used an exact search for the inner product\n(faiss.IndexFlatIP) in all experiments. The\nplain index of FACTOIDWIKIis approximately\n768GB in size. To reduce memory pressure, the\nembeddings are split into 8 shards.", "chunk_type": "semantic", "chunker": "semantic_adjacent_v1"}
{"chunk_id": "drag_semantic_p14_c135", "pdf_name": "drag", "page_number": 14, "char_count": 385, "text": "An approximate\nnearest neighbor search is conducted per shard be-\nfore aggregating all results. Although the number of propositions is six times\nthat of passages, using efficient indexing tech-\nniques can enable sub-linear search times relative\nto the total count of vectors. Moreover, utilizing\nGPU parallelism and distributed indexes signifi-\ncantly decreases the online search time.", "chunk_type": "semantic", "chunker": "semantic_adjacent_v1"}
{"chunk_id": "drag_semantic_p14_c136", "pdf_name": "drag", "page_number": 14, "char_count": 476, "text": "As a result,\nwith proper implementation, we can make propo-\nsition retrieval a practically viable and efficient\noption. E\nRetriever Models and QA Models\nWe used transformers and sentence-tra\nnsformers packages for the model implementa-\ntion. We used the following checkpoints released\non HuggingFace: SimCSE (princeton-nlp/u\nnsup-simcse-bert-base-uncased), Con-\ntriever (facebook/contriever), DPR (fac\nebook/dpr-ctx_encoder-multiset-ba\nse, facebook/dpr-question_encoder-\n15172", "chunk_type": "semantic", "chunker": "semantic_adjacent_v1"}
{"chunk_id": "drag_fixed_p15_c83", "pdf_name": "drag", "page_number": 15, "char_count": 937, "text": "multiset-base), GTR (sentence-trans\nformers/gtr-t5-base). We\nuse\nT5-large\nsize\nFusion-in-decoder\nmodel (nq_reader_large) released by\nthe\nauthors\nin\nhttps://github.com/\nfacebookresearch/FiD. We use Hugging-\nFace checkpoint (meta-llama/Llama-2-7b)\nfor LLaMA-2-7B. F\nAdditional Results\nIn Section 5.2, we demonstrated the advantage\nof retrieval by proposition over retrieval by sen-\ntence, particularly as the population of the entity\ndecreases in EQ. We used the occurrence in the\ntop-1000 paragraphs retrieved by BM25 as a proxy\nfor frequency, rather than counting the number of\nhyperlinks to the entity used in Sciavolino et al.,\n2021. Therefore, the trend in the performance ver-\nsus frequency plot shows some differences (Fig-\nure 6) between our results and those in Sciavolino\net al., 2021. For example, some entities are am-\nbiguous (e.g., 1992, a TV series). In such cases,\nthe occurrence of the surface form of the entity is\nlarge.", "chunk_type": "fixed", "chunker": "fixed_recursive_v1"}
{"chunk_id": "drag_fixed_p15_c84", "pdf_name": "drag", "page_number": 15, "char_count": 965, "text": "In such cases,\nthe occurrence of the surface form of the entity is\nlarge. Simultaneously, questions related to ambigu-\nous entities are challenging to answer, leading to\nlower recall. In Section 6.2, we discussed the recall of an-\nswers in the retrieved text with respect to the con-\ntext length. We further illustrate the performance\ntrends of six dense retrievers, as detailed in Fig-\nure 7. The results indicate that the recall rate of\npropositions consistently outperforms that of sen-\ntences and passages. Our findings lead to the con-\nclusion that question-related density is greater in\nproposition units compared to sentences and pas-\nsages. G\nError Case Study\nTo understand the source of errors from each type\nof retrieval granularity, we present and discuss four\ntypical examples of mistakes in Table 6 and Table 7. With each example, we show the question and its\ncorresponding top-1 retrieved text unit by the GTR\nretriever across the three granularities.", "chunk_type": "fixed", "chunker": "fixed_recursive_v1"}
{"chunk_id": "drag_fixed_p15_c85", "pdf_name": "drag", "page_number": 15, "char_count": 849, "text": "With each example, we show the question and its\ncorresponding top-1 retrieved text unit by the GTR\nretriever across the three granularities. We observe that with passage-level retrieval, the\nambiguity of an entity or its references presents a\nchallenge for dense retrievers, which echoes find-\nings from (Min et al., 2020). For instance, in exam-\nple Q1, the question asks for “Super Bowl 50”, but\nthe retrieved passage and sentence refers to “Super\nBowl 5”. In Example Q2, passage retrieval fails\nto identify the part referring to the correct “atomic\nnumber”. Instead, the top-1 retrieved passage men-\ntions “atomic number” in a different and irrelevant\ncontext to the question. Retrieval by sentences can\nalso have a similar problem as retrieval by passages\nlike Example Q1. Also, retrieval by sentences faces\nanother challenge of lacking context.", "chunk_type": "fixed", "chunker": "fixed_recursive_v1"}
{"chunk_id": "drag_fixed_p15_c86", "pdf_name": "drag", "page_number": 15, "char_count": 771, "text": "Also, retrieval by sentences faces\nanother challenge of lacking context. In Example\nQ3 (shown in Table 7), sentence-based retrieval\nfails as the correct sentence in the retrieved passage\nuses “it” to refer to the pericardial sac. Retrieval by propositions tackles the aforemen-\ntioned problems by ensuring each retrieval unit\ncontains one piece of fact only and necessary con-\ntext is incorporated in the propositions. However,\nproposition-based retrieval faces challenges with\nquestions that involve multi-hop reasoning over\nlong-range textual analysis. In Example Q4 (shown\nin Table 7), the retrieved passage separately de-\nscribes the actor’s name and the character they por-\ntray. There is not a single proposition that entails\nboth the question and the answer. 15173", "chunk_type": "fixed", "chunker": "fixed_recursive_v1"}
{"chunk_id": "drag_semantic_p15_c137", "pdf_name": "drag", "page_number": 15, "char_count": 448, "text": "multiset-base), GTR (sentence-trans\nformers/gtr-t5-base). We\nuse\nT5-large\nsize\nFusion-in-decoder\nmodel (nq_reader_large) released by\nthe\nauthors\nin\nhttps://github.com/\nfacebookresearch/FiD. We use Hugging-\nFace checkpoint (meta-llama/Llama-2-7b)\nfor LLaMA-2-7B. F\nAdditional Results\nIn Section 5.2, we demonstrated the advantage\nof retrieval by proposition over retrieval by sen-\ntence, particularly as the population of the entity\ndecreases in EQ.", "chunk_type": "semantic", "chunker": "semantic_adjacent_v1"}
{"chunk_id": "drag_semantic_p15_c138", "pdf_name": "drag", "page_number": 15, "char_count": 344, "text": "We used the occurrence in the\ntop-1000 paragraphs retrieved by BM25 as a proxy\nfor frequency, rather than counting the number of\nhyperlinks to the entity used in Sciavolino et al.,\n2021. Therefore, the trend in the performance ver-\nsus frequency plot shows some differences (Fig-\nure 6) between our results and those in Sciavolino\net al., 2021.", "chunk_type": "semantic", "chunker": "semantic_adjacent_v1"}
{"chunk_id": "drag_semantic_p15_c139", "pdf_name": "drag", "page_number": 15, "char_count": 366, "text": "For example, some entities are am-\nbiguous (e.g., 1992, a TV series). In such cases,\nthe occurrence of the surface form of the entity is\nlarge. Simultaneously, questions related to ambigu-\nous entities are challenging to answer, leading to\nlower recall. In Section 6.2, we discussed the recall of an-\nswers in the retrieved text with respect to the con-\ntext length.", "chunk_type": "semantic", "chunker": "semantic_adjacent_v1"}
{"chunk_id": "drag_semantic_p15_c140", "pdf_name": "drag", "page_number": 15, "char_count": 351, "text": "We further illustrate the performance\ntrends of six dense retrievers, as detailed in Fig-\nure 7. The results indicate that the recall rate of\npropositions consistently outperforms that of sen-\ntences and passages. Our findings lead to the con-\nclusion that question-related density is greater in\nproposition units compared to sentences and pas-\nsages.", "chunk_type": "semantic", "chunker": "semantic_adjacent_v1"}
{"chunk_id": "drag_semantic_p15_c141", "pdf_name": "drag", "page_number": 15, "char_count": 316, "text": "G\nError Case Study\nTo understand the source of errors from each type\nof retrieval granularity, we present and discuss four\ntypical examples of mistakes in Table 6 and Table 7. With each example, we show the question and its\ncorresponding top-1 retrieved text unit by the GTR\nretriever across the three granularities.", "chunk_type": "semantic", "chunker": "semantic_adjacent_v1"}
{"chunk_id": "drag_semantic_p15_c142", "pdf_name": "drag", "page_number": 15, "char_count": 317, "text": "We observe that with passage-level retrieval, the\nambiguity of an entity or its references presents a\nchallenge for dense retrievers, which echoes find-\nings from (Min et al., 2020). For instance, in exam-\nple Q1, the question asks for “Super Bowl 50”, but\nthe retrieved passage and sentence refers to “Super\nBowl 5”.", "chunk_type": "semantic", "chunker": "semantic_adjacent_v1"}
{"chunk_id": "drag_semantic_p15_c143", "pdf_name": "drag", "page_number": 15, "char_count": 390, "text": "In Example Q2, passage retrieval fails\nto identify the part referring to the correct “atomic\nnumber”. Instead, the top-1 retrieved passage men-\ntions “atomic number” in a different and irrelevant\ncontext to the question. Retrieval by sentences can\nalso have a similar problem as retrieval by passages\nlike Example Q1. Also, retrieval by sentences faces\nanother challenge of lacking context.", "chunk_type": "semantic", "chunker": "semantic_adjacent_v1"}
{"chunk_id": "drag_semantic_p15_c144", "pdf_name": "drag", "page_number": 15, "char_count": 698, "text": "In Example\nQ3 (shown in Table 7), sentence-based retrieval\nfails as the correct sentence in the retrieved passage\nuses “it” to refer to the pericardial sac. Retrieval by propositions tackles the aforemen-\ntioned problems by ensuring each retrieval unit\ncontains one piece of fact only and necessary con-\ntext is incorporated in the propositions. However,\nproposition-based retrieval faces challenges with\nquestions that involve multi-hop reasoning over\nlong-range textual analysis. In Example Q4 (shown\nin Table 7), the retrieved passage separately de-\nscribes the actor’s name and the character they por-\ntray. There is not a single proposition that entails\nboth the question and the answer. 15173", "chunk_type": "semantic", "chunker": "semantic_adjacent_v1"}
{"chunk_id": "drag_fixed_p16_c87", "pdf_name": "drag", "page_number": 16, "char_count": 906, "text": "Passage Retrieval\nSentence Retrieval\nProposition Retrieval\nQ1: What was the theme of Super Bowl 50? Title: Super Bowl X\n✗\nThe overall theme of the Super Bowl enter-\ntainment was to celebrate the United States\nBicentennial. Each Cowboys and Steelers\nplayer wore a special patch with the Bicen-\ntennial logo on their jerseys... Title: Super Bowl X\n✗\nThe overall theme of the Super Bowl\nentertainment was to celebrate the\nUnited States Bicentennial. Title: Super Bowl XLV\n✓\n... As this was the 50th Super Bowl game,\nthe league [Super Bowl 50] emphasized\nthe \"golden anniversary\" with various gold-\nthemed initiatives during the 2015 season, as\nwell as... Q2: The atomic number of indium which belongs to 5th period is? Title: Period 5 element\n✗\nThe periodic table is laid out in rows to illus-\ntrate recurring (periodic) trends in the chemi-\ncal behaviour of the elements as their atomic\nnumber increases: ...", "chunk_type": "fixed", "chunker": "fixed_recursive_v1"}
{"chunk_id": "drag_fixed_p16_c88", "pdf_name": "drag", "page_number": 16, "char_count": 925, "text": "Title: Period 5 element\n✗\nThe periodic table is laid out in rows to illus-\ntrate recurring (periodic) trends in the chemi-\ncal behaviour of the elements as their atomic\nnumber increases: ... Title: Period 5 element\n✓\nIndium is a chemical element with the\nsymbol In and atomic number 49. Title: Period 5 element\n✓\nIndium is a chemical element with the sym-\nbol In and [Indium has a] atomic number 49. This rare, very soft, malleable ... Table 6: Example cases where top-1 retrieved text unit of each retrieval granularity fails to provide the correct\nanswer. The underlined text is the correct answer. The gray text is the context of propositions, but it is for illustration\npurpose only and not provided to the retrievers and downstream QA models. Passage Retrieval\nSentence Retrieval\nProposition Retrieval\nQ3: What is the function of the pericardial sac? Title: Pericardium\n✓\nThe pericardium, also called pericardial sac\n...", "chunk_type": "fixed", "chunker": "fixed_recursive_v1"}
{"chunk_id": "drag_fixed_p16_c89", "pdf_name": "drag", "page_number": 16, "char_count": 862, "text": "Title: Pericardium\n✓\nThe pericardium, also called pericardial sac\n... It separates the heart from interference of\nother structures, protects it against infection\nand blunt trauma, and lubricates the heart’s\nmovements. Title: Pericardium\n✗\nThe pericardium, also called pericar-\ndial sac, is a double-walled sac con-\ntaining the heart and the roots of the\ngreat vessels. Title: Cardiac muscle\n✓\nOn the outer aspect of the myocardium is the\nepicardium which forms part of the pericar-\ndial sac that surrounds, protects, and lubri-\ncates the heart. Q4: What is the main character’s name in layer cake? Title: Layer Cake (film)\n✓\n... The film’s plot revolves around a London-\nbased criminal, played by Daniel Craig, ... Craig’s character is unnamed in the film and\nis listed in the credits as \"XXXX\". Title: Angelic Layer\n✗\nThe primary protagonist is Misaki\nSuzuhara.", "chunk_type": "fixed", "chunker": "fixed_recursive_v1"}
{"chunk_id": "drag_fixed_p16_c90", "pdf_name": "drag", "page_number": 16, "char_count": 821, "text": "Title: Angelic Layer\n✗\nThe primary protagonist is Misaki\nSuzuhara. Title: Plot twist\n✗\nSometimes the audience may discover that\nthe true identity of a character is , in fact,\nunknown [in Layer Cake] , as in Layer Cake\nor the eponymous assassins in V for Vendetta\nand The Day of the Jackal. Table 7: Example cases where top-1 retrieved text unit of each retrieval granularity fails to provide the correct\nanswer. The underlined text is the correct answer. The gray text is the context of propositions, but it is for illustration\npurpose only and not provided to the retrievers and downstream QA models. 101\n102\nFrequency\n20\n40\n60\nRecall@5\nSimCSE\n101\n102\nFrequency\n30\n40\n50\n60\nRecall@5\nContriever\n101\n102\nFrequency\n40\n60\nRecall@5\nDPR\n101\n102\nFrequency\n70\n80\nRecall@5\nGTR\nPassage\nSentence\nProposition\n(a) Where was [X] born?", "chunk_type": "fixed", "chunker": "fixed_recursive_v1"}
{"chunk_id": "drag_fixed_p16_c91", "pdf_name": "drag", "page_number": 16, "char_count": 605, "text": "0\n40\n60\nRecall@5\nSimCSE\n101\n102\nFrequency\n30\n40\n50\n60\nRecall@5\nContriever\n101\n102\nFrequency\n40\n60\nRecall@5\nDPR\n101\n102\nFrequency\n70\n80\nRecall@5\nGTR\nPassage\nSentence\nProposition\n(a) Where was [X] born? 101\n102\nFrequency\n20\n40\n60\nRecall@5\nSimCSE\n101\n102\nFrequency\n20\n40\n60\nRecall@5\nContriever\n101\n102\nFrequency\n40\n60\n80\nRecall@5\nDPR\n101\n102\nFrequency\n70\n80\n90\nRecall@5\nGTR\nPassage\nSentence\nProposition\n(b) Who was [X] created by? Figure 6: Document retrieval recall vs. the frequency of the target entity in each question from the Entity Questions\ndataset. We display the performance of two relations. 15174", "chunk_type": "fixed", "chunker": "fixed_recursive_v1"}
{"chunk_id": "drag_semantic_p16_c145", "pdf_name": "drag", "page_number": 16, "char_count": 325, "text": "Passage Retrieval\nSentence Retrieval\nProposition Retrieval\nQ1: What was the theme of Super Bowl 50? Title: Super Bowl X\n✗\nThe overall theme of the Super Bowl enter-\ntainment was to celebrate the United States\nBicentennial. Each Cowboys and Steelers\nplayer wore a special patch with the Bicen-\ntennial logo on their jerseys...", "chunk_type": "semantic", "chunker": "semantic_adjacent_v1"}
{"chunk_id": "drag_semantic_p16_c146", "pdf_name": "drag", "page_number": 16, "char_count": 325, "text": "Title: Super Bowl X\n✗\nThe overall theme of the Super Bowl\nentertainment was to celebrate the\nUnited States Bicentennial. Title: Super Bowl XLV\n✓\n... As this was the 50th Super Bowl game,\nthe league [Super Bowl 50] emphasized\nthe \"golden anniversary\" with various gold-\nthemed initiatives during the 2015 season, as\nwell as...", "chunk_type": "semantic", "chunker": "semantic_adjacent_v1"}
{"chunk_id": "drag_semantic_p16_c147", "pdf_name": "drag", "page_number": 16, "char_count": 463, "text": "Q2: The atomic number of indium which belongs to 5th period is? Title: Period 5 element\n✗\nThe periodic table is laid out in rows to illus-\ntrate recurring (periodic) trends in the chemi-\ncal behaviour of the elements as their atomic\nnumber increases: ... Title: Period 5 element\n✓\nIndium is a chemical element with the\nsymbol In and atomic number 49. Title: Period 5 element\n✓\nIndium is a chemical element with the sym-\nbol In and [Indium has a] atomic number 49.", "chunk_type": "semantic", "chunker": "semantic_adjacent_v1"}
{"chunk_id": "drag_semantic_p16_c148", "pdf_name": "drag", "page_number": 16, "char_count": 347, "text": "This rare, very soft, malleable ... Table 6: Example cases where top-1 retrieved text unit of each retrieval granularity fails to provide the correct\nanswer. The underlined text is the correct answer. The gray text is the context of propositions, but it is for illustration\npurpose only and not provided to the retrievers and downstream QA models.", "chunk_type": "semantic", "chunker": "semantic_adjacent_v1"}
{"chunk_id": "drag_semantic_p16_c149", "pdf_name": "drag", "page_number": 16, "char_count": 325, "text": "Passage Retrieval\nSentence Retrieval\nProposition Retrieval\nQ3: What is the function of the pericardial sac? Title: Pericardium\n✓\nThe pericardium, also called pericardial sac\n... It separates the heart from interference of\nother structures, protects it against infection\nand blunt trauma, and lubricates the heart’s\nmovements.", "chunk_type": "semantic", "chunker": "semantic_adjacent_v1"}
{"chunk_id": "drag_semantic_p16_c150", "pdf_name": "drag", "page_number": 16, "char_count": 326, "text": "Title: Pericardium\n✗\nThe pericardium, also called pericar-\ndial sac, is a double-walled sac con-\ntaining the heart and the roots of the\ngreat vessels. Title: Cardiac muscle\n✓\nOn the outer aspect of the myocardium is the\nepicardium which forms part of the pericar-\ndial sac that surrounds, protects, and lubri-\ncates the heart.", "chunk_type": "semantic", "chunker": "semantic_adjacent_v1"}
{"chunk_id": "drag_semantic_p16_c151", "pdf_name": "drag", "page_number": 16, "char_count": 317, "text": "Q4: What is the main character’s name in layer cake? Title: Layer Cake (film)\n✓\n... The film’s plot revolves around a London-\nbased criminal, played by Daniel Craig, ... Craig’s character is unnamed in the film and\nis listed in the credits as \"XXXX\". Title: Angelic Layer\n✗\nThe primary protagonist is Misaki\nSuzuhara.", "chunk_type": "semantic", "chunker": "semantic_adjacent_v1"}
{"chunk_id": "drag_semantic_p16_c152", "pdf_name": "drag", "page_number": 16, "char_count": 344, "text": "Title: Plot twist\n✗\nSometimes the audience may discover that\nthe true identity of a character is , in fact,\nunknown [in Layer Cake] , as in Layer Cake\nor the eponymous assassins in V for Vendetta\nand The Day of the Jackal. Table 7: Example cases where top-1 retrieved text unit of each retrieval granularity fails to provide the correct\nanswer.", "chunk_type": "semantic", "chunker": "semantic_adjacent_v1"}
{"chunk_id": "drag_semantic_p16_c153", "pdf_name": "drag", "page_number": 16, "char_count": 814, "text": "The underlined text is the correct answer. The gray text is the context of propositions, but it is for illustration\npurpose only and not provided to the retrievers and downstream QA models. 101\n102\nFrequency\n20\n40\n60\nRecall@5\nSimCSE\n101\n102\nFrequency\n30\n40\n50\n60\nRecall@5\nContriever\n101\n102\nFrequency\n40\n60\nRecall@5\nDPR\n101\n102\nFrequency\n70\n80\nRecall@5\nGTR\nPassage\nSentence\nProposition\n(a) Where was [X] born? 101\n102\nFrequency\n20\n40\n60\nRecall@5\nSimCSE\n101\n102\nFrequency\n20\n40\n60\nRecall@5\nContriever\n101\n102\nFrequency\n40\n60\n80\nRecall@5\nDPR\n101\n102\nFrequency\n70\n80\n90\nRecall@5\nGTR\nPassage\nSentence\nProposition\n(b) Who was [X] created by? Figure 6: Document retrieval recall vs. the frequency of the target entity in each question from the Entity Questions\ndataset. We display the performance of two relations. 15174", "chunk_type": "semantic", "chunker": "semantic_adjacent_v1"}
{"chunk_id": "drag_fixed_p17_c92", "pdf_name": "drag", "page_number": 17, "char_count": 998, "text": "0 200 400 #Words 20 30 40 Recall (%) SimCSE / NQ 0 200 400 #Words 30 40 50 Recall (%) SimCSE / TQA 0 200 400 #Words 30 40 50 Recall (%) SimCSE / WebQ 0 200 400 #Words 20 30 40 Recall (%) SimCSE / SQuAD 0 200 400 #Words 30 40 50 Recall (%) SimCSE / EQ Passage Sentence Proposition 0 200 400 #Words 20 30 40 50 Recall (%) Contriever / NQ 0 200 400 #Words 40 50 60 70 Recall (%) Contriever / TQA 0 200 400 #Words 20 30 40 50 Recall (%) Contriever / WebQ 0 200 400 #Words 30 40 50 Recall (%) Contriever / SQuAD 0 200 400 #Words 20 30 40 50 Recall (%) Contriever / EQ Passage Sentence Proposition 0 200 400 #Words 50 60 70 Recall (%) DPR / NQ 0 200 400 #Words 60 70 Recall (%) DPR / TQA 0 200 400 #Words 50 60 70 Recall (%) DPR / WebQ 0 200 400 #Words 30 40 Recall (%) DPR / SQuAD 0 200 400 #Words 40 50 60 Recall (%) DPR / EQ Passage Sentence Proposition 0 200 400 #Words 50 60 70 Recall (%) GTR / NQ 0 200 400 #Words 60 70 Recall (%) GTR / TQA 0 200 400 #Words 50 60 70 Recall (%) GTR / WebQ 0 200 400", "chunk_type": "fixed", "chunker": "fixed_recursive_v1"}
{"chunk_id": "drag_fixed_p17_c93", "pdf_name": "drag", "page_number": 17, "char_count": 954, "text": "#Words 40 50 60 Recall (%) GTR / SQuAD 0 200 400 #Words 60 70 80 Recall (%) GTR / EQ Passage Sentence Proposition Figure 7: Recall of the gold answer in the retrieved text limited to first k words. Finer-grained retrieval has a higher\nrecall across all numbers of words. Is the proposition fully supported by the passage? No : The information provided relates to\nthe proposition, but there are some gaps or inconsistencies that prevent full support. Maybe\n: The information provided supports the proposition adequately, covering most aspects well;\nhowever, minor details or implications might not be fully explored or clarified. Yes: The\ninformation provided clearly and comprehensively addresses all aspects of the proposition,\nleaving no relevant details unexplained or ambiguous. Should the given propositions be further split into separate propositions? No: The\nproposition has a compound structure that could be separated into distinct propositions.", "chunk_type": "fixed", "chunker": "fixed_recursive_v1"}
{"chunk_id": "drag_fixed_p17_c94", "pdf_name": "drag", "page_number": 17, "char_count": 924, "text": "No: The\nproposition has a compound structure that could be separated into distinct propositions. Maybe:\nThe proposition is mostly straightforward with a single main idea and perhaps a minor additional\ndetail. Splitting might enhance clarity but is not strictly necessary. Yes: The proposition\nis already concise and does not contain a compound structure. Splitting it into separate\npropositions would likely reduce clarity. Is the given proposition self-contained? No: The proposition contains pronouns, terms, or\nreferences whose full names or meanings are not in the proposition. Maybe: The proposition is\nalmost entirely self-contained, with only a few minor terms that might be ambiguous without\nadditional context. Yes: The proposition is a self-contained claim without any ambiguities,\nfully understandable on its own. Table 8: Instructions for data annotation in analyzing the quality of generated propositions. 15175", "chunk_type": "fixed", "chunker": "fixed_recursive_v1"}
{"chunk_id": "drag_semantic_p17_c154", "pdf_name": "drag", "page_number": 17, "char_count": 1196, "text": "0\n200\n400\n#Words\n20\n30\n40\nRecall (%)\nSimCSE / NQ\n0\n200\n400\n#Words\n30\n40\n50\nRecall (%)\nSimCSE / TQA\n0\n200\n400\n#Words\n30\n40\n50\nRecall (%)\nSimCSE / WebQ\n0\n200\n400\n#Words\n20\n30\n40\nRecall (%)\nSimCSE / SQuAD\n0\n200\n400\n#Words\n30\n40\n50\nRecall (%)\nSimCSE / EQ\nPassage\nSentence\nProposition\n0\n200\n400\n#Words\n20\n30\n40\n50\nRecall (%)\nContriever / NQ\n0\n200\n400\n#Words\n40\n50\n60\n70\nRecall (%)\nContriever / TQA\n0\n200\n400\n#Words\n20\n30\n40\n50\nRecall (%)\nContriever / WebQ\n0\n200\n400\n#Words\n30\n40\n50\nRecall (%)\nContriever / SQuAD\n0\n200\n400\n#Words\n20\n30\n40\n50\nRecall (%)\nContriever / EQ\nPassage\nSentence\nProposition\n0\n200\n400\n#Words\n50\n60\n70\nRecall (%)\nDPR / NQ\n0\n200\n400\n#Words\n60\n70\nRecall (%)\nDPR / TQA\n0\n200\n400\n#Words\n50\n60\n70\nRecall (%)\nDPR / WebQ\n0\n200\n400\n#Words\n30\n40\nRecall (%)\nDPR / SQuAD\n0\n200\n400\n#Words\n40\n50\n60\nRecall (%)\nDPR / EQ\nPassage\nSentence\nProposition\n0\n200\n400\n#Words\n50\n60\n70\nRecall (%)\nGTR / NQ\n0\n200\n400\n#Words\n60\n70\nRecall (%)\nGTR / TQA\n0\n200\n400\n#Words\n50\n60\n70\nRecall (%)\nGTR / WebQ\n0\n200\n400\n#Words\n40\n50\n60\nRecall (%)\nGTR / SQuAD\n0\n200\n400\n#Words\n60\n70\n80\nRecall (%)\nGTR / EQ\nPassage\nSentence\nProposition\nFigure 7: Recall of the gold answer in the retrieved text limited to first k words.", "chunk_type": "semantic", "chunker": "semantic_adjacent_v1"}
{"chunk_id": "drag_semantic_p17_c155", "pdf_name": "drag", "page_number": 17, "char_count": 584, "text": "Finer-grained retrieval has a higher\nrecall across all numbers of words. Is the proposition fully supported by the passage? No : The information provided relates to\nthe proposition, but there are some gaps or inconsistencies that prevent full support. Maybe\n: The information provided supports the proposition adequately, covering most aspects well;\nhowever, minor details or implications might not be fully explored or clarified. Yes: The\ninformation provided clearly and comprehensively addresses all aspects of the proposition,\nleaving no relevant details unexplained or ambiguous.", "chunk_type": "semantic", "chunker": "semantic_adjacent_v1"}
{"chunk_id": "drag_semantic_p17_c156", "pdf_name": "drag", "page_number": 17, "char_count": 346, "text": "Should the given propositions be further split into separate propositions? No: The\nproposition has a compound structure that could be separated into distinct propositions. Maybe:\nThe proposition is mostly straightforward with a single main idea and perhaps a minor additional\ndetail. Splitting might enhance clarity but is not strictly necessary.", "chunk_type": "semantic", "chunker": "semantic_adjacent_v1"}
{"chunk_id": "drag_semantic_p17_c157", "pdf_name": "drag", "page_number": 17, "char_count": 652, "text": "Yes: The proposition\nis already concise and does not contain a compound structure. Splitting it into separate\npropositions would likely reduce clarity. Is the given proposition self-contained? No: The proposition contains pronouns, terms, or\nreferences whose full names or meanings are not in the proposition. Maybe: The proposition is\nalmost entirely self-contained, with only a few minor terms that might be ambiguous without\nadditional context. Yes: The proposition is a self-contained claim without any ambiguities,\nfully understandable on its own. Table 8: Instructions for data annotation in analyzing the quality of generated propositions. 15175", "chunk_type": "semantic", "chunker": "semantic_adjacent_v1"}
{"chunk_id": "drag_fixed_p18_c95", "pdf_name": "drag", "page_number": 18, "char_count": 996, "text": "Passage ⇒Propositions\nDecompose the \"Content\" into clear and simple propositions, ensuring they are interpretable out of\ncontext. 1. Split compound sentence into simple sentences. Maintain the original phrasing from the input\nwhenever possible. 2. For any named entity that is accompanied by additional descriptive information, separate this\ninformation into its own distinct proposition. 3. Decontextualize the proposition by adding necessary modifier to nouns or entire sentences\nand replacing pronouns (e.g., \"it\", \"he\", \"she\", \"they\", \"this\", \"that\") with the full name of the\nentities they refer to. 4. Present the results as a list of strings, formatted in JSON. Input: Title: ¯Eostre. Section: Theories and interpretations, Connection to Easter Hares. Content:\nThe earliest evidence for the Easter Hare (Osterhase) was recorded in south-west Germany in\n1678 by the professor of medicine Georg Franck von Franckenau, but it remained unknown in\nother parts of Germany until the 18th century.", "chunk_type": "fixed", "chunker": "fixed_recursive_v1"}
{"chunk_id": "drag_fixed_p18_c96", "pdf_name": "drag", "page_number": 18, "char_count": 735, "text": "e Easter Hare (Osterhase) was recorded in south-west Germany in\n1678 by the professor of medicine Georg Franck von Franckenau, but it remained unknown in\nother parts of Germany until the 18th century. Scholar Richard Sermon writes that \"hares were\nfrequently seen in gardens in spring, and thus may have served as a convenient explanation for the\norigin of the colored eggs hidden there for children. Alternatively, there is a European tradition\nthat hares laid eggs, since a hare’s scratch or form and a lapwing’s nest look very similar, and\nboth occur on grassland and are first seen in the spring. In the nineteenth century the influence\nof Easter cards, toys, and books was to make the Easter Hare/Rabbit popular throughout Europe.", "chunk_type": "fixed", "chunker": "fixed_recursive_v1"}
{"chunk_id": "drag_fixed_p18_c97", "pdf_name": "drag", "page_number": 18, "char_count": 1511, "text": "In the nineteenth century the influence\nof Easter cards, toys, and books was to make the Easter Hare/Rabbit popular throughout Europe. German immigrants then exported the custom to Britain and America where it evolved into the\nEaster Bunny.\"\nOutput: [ \"The earliest evidence for the Easter Hare was recorded in south-west Germany in\n1678 by Georg Franck von Franckenau.\", \"Georg Franck von Franckenau was a professor of\nmedicine.\", \"The evidence for the Easter Hare remained unknown in other parts of Germany until\nthe 18th century.\", \"Richard Sermon was a scholar.\", \"Richard Sermon writes a hypothesis about\nthe possible explanation for the connection between hares and the tradition during Easter\", \"Hares\nwere frequently seen in gardens in spring.\", \"Hares may have served as a convenient explanation\nfor the origin of the colored eggs hidden in gardens for children.\", \"There is a European tradition\nthat hares laid eggs.\", \"A hare’s scratch or form and a lapwing’s nest look very similar.\", \"Both\nhares and lapwing’s nests occur on grassland and are first seen in the spring.\", \"In the nineteenth\ncentury the influence of Easter cards, toys, and books was to make the Easter Hare/Rabbit popular\nthroughout Europe.\", \"German immigrants exported the custom of the Easter Hare/Rabbit to\nBritain and America.\", \"The custom of the Easter Hare/Rabbit evolved into the Easter Bunny in\nBritain and America.\" ]\nInput: <a new passage>\nOutput:\nFigure 8: Prompt for generating propositions from a passage using GPT-4.", "chunk_type": "fixed", "chunker": "fixed_recursive_v1"}
{"chunk_id": "drag_fixed_p18_c98", "pdf_name": "drag", "page_number": 18, "char_count": 206, "text": ", \"The custom of the Easter Hare/Rabbit evolved into the Easter Bunny in\nBritain and America.\" ]\nInput: <a new passage>\nOutput:\nFigure 8: Prompt for generating propositions from a passage using GPT-4. 15176", "chunk_type": "fixed", "chunker": "fixed_recursive_v1"}
{"chunk_id": "drag_semantic_p18_c158", "pdf_name": "drag", "page_number": 18, "char_count": 388, "text": "Passage ⇒Propositions\nDecompose the \"Content\" into clear and simple propositions, ensuring they are interpretable out of\ncontext. 1. Split compound sentence into simple sentences. Maintain the original phrasing from the input\nwhenever possible. 2. For any named entity that is accompanied by additional descriptive information, separate this\ninformation into its own distinct proposition.", "chunk_type": "semantic", "chunker": "semantic_adjacent_v1"}
{"chunk_id": "drag_semantic_p18_c159", "pdf_name": "drag", "page_number": 18, "char_count": 302, "text": "3. Decontextualize the proposition by adding necessary modifier to nouns or entire sentences\nand replacing pronouns (e.g., \"it\", \"he\", \"she\", \"they\", \"this\", \"that\") with the full name of the\nentities they refer to. 4. Present the results as a list of strings, formatted in JSON. Input: Title: ¯Eostre.", "chunk_type": "semantic", "chunker": "semantic_adjacent_v1"}
{"chunk_id": "drag_semantic_p18_c160", "pdf_name": "drag", "page_number": 18, "char_count": 704, "text": "Section: Theories and interpretations, Connection to Easter Hares. Content:\nThe earliest evidence for the Easter Hare (Osterhase) was recorded in south-west Germany in\n1678 by the professor of medicine Georg Franck von Franckenau, but it remained unknown in\nother parts of Germany until the 18th century. Scholar Richard Sermon writes that \"hares were\nfrequently seen in gardens in spring, and thus may have served as a convenient explanation for the\norigin of the colored eggs hidden there for children. Alternatively, there is a European tradition\nthat hares laid eggs, since a hare’s scratch or form and a lapwing’s nest look very similar, and\nboth occur on grassland and are first seen in the spring.", "chunk_type": "semantic", "chunker": "semantic_adjacent_v1"}
{"chunk_id": "drag_semantic_p18_c161", "pdf_name": "drag", "page_number": 18, "char_count": 1517, "text": "In the nineteenth century the influence\nof Easter cards, toys, and books was to make the Easter Hare/Rabbit popular throughout Europe. German immigrants then exported the custom to Britain and America where it evolved into the\nEaster Bunny.\"\nOutput: [ \"The earliest evidence for the Easter Hare was recorded in south-west Germany in\n1678 by Georg Franck von Franckenau.\", \"Georg Franck von Franckenau was a professor of\nmedicine.\", \"The evidence for the Easter Hare remained unknown in other parts of Germany until\nthe 18th century.\", \"Richard Sermon was a scholar.\", \"Richard Sermon writes a hypothesis about\nthe possible explanation for the connection between hares and the tradition during Easter\", \"Hares\nwere frequently seen in gardens in spring.\", \"Hares may have served as a convenient explanation\nfor the origin of the colored eggs hidden in gardens for children.\", \"There is a European tradition\nthat hares laid eggs.\", \"A hare’s scratch or form and a lapwing’s nest look very similar.\", \"Both\nhares and lapwing’s nests occur on grassland and are first seen in the spring.\", \"In the nineteenth\ncentury the influence of Easter cards, toys, and books was to make the Easter Hare/Rabbit popular\nthroughout Europe.\", \"German immigrants exported the custom of the Easter Hare/Rabbit to\nBritain and America.\", \"The custom of the Easter Hare/Rabbit evolved into the Easter Bunny in\nBritain and America.\" ]\nInput: <a new passage>\nOutput:\nFigure 8: Prompt for generating propositions from a passage using GPT-4. 15176", "chunk_type": "semantic", "chunker": "semantic_adjacent_v1"}
{"chunk_id": "drag_fixed_p19_c99", "pdf_name": "drag", "page_number": 19, "char_count": 660, "text": "Open-domain QA for LLaMA-2-7B\n... [demonstrations] ... Refer to the passages below and answer the following question with just a few words. Title: 1972 in spaceflight. Passage: In 1972, humanity’s last crewed mission to the Moon of the\n20th century was Apollo 17. Title: 1970s. Passage: Apollo 17 Astronaut Gene Cernan becomes the last man on the Moon on\nDecember 13, 1972. Title: List of Apollo missions\nRefer to the context above and answer the following question with just a few words. Question: when was the last time anyone was on the moon\nThe answer is\nFigure 9: Prompt for retrieval-augmented generation of open-domain QA for the LLaMA-2-7B model. 15177", "chunk_type": "fixed", "chunker": "fixed_recursive_v1"}
{"chunk_id": "drag_semantic_p19_c162", "pdf_name": "drag", "page_number": 19, "char_count": 660, "text": "Open-domain QA for LLaMA-2-7B\n... [demonstrations] ... Refer to the passages below and answer the following question with just a few words. Title: 1972 in spaceflight. Passage: In 1972, humanity’s last crewed mission to the Moon of the\n20th century was Apollo 17. Title: 1970s. Passage: Apollo 17 Astronaut Gene Cernan becomes the last man on the Moon on\nDecember 13, 1972. Title: List of Apollo missions\nRefer to the context above and answer the following question with just a few words. Question: when was the last time anyone was on the moon\nThe answer is\nFigure 9: Prompt for retrieval-augmented generation of open-domain QA for the LLaMA-2-7B model. 15177", "chunk_type": "semantic", "chunker": "semantic_adjacent_v1"}
